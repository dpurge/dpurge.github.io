[{"id":0,"href":"/docs/devops/ansible/installation/","title":"Installation","section":"Ansible","content":" Installation # Install Ansible:\nsudo apt-add-repository ppa:ansible/ansible sudo apt update sudo apt install ansible "},{"id":1,"href":"/docs/devops/argo/installation/","title":"Installation","section":"ArgoCD","content":" Installation # Install ArgoCD:\nkubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml helm repo add argo https://argoproj.github.io/argo-helm Forward port:\nkubectl get all -n argocd kubectl port-forward service/argocd-server -n argocd 8080:443 Extract credential:\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d Login from CLI:\nkubectl port-forward svc/argocd-server -n argocd 8080:443 argocd login 127.0.0.1:8080 Install application:\nargocd app create nginx-prod \\ --repo https://github.com/dpurge/argo-config.git \\ --path nginx/overlays/prod \\ --dest-server https://kubernetes.default.svc \\ --dest-namespace prod Cheatsheet:\nargocd app create # Create a new Argo CD application. argocd app list # List all applications in Argo CD. argocd app logs \u0026lt;appname\u0026gt; # Get the application’s log output. argocd app get \u0026lt;appname\u0026gt; # Get information about an Argo CD application. argocd app diff \u0026lt;appname\u0026gt; # Compare the application’s configuration to its source repository. argocd app sync \u0026lt;appname\u0026gt; # Synchronize the application with its source repository. argocd app history \u0026lt;appname\u0026gt; # Get information about an Argo CD application. argocd app rollback \u0026lt;appname\u0026gt; # Rollback to a previous version argocd app set \u0026lt;appname\u0026gt; # Set the application’s configuration. argocd app delete \u0026lt;appname\u0026gt; # Delete an Argo CD application. Cleanup:\nkubectl delete CustomResourceDefinition applications.argoproj.io kubectl delete CustomResourceDefinition applicationsets.argoproj.io kubectl delete CustomResourceDefinition appprojects.argoproj.io Examples:\nhelm template infra-cluster001/ | kubectl apply -f - argocd app create argo-cd --repo https://xxx/homelab.git --path environments/dev/argo-cd --dest-server https://kubernetes.default.svc --dest-namespace argocd argocd proj create infra --description \u0026#39;Infrastructure\u0026#39; --dest \u0026#39;*,*\u0026#39; --src \u0026#39;*\u0026#39; --allow-cluster-resource \u0026#39;*/*\u0026#39; argocd repocreds add https://xxx/_git/infrastructure-helm-charts --username infrastructure-helm-charts --password xxxxxxxxxxxx argocd app create root --repo https://xxx/_git/infrastructure-helm-charts --path infra-cluster001 --dest-namespace default --dest-server https://kubernetes.default.svc "},{"id":2,"href":"/docs/devops/aws/architecture/","title":"Architecture","section":"AWS","content":" Learn architecture # Exercises in simple application architecture design.\nTutorial: AWS Prescriptive Guidance\nStatic website # Web application with relational database # Use Fargate for ECS.\nData processing # Serverless data source monitoring # Problems:\nMonitoring Statistics Secrets management Configuration for Lambda functions Feature: process changes in the data source Scenario: data source has changes Given for each $currentValue in $batchOfChanges And $previousValue of $currentValue When $currenValue != $previousValue Then save $currentValue in the data index Scenario: data source has no changes Given new batch of changes When current value equals previous value Feature: notify users about interesting changes Scenario: the change is interesting Given data source has changed When the change is small Then do nothing Scenario: the change is not interesting Given data source has changed When the change is big Then send email to the user Feature: show monitoring dashboard Scenario: new event saved to data index "},{"id":3,"href":"/docs/devops/aws/aws-cli/","title":"Aws CLI","section":"AWS","content":" AWS command line interface # GitHub project "},{"id":4,"href":"/docs/devops/azure/exploring/","title":"Exploring","section":"Azure","content":" Exploring Azure resources # List subscription and tenant # az account show --query \u0026#34;{subscriptionId:id, tenantId:tenantId}\u0026#34; -o table az login az account subscription list Create service principal for RBAC # az ad sp create-for-rbac --role Contributor --name my-name-001 --scope /subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx az cloud set -n AzureCloud az login --service-principal -u *** --password=*** --tenant xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx --allow-no-subscriptions az account set --subscription xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx az aks get-credentials --resource-group $(ResourceGroup) --name $(KubernetesService) Find VM images # az vm image list-offers --publisher Canonical --location westeurope -o table az vm image list-skus --publisher Canonical --offer 0001-com-ubuntu-server-jammy --location westeurope -o table az vm image list --all --publisher Canonical --offer 0001-com-ubuntu-server-jammy --sku 22_04-lts --location westeurope -o table Refer to this image: Canonical:0001-com-ubuntu-server-jammy:22_04-lts\nList public and private IP # For current subscription:\naz vm list -d --query \u0026#34;[].{Name:name, PublicIPs:publicIps, PrivateIPs:privateIps}\u0026#34; -o table For all subscriptions:\nfor i in `az account list --query \u0026#34;[].{id:id}\u0026#34; --output tsv`; do az account set --subscription $i; az vm list -d --query \u0026#34;[].{Name:name, PublicIPs:publicIps, PrivateIPs:privateIps}\u0026#34; --output tsv; done Examples:\naz vmss show --resource-group ResGrp-xxx --name DEV-VMSS001 --query [sku] --output table az acr login --name dockercr001 Invoke-RestMethod -Headers @{\u0026#34;Metadata\u0026#34;=\u0026#34;true\u0026#34;} -Method GET -NoProxy -Uri \u0026#34;http://x.x.x.x/metadata/instance?api-version=2021-02-01\u0026#34; | ConvertTo-Json -Depth 64 $env:ARM_ACCESS_KEY = $(az storage account keys list --resource-group ResGrp-Terraform --account-name xxxx --query \u0026#39;[0].value\u0026#39; -o tsv) if ((kubectl get ns -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;).split(\u0026#39; \u0026#39;) -contains \u0026#39;keda\u0026#39;) {\u0026#34;Keda is installed\u0026#34;} $ctx = (Get-AzStorageAccount -ResourceGroupName ResGrp-xxx -Name xxx).Context $StgTable = Get-AzStorageTable -Name xxx -Context $Ctx foreach ($Row in (Get-AzTableRow -table $StgTable.CloudTable)) { $VMSize = $Row.VMSize $BuildTime = $Row.FinishTime - $Row.StartTime Write-Host (\u0026#39;{0}: {1:hh} hr {1:mm} min {1:ss} sec\u0026#39; -f $VMSize,$BuildTime) } "},{"id":5,"href":"/docs/devops/azure/snippets/","title":"Snippets","section":"Azure","content":" Snippets # az group create -g $group -l northeurope az vm create \\ -n vm1 \\ -g $group \\ -l northeurope \\ --image Win2019Datacenter \\ --admin-username $username \\ --admin-password $password \\ --nsg-rule rdp az appservice plan create \\ -n web-plan \\ -g $group \\ -l northeurope \\ --sku S1 az webapp create \\ -n $appname \\ -g $group \\ -p web-plan az webapp list -g $group --query \u0026#34;[].enabledHostNames\u0026#34; -o jsonc az group delete -g $group "},{"id":6,"href":"/docs/devops/docker/azure/","title":"Azure","section":"Docker","content":" Azure from docker # Access to the storage account # Dockerfile:\nFROM ubuntu:22.04 RUN apt-get update \u0026amp;\u0026amp; apt install -y ca-certificates pkg-config libfuse-dev cmake libcurl4-gnutls-dev libgnutls28-dev uuid-dev libgcrypt20-dev wget RUN wget https://packages.microsoft.com/config/ubuntu/20.04/packages-microsoft-prod.deb \\ \u0026amp;\u0026amp; dpkg -i packages-microsoft-prod.deb \\ \u0026amp;\u0026amp; rm packages-microsoft-prod.deb RUN apt-get update RUN apt-get install -y blobfuse fuse RUN mkdir /storage-container COPY fuse_connection.cfg /fuse_connection.cfg Contents of fuse_connection.cfg:\naccountName mystgaccnt accountKey xxxxxxxxxxxxxxxxxxxxxx containerName test-data Build and start the container:\ndocker build -t azure-data . docker run -it --rm --cap-add SYS_ADMIN --device /dev/fuse azure-data bash Create blob in test-data container:\nblobfuse /storage-container --tmp-path=/mnt/resource/blobfusetmp --config-file=/fuse_connection.cfg -o attr_timeout=240 -o entry_timeout=240 -o negative_timeout=120 mkdir /storage-container/test echo \u0026#34;hello world\u0026#34; \u0026gt; test/hello-blob.txt In another instance, read that data:\nblobfuse /storage-container --tmp-path=/mnt/resource/blobfusetmp --config-file=/fuse_connection.cfg -o attr_timeout=240 -o entry_timeout=240 -o negative_timeout=120 cat /storage-container/test/hello-blob.txt Delete blob and the directory:\nrm /storage-container/test/hello-blob.txt rmdir /storage-container/test Example:\naz login az account set --subscription \u0026#34;DEV environment\u0026#34; az acr login -n devdockercr001 docker pull devdockercr001.azurecr.io/devops-tools docker pull postgres docker run -e POSTGRES_PASSWORD=postgres -e POSTGRES_USER=postgres -d -p 5432:5432 postgres "},{"id":7,"href":"/docs/devops/docker/basics/","title":"Basics","section":"Docker","content":" Docker basics # "},{"id":8,"href":"/docs/devops/docker/compose/","title":"Compose","section":"Docker","content":" Docker compose # PostgreSQL # Todo \u0026hellip;\nMSSQL Server # ./db/Dockerfile:\nFROM mcr.microsoft.com/mssql/server:2019-latest WORKDIR /usr/src/app COPY ./entrypoint.sh /usr/src/app EXPOSE 1432 EXPOSE 1433 USER root CMD /bin/bash ./entrypoint.sh ./db/entrypoint.sh:\n/opt/mssql/bin/sqlservr ./docker-compose.yaml:\nversion: \u0026#34;3\u0026#34; services: db: restart: always build: context: ./db dockerfile: ./Dockerfile environment: SA_PASSWORD: p@ssw0rd ACCEPT_EULA: \u0026#34;Y\u0026#34; ports: - \u0026#34;1433:1433\u0026#34; "},{"id":9,"href":"/docs/devops/docker/installation/","title":"Installation","section":"Docker","content":" Docker installation # Windows # TODO\nLinux # TODO\nWSL # Enable integration in Docker Desktop: Settings -\u0026gt; Resources -\u0026gt; WSL Integration\n"},{"id":10,"href":"/docs/devops/docker/volumes/","title":"Volumes","section":"Docker","content":" Docker volumes # Docker Desktop for Windows stores volumes in the WSL instance docker-desktop-data. It is used by Docker Desktop backend to store images and volumes.\nTo export it:\nwsl --export docker-desktop-data docker-desktop-data.tar Creating and removing volumes # docker volume create jdp_src docker volume rm jdp_src Mounting volumes # Mounting simple volumes:\ndocker run -it --rm --volume jdp_src:/src alpine docker run -it --rm --mount source=jdp_src,target=/src alpine Mounting from WSL:\ndocker run -it --rm --volume \u0026#34;\\\\wsl$\\Ubuntu\\var\\docker\\volumes\\alpine_persistent_data:/data\u0026#34; alpine "},{"id":11,"href":"/docs/devops/git/branches/","title":"Branches","section":"Git","content":" Branches # git remote rm origin git remote add origin https://xxxx/xxxx git push origin --all New branch:\ngit checkout -b \u0026lt;branch\u0026gt; git push -u origin \u0026lt;branch\u0026gt; Merge:\ngit checkout master git merge hotfix List tags:\ngit fetch --tags git push --tags "},{"id":12,"href":"/docs/devops/git/configuration/","title":"Configuration","section":"Git","content":" Configuration of git # Username and email # git config --global user.name \u0026#34;D. Purge\u0026#34; git config --global user.email \u0026#34;my@email.com\u0026#34; SSH checkouts from Azure DevOps # Generate SSH keypair:\nssh-keygen -t rsa Full parameters:\nssh-keygen \\ -m PEM \\ -t rsa \\ -b 4096 \\ -C \u0026#34;user@server.example.com\u0026#34; \\ -f ~/.ssh/mykeys/privatekey \\ -N passphrase Parameters:\n-m PEM = format key as PEM -t RSA = type of the key, RSA format -b 4096 = number of bits in the key, 4096 bits -C \u0026ldquo; user@server.example.com\u0026rdquo; = a comment appended at the end of the public key, to identify it -f ~/.ssh/mykeys/privatekey = the filename of the private key filet, a corresponding public key file appended with .pub is generated in the same directory which must exist -N passphrase = a passphrase used to access the private key file Add configuration in ~/.ssh/config:\nHost ssh.dev.azure.com\rIdentityFile ~/.ssh/id_rsa\rIdentitiesOnly yes\rUser git\rPubkeyAcceptedAlgorithms +ssh-rsa\rHostkeyAlgorithms +ssh-rsa "},{"id":13,"href":"/docs/devops/helm/charts/","title":"Charts","section":"Helm","content":" Charts # Download # mkdir cluster-config cd cluster-config helm pull --help helm pull bitnami/mysql --untar=true helm install mysql ./mysql/ helm upgrade mysql --values=./mysql/custom-values.yaml ./mysql/ Convert to k8s configuration # helm template mysql ./mysql/ --values=./mysql/custom-values.yaml \u0026gt; mysql-installation.yaml Create charts # helm create example-helm-chart cd example-helm-chart/ helm template . # to test the output Reference values in the template: {{ .Values.MyKey }}\nCall function in the template with space-separated list of parameters: {{ lower .Values.MyKey }}\nPass values to a function through a pipeline: {{ .Values.MyKey | upper }}\nExample of flow control:\nimage: version: v0.1.0{{ if eq .Values.environment \u0026#34;dev\u0026#34; }}-dev{{ end }} Files containing named templates have names starting with underscore.\nExample of named template in _shared.tpl:\n{{ define \u0026#34;example\u0026#34; }} - name: xxx value: yyy {{ end }} Using named template:\nmylist: {{- include \u0026#34;example\u0026#34; . | indent 2}} Prefix k8s object names with release name:\nmetadata: name: {{ .Release.Name }}-webapp "},{"id":14,"href":"/docs/devops/helm/installation/","title":"Installation","section":"Helm","content":" Installation # Linux # TODO\nInstall Helm:\nexport HELM_VERSION=3.9.2 cd /tmp curl \\ -sSL https://get.helm.sh/helm-v${HELM_VERSION}-linux-amd64.tar.gz \\ -o helm.tar.gz tar xf helm.tar.gz linux-amd64/helm mv linux-amd64/helm /usr/local/bin/helm rmdir linux-amd64 rm helm.tar.gz Packages # Find helm charts on https://artifacthub.io/\nExample usage # helm repo list helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update helm install mysql bitnami/mysql helm list helm uninstall mysql Find chart values:\nhelm show values bitnami/mysql \u0026gt; values.yaml vim values.yaml helm upgrade mysql bitnami/mysql --set section.name=value helm upgrade mysql bitnami/mysql --values=values.yaml "},{"id":15,"href":"/docs/devops/k3s/k3d/","title":"K3d","section":"K3s","content":" K3d # K3d is a wrapper of K3s. K3d deploys docker based k3s clusters.\nInstallation # K3d CLI binaries:\nhttps://github.com/k3d-io/k3d/releases/download/v5.4.8/k3d-windows-amd64.exe https://github.com/k3d-io/k3d/releases/download/v5.4.8/k3d-linux-amd64 Test # k3d cluster create mycluster kubectl cluster-info kubectl get all k3d cluster delete mycluster "},{"id":16,"href":"/docs/devops/k3s/k3s/","title":"K3s","section":"K3s","content":" K3s # K3s deploys a virtual machine-based Kubernetes cluster.\nInstallation # curl -Lo /usr/local/bin/k3s https://github.com/k3s-io/k3s/releases/download/v1.27.5%2Bk3s1/k3s chmod a+x /usr/local/bin/k3s curl -Lo /usr/local/bin/kubectl \u0026#34;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026#34; chmod a+x /usr/local/bin/kubectl Test # Start server:\nK3S_KUBECONFIG_MODE=\u0026#34;644\u0026#34; k3s server --flannel-backend none Access server:\nexport KUBECONFIG=/etc/rancher/k3s/k3s.yaml kubectl get pods --all-namespaces helm ls --all-namespaces Install in LXD container:\nsudo su - cd /tmp curl https://get.k3s.io/ -o k3s.sh chmod +x k3s.sh ./k3s.sh systemctl status k3s k3s check-config Troubleshooting:\ncat /etc/os-release ip a s tail /var/log/syslog journalctl -u k3s.service /usr/local/bin/k3s-uninstall.sh "},{"id":17,"href":"/docs/devops/kubernetes/azure-kubernetes/","title":"Azure Kubernetes","section":"Kubernetes","content":" Kubernetes on Azure # Deployment to cluster # Connect to Azure container registry:\naz account set --subscription $SubscriptionID az aks get-credentials --resource-group ResGrp-Name001 --name acrname001 Create namespace YAML:\nkubectl create namespace my-namespace-name -o yaml --dry-run=client Create deployment YAML:\nkubectl create deployment image-name --image=acrname001.azurecr.io/image_name:1.0.0 -o yaml --dry-run=client Apply namespace YAML:\nkubectl apply -f namespace.yaml kubectl get namespaces Apply deployment YAML:\nkubectl apply -f image-name-deployment.yaml kubectl logs -f deployment/image-name --namespace my-namespace-name Check deployments:\nkubectl get deployments --all-namespaces=true kubectl get deployments --namespace $CurrentNamespace kubectl describe deployment $CurrentDeployment --namespace $CurrentNamespace kubectl logs -l $LabelKey=$LabelValue List pods:\nkubectl get pods --namespace my-namespace-name Execute command on a pod:\nkubectl exec image-name-xxxxxxxxxx-xxxxx --namespace my-namespace-name -- python --version Run interactive shell on a pod:\nkubectl exec --stdin --tty image-name-xxxxxxxxxx-xxxxx --namespace my-namespace-name -- bash "},{"id":18,"href":"/docs/devops/kubernetes/design-patterns/","title":"Design Patterns","section":"Kubernetes","content":" Kubernetes design patterns # Foundational # Health probe # Every container should implement APIs allowing the platform to observe and manage the application.\nPredictable demands # Every container should declare its resource requirements and runtime dependencies and stay confined to them.\nAutomated placement # Declarative Deployment # Managed Lifecycle # Structural # Init Container # Sidecar # Adapter # Ambassador # Configuration # EnvVar Configuration # Immutable Configuration # Configuration Template # Configuration Resource # Behavioral # Batch Job # Stateful Service # Service Discovery # Periodic Job # Stateless Service # Singleton Service # Higher Level # Controller # Operator # Serverless Serving # Serverless Eventing # Elastic Scale # Image Builder # "},{"id":19,"href":"/docs/devops/kubernetes/k9s/","title":"K9s","section":"Kubernetes","content":" K9s # Home page GitHub repository Releases Installation # Windows $K9s_Version = \u0026#34;0.27.4\u0026#34; $K2s_Archive = \u0026#34;https://github.com/derailed/k9s/releases/download/v${K9s_Version}/k9s_Windows_amd64.zip\u0026#34; Invoke-WebRequest -Uri $K2s_Archive -OutFile \u0026#34;${pwd}/k9s_Windows_amd64_${K9s_Version}.zip\u0026#34; Add-Type -Assembly System.IO.Compression.FileSystem $zip = [IO.Compression.ZipFile]::OpenRead(\u0026#34;${pwd}/k9s_Windows_amd64_${K9s_Version}.zip\u0026#34;) $Executables = $zip.Entries | Where-Object {$_.Name -like \u0026#39;*.exe\u0026#39;} foreach ($Executable in $Executables) { [System.IO.Compression.ZipFileExtensions]::ExtractToFile($Executable, \u0026#34;${pwd}/$($Executable.Name)\u0026#34;, $True) } $zip.Dispose() Remove-Item -Path \u0026#34;${pwd}/k9s_Windows_amd64_${K9s_Version}.zip\u0026#34; Linux TODO "},{"id":20,"href":"/docs/devops/kubernetes/kubectl-config/","title":"Kubectl Config","section":"Kubernetes","content":" KubeCtl configuration # Configuration files # Default file:\n~/.kube/config %USERPROFILE%\\.kube\\config Multiple files defined in environment variable:\nset KUBECONFIG=%USERPROFILE%\\.kube\\config;%USERPROFILE%\\.kube\\jdpnas02.yaml KUBECONFIG=~/.kube/config:~/.kube/jdpnas02.yaml Display configured contexts:\nkubectl config get-contexts Display merged configs:\nkubectl config view Show current context:\nkubectl config current-context Set default context:\nkubectl config use-context jdpnas02 Examples:\naz account set --subscription xxxxxxxxxxx az aks get-credentials --resource-group ResGrp-xxx-K8s --name xxxx kubectl config current-context kubectl get all --namespace xxx kubectl get cronjob --namespace xxx kubectl describe cronjob --namespace xxx kubectl get pods --namespace xxx kubectl logs --namespace xxx integration-sync-28023457-cbg4j kubectl delete job --namespace xxx integration-sync-28023332 kubectl delete cronjob integration-sync --namespace xxx kubectl exec --stdin --tty xxx-867d77f465-tkcdk --namespace xxx -- /bin/bash curl -X POST 127.0.0.1/sync --data \u0026#39;{\u0026#34;time_to_sync\u0026#34;: 240}\u0026#39; --header \u0026#34;Content-Type: application/json\u0026#34; "},{"id":21,"href":"/docs/devops/kubernetes/minikube/","title":"Minikube","section":"Kubernetes","content":" Minikube # minikube delete minikube start --memory 4096 Edit service xxx:\nkubectl edit svc xxx "},{"id":22,"href":"/docs/devops/kubernetes/sealed-secrets/","title":"Sealed Secrets","section":"Kubernetes","content":" Sealed secrets # Install kubeseal # Command line:\ncurl -sSL https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.19.3/kubeseal-0.19.3-linux-amd64.tar.gz -o kubeseal-linux-amd64.tar.gz tar -xf kubeseal-linux-amd64.tar.gz kubeseal mv kubeseal /usr/local/bin/ kubeseal --version Controller:\nkubectl apply -f https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.19.3/controller.yaml kubectl get pods -n kube-system | grep sealed-secrets-controller kubectl logs sealed-secrets-controller-xxx-xxx -n kube-system kubectl get secret -n kube-system -l sealedsecrets.bitnami.com/sealed-secrets-key -o yaml "},{"id":23,"href":"/docs/devops/linux/blocky/","title":"Blocky","section":"Linux","content":" Blocky # Blocky is a DNS proxy and ad-blocker.\nwget -q \u0026#34;https://github.com/0xERR0R/blocky/releases/download/v0.21/blocky_v0.21_Linux_x86_64.tar.gz\u0026#34; tar xf blocky_v0.21_Linux_x86_64.tar.gz sudo mv blocky /usr/local/bin/ sudo setcap cap_net_bind_service=ep /usr/local/bin/blocky Create user:\nsudo groupadd --system blocky sudo useradd --system --gid blocky --create-home --home-dir /var/lib/blocky --shell /usr/sbin/nologin --comment \u0026#34;Blocky DNS proxy\u0026#34; blocky Configuration file /etc/blocky/config.yml:\nupstream: default: - 8.8.8.8 - 1.1.1.1 blocking: blackLists: ads: - https://raw.githubusercontent.com/dpurge/dns-hole/main/blacklist/dpurge.txt - https://raw.githubusercontent.com/StevenBlack/hosts/master/hosts whiteLists: ads: - https://raw.githubusercontent.com/dpurge/dns-hole/main/whitelist/dpurge.txt clientGroupsBlock: default: - ads downloadTimeout: 4m downloadAttempts: 5 downloadCooldown: 10s customDNS: mapping: example.home.arpa: 127.0.0.1 ports: dns: 53 bootstrapDns: - 8.8.8.8 - 1.1.1.1 log: level: warn Service file /etc/systemd/system/blocky.service:\n[Unit] Description=Blocky DNS resolver ConditionPathExists=/usr/local/bin/blocky After=local-fs.target [Service] User=blocky Group=blocky Type=simple WorkingDirectory=/var/lib/blocky ExecStart=/usr/local/bin/blocky --config /etc/blocky/config.yml Restart=on-failure RestartSec=10 SyslogIdentifier=blocky [Install] WantedBy=multi-user.target Install:\nsystemctl is-active systemd-resolved sudo systemctl disable --now systemd-resolved.service sudo systemctl daemon-reload sudo systemctl enable blocky sudo systemctl start blocky systemctl is-active blocky sudo less /var/log/syslog Manage:\nsudo systemctl status blocky sudo systemctl stop blocky sudo systemctl start blocky sudo systemctl restart blocky Configure:\nsudo vim /etc/blocky/config.yml sudo vim /etc/systemd/system/blocky.service Uninstall:\nsystemctl stop blocky systemctl disable --now blocky.service systemctl enable --now systemd-resolved.service rm -rf /etc/systemd/system/blocky.service rm -rf /usr/local/bin/blocky userdel -r -f blocky "},{"id":24,"href":"/docs/devops/linux/caddy/","title":"Caddy","section":"Linux","content":" Caddy # Install xcaddy # wget -q https://github.com/caddyserver/xcaddy/releases/download/v0.3.4/xcaddy_0.3.4_linux_amd64.tar.gz tar xf xcaddy_0.3.4_linux_amd64.tar.gz sudo mv xcaddy /usr/local/bin/ Build caddy # xcaddy build \\ --with github.com/wxh06/caddy-uwsgi-transport sudo mv caddy /usr/local/bin/ sudo setcap cap_net_bind_service+eip /usr/local/bin/caddy Install caddy # sudo groupadd --system caddy sudo useradd --system --gid caddy --create-home --home-dir /var/lib/caddy --shell /usr/sbin/nologin --comment \u0026#34;Caddy web server\u0026#34; caddy Create files and directories:\nsudo mkdir /etc/caddy sudo touch /etc/caddy/Caddyfile sudo chown -R root:caddy /etc/caddy sudo mkdir /etc/ssl/caddy sudo chown -R root:caddy /etc/ssl/caddy sudo chmod 0770 /etc/ssl/caddy sudo mkdir -p /var/www/public_html sudo chown caddy:caddy /var/www/public_html sudo touch /etc/systemd/system/caddy.service sudo chmod 644 /etc/systemd/system/caddy.service sudo touch /var/www/public_html/index.html Service file /etc/systemd/system/caddy.service:\n[Unit] Description=Caddy web server Documentation=https://caddyserver.com/docs/ After=network.target network-online.target Requires=network-online.target [Service] Type=notify User=caddy Group=caddy ExecStart=/usr/local/bin/caddy run --environ --config /etc/caddy/Caddyfile ExecReload=/usr/local/bin/caddy reload --config /etc/caddy/Caddyfile --force TimeoutStopSec=5s LimitNOFILE=1048576 LimitNPROC=512 PrivateTmp=true ProtectSystem=full AmbientCapabilities=CAP_NET_ADMIN CAP_NET_BIND_SERVICE [Install] WantedBy=multi-user.target Caddy configuration file /etc/caddy/Caddyfile:\n{ auto_https off } http://hide.example.home.arpa { root * /var/www/public_html encode gzip file_server header / { Content-Security-Policy = \u0026#34;upgrade-insecure-requests; default-src \u0026#39;self\u0026#39;; style-src \u0026#39;self\u0026#39;; script-src \u0026#39;self\u0026#39;; img-src \u0026#39;self\u0026#39;; object-src \u0026#39;self\u0026#39;; worker-src \u0026#39;self\u0026#39;; manifest-src \u0026#39;self\u0026#39;;\u0026#34; Strict-Transport-Security = \u0026#34;max-age=63072000; includeSubDomains; preload\u0026#34; X-Xss-Protection = \u0026#34;1; mode=block\u0026#34; X-Frame-Options = \u0026#34;DENY\u0026#34; X-Content-Type-Options = \u0026#34;nosniff\u0026#34; Referrer-Policy = \u0026#34;strict-origin-when-cross-origin\u0026#34; Permissions-Policy = \u0026#34;fullscreen=(self)\u0026#34; Cache-Control = \u0026#34;max-age=0,no-cache,no-store,must-revalidate\u0026#34; } } Index file /var/www/public_html/index.html:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta http-equiv=\u0026#34;content-type\u0026#34; content=\u0026#34;text/html; charset=utf-8\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34;content=\u0026#34;width=device-width,initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;link href=\u0026#34;https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; integrity=\u0026#34;sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js\u0026#34; integrity=\u0026#34;sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;title\u0026gt;Static page example\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body class=\u0026#34;text-center\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;jumbotron text-center\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;Welcome to Caddy\u0026lt;/h1\u0026gt; \u0026lt;h2 class=\u0026#34;h2 card-title\u0026#34;\u0026gt;Welcome to Caddy\u0026lt;/h2\u0026gt; \u0026lt;h6 class=\u0026#34;h6 card-text\u0026#34;\u0026gt;Your installation of Caddy is working.\u0026lt;/h6\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Run service:\nsudo systemctl daemon-reload sudo systemctl enable caddy sudo systemctl start caddy sudo systemctl stop caddy systemctl status caddy.service sudo journalctl -xeu caddy.service sudo journalctl -u caddy --no-pager | less http://example.home.arpa { @api { path /config path /healthz path /stats/errors path /stats/checker } @static { path /static/* } @notstatic { not path /static/* } @imageproxy { path /image_proxy } @notimageproxy { not path /image_proxy } header { X-XSS-Protection \u0026#34;1; mode=block\u0026#34; X-Content-Type-Options \u0026#34;nosniff\u0026#34; X-Robots-Tag \u0026#34;noindex, noarchive, nofollow\u0026#34; -Server } header @api { Access-Control-Allow-Methods \u0026#34;GET, OPTIONS\u0026#34; Access-Control-Allow-Origin \u0026#34;*\u0026#34; } handle { encode zstd gzip reverse_proxy localhost:8001 { header_up X-Forwarded-Port {http.request.port} header_up X-Forwarded-Proto {http.request.scheme} } } } "},{"id":25,"href":"/docs/devops/linux/screen/","title":"Screen","section":"Linux","content":" Screen # Basics # Start screen session and give it a name: screen -S ScreenTest Detach from current session: Ctrl-A d List sessions: screen -ls Re-attach to an existing session: screen -r \u0026lt;PID\u0026gt; Re-attach session, if necessary detach or create: screen -dRR ScreenTest Force-exit screen: Ctrl-A Ctrl-\\ Kill session: screen -S ScreenTest -X quit Clean all dead screen sessions: screen -wipe "},{"id":26,"href":"/docs/devops/linux/searxng/","title":"Searxng","section":"Linux","content":" SearXNG # sudo apt update -y sudo apt upgrade -y sudo apt-get install -y python3-dev python3-babel python3-venv uwsgi uwsgi-plugin-python3 git build-essential libxslt-dev zlib1g-dev libffi-dev libssl-dev Install # Create user:\nsudo groupadd --system searxng sudo useradd --system --gid searxng --create-home --home-dir /var/lib/searxng --shell /bin/bash --comment \u0026#34;Privacy-respecting metasearch engine\u0026#34; searxng sudo -u searxng -i (searxng)$ git clone \u0026#34;https://github.com/searxng/searxng\u0026#34; \u0026#34;/var/lib/searxng/searxng-src\u0026#34; (searxng)$ python3 -m venv \u0026#34;/var/lib/searxng/searxng-pyenv\u0026#34; Autoload virtual enviroment in /var/lib/searxng/.profile:\n# Automatically load virtual environment if [ -f \u0026#34;$HOME/searxng-pyenv/bin/activate\u0026#34; ] ; then . \u0026#34;$HOME/searxng-pyenv/bin/activate\u0026#34; fi Exit and start in a new session. Make sure that pyenv was activated automatically:\n(searxng-pyenv)$ command -v python (searxng-pyenv)$ python --version Install dependencies in pyenv:\ncd $HOME/searxng-src pip install -U pip pip install -U setuptools pip install -U wheel pip install -U pyyaml pip install -e . Configure # sudo mkdir -p \u0026#34;/etc/searxng\u0026#34; sudo cp \u0026#34;/var/lib/searxng/searxng-src/utils/templates/etc/searxng/settings.yml\u0026#34; \u0026#34;/etc/searxng/settings.yml\u0026#34; Generate secret key:\nopenssl rand -hex 16 Update secret_key in the settings. Remove connection to Redis. Set port to 8001. Set bind_address to 127.0.0.1.\nuse_default_settings: true server: base_url: http://jdplxd01.home.arpa # change this! port: 8001 bind_address: \u0026#34;127.0.0.1\u0026#34; secret_key: \u0026#34;ultrasecretkey\u0026#34; # change this! limiter: false # can be disabled for a private instance image_proxy: true ui: static_use_hash: true redis: #url: redis://redis:6379/0 url: false Test interactively:\nsudo -u searxng -i cd /var/lib/searxng/searxng-src export SEARXNG_SETTINGS_PATH=\u0026#34;/etc/searxng/settings.yml\u0026#34; python searx/webapp.py curl --location --verbose --head --insecure 127.0.0.1:8001 uWSGI # sudo touch /etc/uwsgi/apps-available/searxng.ini sudo ln -s /etc/uwsgi/apps-available/searxng.ini /etc/uwsgi/apps-enabled/ Set configuration in /etc/uwsgi/apps-available/searxng.ini:\n[uwsgi] uid = searxng gid = searxng env = LANG=C.UTF-8 env = LANGUAGE=C.UTF-8 env = LC_ALL=C.UTF-8 chdir = /var/lib/searxng/searxng-src/searx env = SEARXNG_SETTINGS_PATH=/etc/searxng/settings.yml disable-logging = true chmod-socket = 666 single-interpreter = true master = true lazy-apps = true plugin = python3,http enable-threads = true module = searx.webapp virtualenv = /var/lib/searxng/searxng-pyenv pythonpath = /var/lib/searxng/searxng-src socket = /var/lib/searxng/run/socket buffer-size = 8192 static-map = /static=/usr/local/searxng/searxng-src/searx/static static-expires = /* 31557600 static-gzip-all = True offload-threads = %k cache2 = name=searxngcache,items=2000,blocks=2000,blocksize=4096,bitmap=1 Management of the service:\ncreate /etc/uwsgi/apps-available/searxng.ini enable: sudo -H ln -s /etc/uwsgi/apps-available/searxng.ini /etc/uwsgi/apps-enabled/ start: sudo -H service uwsgi start searxng restart: sudo -H service uwsgi restart searxng stop: sudo -H service uwsgi stop searxng disable: sudo -H rm /etc/uwsgi/apps-enabled/searxng.ini uwsgi \u0026ndash;socket 0.0.0.0:8001 \u0026ndash;protocol=http -w wsgi:searx.webapp sudo usermod -a -G www-data caddy\n[Unit] Description=SearXNG After=network.target [Service] User=searxng Group=www-data WorkingDirectory=/var/lib/searxng/searxng-src Environment=\u0026#34;SEARXNG_SETTINGS_PATH=/etc/searxng/settings.yml\u0026#34; ExecStart=python searx/webapp.py [Install] WantedBy=multi-user.target "},{"id":27,"href":"/docs/devops/linux/setup/","title":"Setup","section":"Linux","content":" Setup # Boot in text mode # Change the grub configuration file (/etc/default/grub):\n# GRUB_CMDLINE_LINUX_DEFAULT=\u0026#34;quiet splash\u0026#34; GRUB_CMDLINE_LINUX=\u0026#34;text\u0026#34; GRUB_TERMINAL=console sudo update-grub sudo systemctl set-default multi-user.target shutdown -r now SSH # sudo apt update sudo apt install openssh-server sudo systemctl status ssh sudo ufw allow ssh Check network configuration:\nsudo apt install net-tools ifconfig Docker # sudo apt install docker.io -y sudo apt install docker-compose -y Git # "},{"id":28,"href":"/docs/devops/linux/ubuntu/","title":"Ubuntu","section":"Linux","content":" Ubuntu # Get network configuration:\nip a s ifconfig -a Static IP in /etc/netplan/10-hostname.yaml:\nnetwork: version: 2 ethernets: eth0: dhcp4: false dhcp6: false dhcp-identifier: mac addresses: - 192.168.0.11/24 routes: - to: default via: 192.168.0.1 nameservers: addresses: - 192.168.0.1 Check disk size:\nlsblk --scsi sudo fdisk -l /dev/sda sudo parted -l df -h Check memory:\nfree Resize logical volume to all disk:\nvgdisplay lvdisplay lvextend -l +100%FREE /dev/ubuntu-vg/ubuntu-lv resize2fs /dev/mapper/ubuntu--vg-ubuntu--lv "},{"id":29,"href":"/docs/devops/macos/homebrew/","title":"Homebrew","section":"MacOS","content":" Homebrew # Install # Download package from https://github.com/Homebrew/brew/releases/.\nIt installs in /opt/homebrew.\nConfiguration # Add line to ~/.zprofile:\neval \u0026#34;$(/opt/homebrew/bin/brew shellenv)\u0026#34; Usage # brew help brew analytics off brew doctor brew update brew shellenv brew cleanup brew install xxx brew reinstall xxx brew uninstall xxx brew bundle dump brew bundle install --file ./Brewfile "},{"id":30,"href":"/docs/devops/macos/multipass/","title":"Multipass","section":"MacOS","content":" Multipass # Install with package from https://multipass.run/install.\nThe command is installed in /usr/local/bin/multipass.\nLogs are in /Library/Logs/Multipass/multipassd.log.\nUsage:\nmultipass networks multipass set local.bridged-network=en0 multipass launch docker --name jdp --cpus 2 --memory 6G --disk 50G --bridged multipass info jdp multipass shell jdp multipass start jdp multipass restart jdp multipass stop jdp multipass delete jdp multipass launch docker --cpus 2 --memory 6G --disk 50G multipass list multipass info --all multipass purge Config # Add to ~/.zprofile:\nPATH=\u0026#34;${PATH}:/Users/david/Library/Application Support/multipass/bin\u0026#34; Applications to install:\nKubeCtl K3D Inside Ubuntu, install additional tools:\n# kubectl curl -LO https://dl.k8s.io/release/v1.28.3/bin/linux/arm64/kubectl sudo chmod +x kubectl sudo mv kubectl /usr/local/bin # K3D sudo curl -sfL https://github.com/k3d-io/k3d/releases/download/v5.6.0/k3d-linux-arm64 --output k3d sudo chmod +x k3d sudo mv k3d /usr/local/bin In MacOS terminal:\nmultipass mount $HOME jdp multipass mount /opt/jdp/src jdp multipass alias docker:kubectl kubectl multipass alias docker:k3d k3d MacOS firewall has problems.\nOn unmanaged Mac:\n/usr/libexec/ApplicationFirewall/socketfilterfw --add /usr/libexec/bootpd /usr/libexec/ApplicationFirewall/socketfilterfw --unblock /usr/libexec/bootpd "},{"id":31,"href":"/docs/devops/packer/installation/","title":"Installation","section":"Packer","content":" Installation # Linux # export PACKER_VERSION=1.7.10 curl \\ -sSL https://releases.hashicorp.com/packer/${PACKER_VERSION}/packer_${PACKER_VERSION}_linux_amd64.zip \\ -o /tmp/packer_linux_amd64.zip cd /usr/local/bin unzip /tmp/packer_linux_amd64.zip rm /tmp/packer_linux_amd64.zip Example:\npacker validate src/build-agent-ubuntu.pkr.hcl packer build src/build-agent-ubuntu.pkr.hcl "},{"id":32,"href":"/docs/devops/terraform/basics/","title":"Basics","section":"Terraform","content":" Basic commands # Resource management # terraform init terraform plan terraform apply terraform destroy Work with code # terraform validate terraform fmt Work with state # terraform state list terraform state mv terraform state show terraform state rm \\\u0026lt;type\\\u0026gt;.\\\u0026lt;resource\\\u0026gt; Examples:\n[System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String((kubectl get secret api-secret -o jsonpath=\u0026#39;{.data.db-connectionstring}\u0026#39;))) terragrunt plan -var-file secrets.tfvars --terragrunt-log-level debug az ad group show --group xxx-Contributors --query id --output tsv "},{"id":33,"href":"/docs/devops/terraform/installation/","title":"Installation","section":"Terraform","content":" Installation # Linux # Install tfswitch:\nexport TFSWITCH_VERSION=0.13.1300 curl \\ -sSL https://github.com/warrensbox/terraform-switcher/releases/download/${TFSWITCH_VERSION}/terraform-switcher_${TFSWITCH_VERSION}_linux_amd64.tar.gz \\ -o /tmp/tfswitch_linux_amd64.tar.gz tar --directory=/usr/local/bin -xf /tmp/tfswitch_linux_amd64.tar.gz tfswitch rm /tmp/tfswitch_linux_amd64.tar.gz Install terraform:\ntfswitch 1.3.7 Install tflint:\nexport TFLINT_VERSION=0.38.1 curl \\ -sSL https://github.com/terraform-linters/tflint/releases/download/v${TFLINT_VERSION}/tflint_linux_amd64.zip \\ -o /tmp/tflint_linux_amd64.zip cd /usr/local/bin unzip /tmp/tflint_linux_amd64.zip rm /tmp/tflint_linux_amd64.zip "},{"id":34,"href":"/docs/devops/terragrunt/basics/","title":"Basics","section":"Terragrunt","content":" Basic commands # Work with state # terragrunt state pull \u0026gt; backup.tfstate terragrunt state push backup.tfstate "},{"id":35,"href":"/docs/devops/vagrant/installation/","title":"Installation","section":"Vagrant","content":" Installation # Windows # Install Hyper-V Check Samba configuration Configure Hyper-V as default Vagrant provider Install Vagrant\u0026rsquo;s reload plugin Create NAT switch for Hyper-V Get-SmbServerConfiguration [Environment]::SetEnvironmentVariable(\u0026#34;VAGRANT_DEFAULT_PROVIDER\u0026#34;, \u0026#34;hyperv\u0026#34;, \u0026#34;User\u0026#34;) vagrant plugin install vagrant-reload # Get-NetAdapter | Format-Table -AutoSize Get-VMSwitch | Select-Object -ExpandProperty Name # if NatSwitch not listed New-VMSwitch -SwitchName \u0026#34;NATSwitch\u0026#34; -SwitchType Internal Get-NetIPAddress | Select-Object -ExpandProperty IPAddress # if 192.168.200.1 not listed New-NetIPAddress -IPAddress 192.168.200.1 -PrefixLength 24 -InterfaceAlias \u0026#34;vEthernet (NATSwitch)\u0026#34; Get-NetNAT | Select-Object -ExpandProperty InternalIPInterfaceAddressPrefix # if 192.168.200.0/24 not listed New-NetNAT -Name \u0026#34;NATNetwork\u0026#34; -InternalIPInterfaceAddressPrefix 192.168.200.0/24 Vagrantfile # This template is ready to create several VMs on Hyper-V and set static address to all of them.\n# Groovy Gorilla Vagrant.configure(\u0026#34;2\u0026#34;) do |config| servers=[ { :hostname =\u0026gt; \u0026#34;srv01\u0026#34;, :box =\u0026gt; \u0026#34;generic/ubuntu2010\u0026#34;, :ip =\u0026gt; \u0026#34;192.168.200.101\u0026#34;, :ssh_port =\u0026gt; \u0026#39;2201\u0026#39;, :memory =\u0026gt; 512, :cpus =\u0026gt; 1 } ] gateway=\u0026#34;192.168.200.1\u0026#34; servers.each do |srv| config.vm.define srv[:hostname] do |node| node.vm.box = srv[:box] node.vm.hostname = srv[:hostname] node.vm.provider :hyperv node.vm.network :public_network, auto_config: false node.vm.network \u0026#34;forwarded_port\u0026#34;, guest: 22, host: srv[:ssh_port], id: \u0026#34;ssh\u0026#34; node.vm.synced_folder \u0026#34;.\u0026#34;, \u0026#34;/vagrant\u0026#34;, disabled: true node.vm.provision \u0026#34;shell\u0026#34;, path: \u0026#34;./scripts/configure-static-ip.sh\u0026#34;, :args =\u0026gt; \u0026#34;#{srv[:ip]} #{gateway}\u0026#34; servers.each do |host| node.vm.provision \u0026#34;shell\u0026#34;, path: \u0026#34;./scripts/add-hosts-entry.sh\u0026#34;, :args =\u0026gt; \u0026#34;#{host[:ip]} #{host[:hostname]}\u0026#34; end node.vm.provision :reload node.vm.provider :hyperv do |h| h.vmname = srv[:hostname] h.enable_virtualization_extensions = true h.linked_clone = true h.maxmemory = 2048 h.memory = srv[:memory] h.cpus = srv[:cpus] end end end end Script to update /etc/hosts file in scripts/add-hosts-entry.sh:\n#!/bin/sh echo -n \u0026#39;Adding IP to hosts file: \u0026#39; echo $1 echo -n \u0026#39;Host name is: \u0026#39; echo $2 echo \u0026#34;$1 $2\u0026#34; \u0026gt;\u0026gt; /etc/hosts Ubuntu scripts/configure-static-ip.sh # #!/bin/sh echo -n \u0026#39;Setting static IP address: \u0026#39; echo $1 echo -n \u0026#39;Setting gateway address: \u0026#39; echo $2 cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/netplan/01-netcfg.yaml network: version: 2 ethernets: eth0: dhcp4: no addresses: [$1/24] gateway4: $2 nameservers: addresses: [8.8.8.8,8.8.4.4] EOF CentOS scripts/configure-static-ip.sh # #!/bin/sh echo -n \u0026#39;Setting static IP address: \u0026#39; echo $1 echo -n \u0026#39;Setting gateway address: \u0026#39; echo $2 cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=eth0 BOOTPROTO=none ONBOOT=yes PREFIX=24 IPADDR=$1 GATEWAY=$2 DNS1=8.8.8.8 EOF Commands # The Hyper-V provider requires that Vagrant be run with administrative privileges. This is a limitation of Hyper-V itself.\nvagrant up vagrant ssh srv01 vagrant halt vagrant destroy Username is vagrant, password is vagrant.\n"},{"id":36,"href":"/docs/devops/windows/winget/","title":"Winget","section":"Windows","content":" Winget # winget.exe install --id Microsoft.Powershell --source winget Git # winget install --id Git.Git -e --source winget After installation, copy SSH keys to ~\\.ssh.\n"},{"id":37,"href":"/docs/devops/wsl/installation/","title":"Installation","section":"WSL","content":" Installation of WSL # WSL2 # Enable Windows subsystem for Linux:\nEnable-WindowsOptionalFeature -Online -NoRestart -FeatureName Microsoft-Windows-Subsystem-Linux Enable-WindowsOptionalFeature -Online -NoRestart -FeatureName VirtualMachinePlatform Enable version 2 of WSL:\nInvoke-WebRequest https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi -OutFile c:\\wsl_update_x64.msi -UseBasicParsing Invoke-WebRequest https://... -OutFile D:\\dat\\WSL\\kernel\\vmlinux -UseBasicParsing start D:\\dat\\WSL\\wsl_update_x64.msi wsl --set-default-version 2 Configure non-default kernel in %USERPROFILE%\\.wslconfig:\n[wsl2] kernel=D:\\\\dat\\\\WSL\\\\kernel\\\\vmlinux Import Ubuntu image from docker # Download image: Invoke-WebRequest https://raw.githubusercontent.com/tianon/docker-brew-ubuntu-core/fbca80af7960ffcca085d509c20f53ced1697ade/kinetic/ubuntu-kinetic-oci-amd64-root.tar.gz -OutFile C:\\ubuntu-kinetic-oci-amd64-root.tar.gz -UseBasicParsing Import image: wsl --import jdp-ubuntu C:\\dat\\WSL\\jdp-ubuntu C:\\ubuntu-kinetic-oci-amd64-root.tar.gz Update system: apt-get update Import Arch image from docker # Download image: Invoke-WebRequest https://gitlab.archlinux.org/archlinux/archlinux-docker/-/package_files/2816/download -OutFile C:\\arch-base-20220704.0.66039.tar.zst -UseBasicParsing Import image: wsl --import jdp-arch C:\\dat\\WSL\\jdp-arch C:\\arch-base-20220704.0.66039.tar Uncomment locale file: /etc/locale.gen Generate locale: locale-gen Initialize key: pacman-key --init Update system: pacman -Syu Mount host directory # Create directory in WSL: mkdir /src\nEdit /etc/fstab and add line: C:/src /src drvfs defaults 0 0\nReload the fstab file: sudo mount -a\nEnable SystemD # Install store version of WSL: https://aka.ms/wslstorepage\nEnable systemd inside distribution by editting /etc/wsl.conf:\n[boot] systemd=true Reboot WSL and test:\nsystemctl list-units --type=service systemctl list-unit-files --type=service "},{"id":38,"href":"/docs/devops/wsl/ubuntu2004/","title":"Ubuntu2004","section":"WSL","content":" Ubuntu 20.04 # Invoke-WebRequest -Uri https://aka.ms/wslubuntu2004 -OutFile c:\\ubuntu2004.appx -UseBasicParsing Add-AppxPackage c:\\ubuntu2004.appx # DISM.EXE /Online /Add-ProvisionedAppxPackage /PackagePath:c:\\\\ubuntu2004.appx /SkipLicense --AllUsers Start Ubuntu 20.04 console to install the base system, and then run:\nsudo apt update sudo apt upgrade Docker in Ubuntu 20.04 # sudo apt install --no-install-recommends apt-transport-https ca-certificates curl gnupg2 net-tools source /etc/os-release curl -fsSL https://download.docker.com/linux/${ID}/gpg | sudo apt-key add - echo \u0026#34;deb [arch=amd64] https://download.docker.com/linux/${ID} ${VERSION_CODENAME} stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list sudo apt update sudo apt install docker-ce docker-ce-cli containerd.io docker-compose "},{"id":39,"href":"/docs/devops/wsl/ubuntu2204/","title":"Ubuntu2204","section":"WSL","content":" Ubuntu 22.04 # Update the system:\nsudo apt update sudo apt -y upgrade RDP connections # Install xrdp and xwindows:\nsudo apt install -y xrdp sudo apt install -y xfce4 xfce4-goodies sudo sed -i \u0026#39;s/3389/3390/g\u0026#39; /etc/xrdp/xrdp.ini sudo sed -i \u0026#39;s/max_bpp=32/max_bpp=128/g\u0026#39; /etc/xrdp/xrdp.ini sudo sed -i \u0026#39;s/xserverbpp=24/xserverbpp=128/g\u0026#39; /etc/xrdp/xrdp.ini echo xfce4-session \u0026gt; ~/.xsession Comment out and add line in /etc/xrdp/startwm.sh:\n# test -x /etc/X11/Xsession \u0026amp;\u0026amp; exec /etc/X11/Xsession # exec /bin/sh /etc/X11/Xsession startxfce4 Start XRDP:\nsudo /etc/init.d/xrdp start Connect in RDP to localhost:3390.\n"},{"id":40,"href":"/docs/other/books/snippets/","title":"Snippets","section":"Books","content":" Snippets # Export PDF to PNG # Import-Module -Name D:\\src\\github.com\\dpurge\\jdp-psmodule\\src\\JdpBookbind Invoke-Book2Image -OutputDirectory img -InputFile book.pdf Merge two directories with images # $a = Get-ChildItem -Filter *.png ./img1 $b = Get-ChildItem -Filter *.png ./img2 $imgs = $a + $b $i = 0 foreach ($img in $imgs) { $f = \u0026#34;./img/page-{0:d3}.png\u0026#34; -f $i Write-Host \u0026#34;$img -\u0026gt; $f\u0026#34; Move-Item -Path $img -Destination $f $i++ } Render PDF pages # 0..350 | %{D:\\pgm\\ImageMagick\\convert.exe -density 300 book.pdf[$_] -quality 100 (\u0026#34;./img/page-{0:d3}.tif\u0026#34; -f $_)} 0..350 | %{D:\\pgm\\ImageMagick\\convert.exe -density 300 book.pdf[$_] -quality 100 -type Grayscale -filter Lanczos (\u0026#34;./img/page-{0:d3}.png\u0026#34; -f $_)} Convert to grayscale # $files = Get-ChildItem -Filter *.png foreach($file in $files) { $outfile = \u0026#34;${pwd}\\out\\$($file.Name)\u0026#34; Write-Host \u0026#34;$file -\u0026gt; $outfile\u0026#34; # D:\\pgm\\ImageMagick\\convert.exe -grayscale Rec709Luminance $file $outfile D:\\pgm\\ImageMagick\\convert.exe -negate $file $outfile # D:\\pgm\\ImageMagick\\convert.exe $file -depth 4 -colorspace gray -define png:color-type=0 -define png:bit-depth=4 $outfile } Convert FLAC to MP3 # $files = Get-ChildItem -Filter *.flac foreach($file in $files) { $outfile = ($file.Fullname -replace \u0026#39;.flac$\u0026#39;,\u0026#39;.mp3\u0026#39;) Write-Host \u0026#34;$file -\u0026gt; $outfile\u0026#34; D:\\pgm\\ffmpeg\\bin\\ffmpeg.exe -i $file -ab 320k -map_metadata 0 -id3v2_version 3 $outfile } Convert WMA to MP3 # $files = Get-ChildItem -Filter *.wma foreach($file in $files) { $outfile = ($file.Fullname -replace \u0026#39;.wma$\u0026#39;,\u0026#39;.mp3\u0026#39;) Write-Host \u0026#34;$file -\u0026gt; $outfile\u0026#34; D:\\pgm\\ffmpeg\\bin\\ffmpeg.exe -i $file -ab 192k $outfile } Convert WEBM to MP3 # D:\\pgm\\ffmpeg\\bin\\ffmpeg.exe -i nagranie.webm -vn -ab 128k -ar 44100 -y nagranie.mp3 Download from YouTube # youtube-dl -x --audio-format mp3 \u0026lt;Video-URL\u0026gt; youtube-dl -f bestaudio[ext=m4a] --embed-thumbnail --add-metadata \u0026lt;Video-URL\u0026gt; Convert PNG to SVG # $files = Get-ChildItem -Filter *.png foreach($pngfile in $files) { $bmpfile = ($pngfile.Fullname -replace \u0026#39;([^.]).png\u0026#39;,\u0026#39;.bmp\u0026#39;) $svgfile = ($pngfile.Fullname -replace \u0026#39;([^.]).png\u0026#39;,\u0026#39;.svg\u0026#39;) Write-Host \u0026#34;$pngfile -\u0026gt; $bmpfile -\u0026gt; $svgfile\u0026#34; D:\\pgm\\ImageMagick\\convert.exe $pngfile $bmpfile potrace.exe -s -a0 $bmpfile } Print scanned book for binding # $sheets = 8 $pages = 4 * $sheets $files = Get-ChildItem -Filter *.png $blank = $files[0] $batches=for($i=0; $i -lt $files.length; $i+=$pages){ ,($files[$i..($i + $pages - 1)])} foreach ($i in 1..($pages - $batches[-1].length)) { $batches[-1] += $blank } $batchnr = 0 foreach($batch in $batches) { $batchnr += 1 $outfile = \u0026#34;../batch-{0:d2}.pdf\u0026#34; -f $batchnr $printBatch = @() foreach ($i in 0..($batch.length / 4 - 1)) { $printBatch += $batch[($pages - 1 - 2 * $i)] # last page $printBatch += $batch[(2 * $i)] # first page $printBatch += $batch[(2 * $i + 1)] # second page $printBatch += $batch[($pages - 1 - 2 * $i - 1)] # last-but-one page } Write-Host $outfile Write-Host $printBatch.basename D:\\pgm\\ImageMagick\\convert.exe $printBatch $outfile } "},{"id":41,"href":"/docs/programming/awk/basics/","title":"Basics","section":"Awk","content":" Awk basics # "},{"id":42,"href":"/docs/programming/bash/basics/","title":"Basics","section":"Bash","content":" Bash basics # Get unix timestamp # date +%s "},{"id":43,"href":"/docs/programming/batch/basics/","title":"Basics","section":"Batch","content":" Batch basics # Map drive to directory # Create mapping:\nsubst G: C:\\jdp\\src\\github.com\\dpurge Remove mapping:\nsubst G: /D "},{"id":44,"href":"/docs/programming/csharp/basics/","title":"Basics","section":"C#","content":" C# Basics # "},{"id":45,"href":"/docs/programming/go/basics/","title":"Basics","section":"Go","content":" Go Basics # Project template # Set up:\nmkdir example-project cd ./example-project go mod init ./.gitignore:\n./Makefile:\n.PHONY: clean build test clean: echo \u0026#34;Not implemented\u0026#34; build: echo \u0026#34;Not implemented\u0026#34; test: echo \u0026#34;Not implemented\u0026#34; install: echo \u0026#34;Not implemented\u0026#34; ./main.go:\npackage main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;Hello, World!\u0026#34;) } Run project:\ngo run . Taskfile # Task file home Task file repository "},{"id":46,"href":"/docs/programming/go/cgi/","title":"Cgi","section":"Go","content":" CGI # Simple CGI app # package main import ( \u0026#34;bytes\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; ) func main() { var buf bytes.Buffer fmt.Fprintf(\u0026amp;buf, \u0026#34;Time at %s: %s\\n\u0026#34;, os.Getenv(\u0026#34;SERVER_NAME\u0026#34;), time.Now().Format(time.RFC1123)) fmt.Println(\u0026#34;Content-type: text/plain\u0026#34;) fmt.Printf(\u0026#34;Content-Length: %d\\n\\n\u0026#34;, buf.Len()) buf.WriteTo(os.Stdout) } "},{"id":47,"href":"/docs/programming/go/installation/","title":"Installation","section":"Go","content":" Installation # sudo apt update sudo apt upgrade -y sudo apt install golang -y Check version: go version\n"},{"id":48,"href":"/docs/programming/powershell/azdo/","title":"Azdo","section":"Powershell","content":" Azure DevOps # URIs and authentication header # $AccessToken = $env:AZDO_ADMIN_TOKEN $Organization = \u0026#34;MyOrganization\u0026#34; $AuthenicationHeader = @{ Authorization = \u0026#39;Basic \u0026#39; + [Convert]::ToBase64String([Text.Encoding]::ASCII.GetBytes(\u0026#34;:${AccessToken}\u0026#34;)) } $OrganizationUri = \u0026#34;https://dev.azure.com/${Organization}/\u0026#34; $ProjectsUri = \u0026#34;${OrganizationUri}_apis/projects?api-version=5.1\u0026#34; $RepositoriesUri = \u0026#34;${OrganizationUri}_apis/git/repositories?api-version=7.0\u0026#34; AzDO projects # $Projects = Invoke-RestMethod -Uri $ProjectsUri -Method get -Headers $AuthenicationHeader foreach ($Project in $Projects.Value) { $ProjectName = $Project.name $ProjectUri = $Project.url Write-Host \u0026#34;${ProjectName}: ${ProjectUri}\u0026#34; } AzDO repositories # $Repositories = Invoke-RestMethod -Uri $RepositoriesUri -Method get -Headers $AuthenicationHeader foreach ($Repository in $Repositories.value) { $ProjectName = $Repository.Project.name $RepositoryName = $Repository.name $RepositoryWebUri = $Repository.webUrl $RepositoryIsActive = -not $Repository.IsDisabled if ($RepositoryIsActive) { $Folder = Join-Path -Path $PWD -ChildPath $ProjectName if (-not (Test-Path $Folder)) { New-Item $Folder -ItemType Directory | Out-Null } Push-Location -Path $Folder Write-Host \u0026#34;${ProjectName}/${RepositoryName} ...\u0026#34; git clone --depth 1 $RepositoryWebUri Pop-Location } } "},{"id":49,"href":"/docs/programming/powershell/basics/","title":"Basics","section":"Powershell","content":" Powershell basics # "},{"id":50,"href":"/docs/programming/powershell/snippets/","title":"Snippets","section":"Powershell","content":" Powershell snippets # Get unix timestamp # [int](Get-Date -UFormat %s -Millisecond 0) Match glob pattern # function Test-GlobMatch { param ( [string] $value, [string] $pattern ) $position = 0 foreach ($char in $pattern.toCharArray()) { Switch ($char) { \u0026#39;?\u0026#39; { continue } \u0026#39;*\u0026#39; { foreach ($i in $value.Length .. $position) { if (Test-GlobMatch $value.Substring($i) $pattern.Substring($position + 1)) { return $True } } return $False } default { if ($value.Length -eq $position -or $pattern[$position] -ne $value[$position]) { return $False } } } $position++ } return $value.Length -eq $position } Remove git directories # Get-ChildItem -Recurse -Directory -Hidden -Include \u0026#39;.git\u0026#39; | Remove-Item -Force -Recurse Compress as standard zip # foreach ($item in (Get-ChildItem -Directory)) { 7z a -mm=Deflate -mfb=258 -mpass=15 -r \u0026#34;${item}.zip\u0026#34; \u0026#34;${item}/*\u0026#34; } Compress maximally # foreach ($item in (Get-ChildItem -Directory)) { 7z a -t7z -mx=9 -mfb=273 -ms -md=31 -myx=9 -mtm=- -mmt -mmtf -md=1536m -mmf=bt3 -mmc=10000 -mpb=0 -mlc=0 \u0026#34;${item}.7z\u0026#34; \u0026#34;${item}\u0026#34; } "},{"id":51,"href":"/docs/programming/python/basics/","title":"Basics","section":"Python","content":" Python basics # Application structure # TODO\nClass structure # TODO\nExecutable module # TODO\n"},{"id":52,"href":"/docs/programming/python/environment/","title":"Environment","section":"Python","content":" Virtual environment # TODO\nVenv # Create and activate virtual environment:\nBash echo Hello Powershell python -m venv .venv .venv\\Scripts\\activate.ps1 Cmd echo Hello Upgrade pip and install requirements.txt:\npython -m pip install --upgrade pip pip install -r requirements.txt Deactivate virtual environment:\ndeactivate Pipenv # TODO\n"},{"id":53,"href":"/docs/programming/python/installation/","title":"Installation","section":"Python","content":" Installation # PyEnv on Linux # Install build dependencies:\nsudo apt update sudo apt install \\ build-essential \\ libssl-dev \\ zlib1g-dev \\ libbz2-dev \\ libreadline-dev \\ libsqlite3-dev \\ curl \\ llvm \\ libncursesw5-dev \\ xz-utils \\ tk-dev \\ libxml2-dev \\ libxmlsec1-dev \\ libffi-dev \\ liblzma-dev Install PyEnv scripts:\ncurl https://pyenv.run | bash Add to ~/.profile:\nexport PATH=\u0026#34;$HOME/.pyenv/bin:$PATH\u0026#34; if command -v pyenv 1\u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then eval \u0026#34;$(pyenv init -)\u0026#34; fi PyEnv on Windows # Copy pyenv-win from https://github.com/pyenv-win/pyenv-win/archive/master.zip to E:\\pgm\\pyenv-win.\nEnvironment variables:\nName Value PYENV_HOME E:\\pgm\\pyenv-win PYENV_ROOT E:\\pgm\\pyenv-win PYENV E:\\pgm\\pyenv-win Add to PATH:\nE:\\pgm\\pyenv-win\\bin E:\\pgm\\pyenv-win\\shims List available versions: pyenv install -l\nInstall chosen version: pyenv install 3.11.4\nSet global Python version: pyenv global 3.11.4\nSet local Python version: pyenv local 3.11.4\nInstall basic packages # pip install invoke pip install pipenv Create environment variable PIPENV_VENV_IN_PROJECT=1.\nInstall additional packages # pip install jupyterlab pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117 "},{"id":54,"href":"/docs/programming/sqlite/data/","title":"Data","section":"SQLite","content":" Data manipulations # CVS data # Example data in data.csv:\nindex, code, name\r1, EX1, Example 1\r2, EX2, Example 2\r3, EX3, Example 3 Start new database:\nsqlite3 data.db Load data into table and delete headers in sqlite shell:\ncreate table data (index, code, name); .mode csv .headers ON .separator , .import data.csv data delete from data where index = \u0026#39;index\u0026#39;; select * from data limit 60; "},{"id":55,"href":"/docs/programming/typescript/nodejs/","title":"Nodejs","section":"Typescript","content":" NodeJS # Installation on Linux # git clone https://github.com/nvm-sh/nvm.git /opt/nvm cd /opt/nvm git checkout v0.39.3 Add to ~/.profile:\nexport NVM_DIR=\u0026#34;$HOME/.nvm\u0026#34; if [ -s \u0026#34;$NVM_DIR/nvm.sh\u0026#34; ]; then . \u0026#34;$NVM_DIR/nvm.sh\u0026#34; fi if [ -s \u0026#34;$NVM_DIR/bash_completion\u0026#34; ]; then . \u0026#34;$NVM_DIR/bash_completion\u0026#34; fi Install NodeJS:\nnvm install node nvm install-latest-npm npm install -g yarn Installation on Windows # Install NVM for Windows\nSettings:\nroot: D:\\pgm\\nvm path: D:\\pgm\\nodejs Install NodeJS:\nnvm list available nvm install latest nvm use newest Install yarn:\nnpm install -g yarn "},{"id":56,"href":"/docs/devops/aws/","title":"AWS","section":"Devops","content":" Amazon Web Services # Common # Regions, Availability Zones, Local Zones IAM (Identity and Access Management) Cloudwatch Logs CDK (Cloud Development Kit) and CloudFormation VPC (Virtual Private Cloud) S3 (Simple Storage Service) Backend # EC2 (Elastic Cloud Compute) Lambda functions ECS (Elastic Container Service) EKS (Elastic Kubernetes Service) Load balancers Certificate managers SNS (Simple Notification Service) SQS (Simple Queue Service) Step functions DynamoDB RDS (Relational Database Service) ElastiCache Frontend # Amplify CloudFront API Gateway Cognito DevOps # CodeCommit CodeBuild CodePipeline Cloudwatch Alarms Cloudwatch Monitoring Cloudwatch Dashboards Data Engineering # Glue Batch Redshift Athena Lake Formation "},{"id":57,"href":"/docs/devops/kubernetes/","title":"Kubernetes","section":"Devops","content":" Kubernetes # Container orchestration:\nHigh availability Scalability Disaster recovery Architecture # A node is a virtual or physical machine.\nA Kubernetes cluster is made from at least one master node and connected to it worker nodes. All nodes are connected by cluster\u0026rsquo;s Virtual Network. All of these components create one unified machine.\nKubernetes does not manage data persistence. It is easier to host databases outside of Kubernetes cluster.\nEach worker node has a kubelet process running. Kubelet is a node agent that allows cluster to communicate with nodes and execute tasks on them. Worker nodes have application containers deployed on them. Applications run on worker nodes.\nMaster node runs Control Plane processes necessary for managing the cluster:\nAPI Server (entry point to the cluster for kubernetes clients) Controller Manager (keeps track of what is happening in the cluster) Scheduler (decides on which worker node to schedule new Pod) etcd key-value store (keeps current configuration and state of the cluster) Components # Pod Abstraction of container runtime for one or more containers. Smallest unit of computing that is assigned an individual IP address and can be deployed and managed. Service A permanent IP address and load balancer for Pods. Publishes its own virtual address either as an environment variable in every Pod or, if cluster is using CoreDNS, as a DNS entry. Services can abstract access not only to Pods, but also to databases, external host or other services. Ingress (forwards requests using domain name to a specific Service) ConfigMap (external configuration for application) Secret (stores secret configuration data encoded in base64) Deployment An abstraction allowing to manage the state of a set of Pods or Replica Sets. It allows to run a group of identical Pods with a common configuration. Volume (external disk storage - local or remote - connected to the cluster) Job (???) CronJob (???) StatefulSet Provides unique network identifiers, persistent storage, ordered deployment and scaling. DaemonSet Ensures that all matching Nodes run a copy of a Pod. ReplicaSet A group of duplicated identical pods. ReplicationController (???) "},{"id":58,"href":"/docs/devops/macos/","title":"MacOS","section":"Devops","content":"Install:\nHomebrew Taskfile Multipass Terraform Terragrunt "}]