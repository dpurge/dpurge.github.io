[{"id":0,"href":"/docs/devops/ansible/installation/","title":"Installation","section":"Ansible","content":" Installation # Install Ansible:\nsudo apt-add-repository ppa:ansible/ansible sudo apt update sudo apt install ansible "},{"id":1,"href":"/docs/devops/argo/installation/","title":"Installation","section":"ArgoCD","content":" Installation # Install ArgoCD:\nkubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml helm repo add argo https://argoproj.github.io/argo-helm Forward port:\nkubectl get all -n argocd kubectl port-forward service/argocd-server -n argocd 8080:443 Extract credential:\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d Login from CLI:\nkubectl port-forward svc/argocd-server -n argocd 8080:443 argocd login 127.0.0.1:8080 Install application:\nargocd app create nginx-prod \\ --repo https://github.com/dpurge/argo-config.git \\ --path nginx/overlays/prod \\ --dest-server https://kubernetes.default.svc \\ --dest-namespace prod Cheatsheet:\nargocd app create # Create a new Argo CD application. argocd app list # List all applications in Argo CD. argocd app logs \u0026lt;appname\u0026gt; # Get the application’s log output. argocd app get \u0026lt;appname\u0026gt; # Get information about an Argo CD application. argocd app diff \u0026lt;appname\u0026gt; # Compare the application’s configuration to its source repository. argocd app sync \u0026lt;appname\u0026gt; # Synchronize the application with its source repository. argocd app history \u0026lt;appname\u0026gt; # Get information about an Argo CD application. argocd app rollback \u0026lt;appname\u0026gt; # Rollback to a previous version argocd app set \u0026lt;appname\u0026gt; # Set the application’s configuration. argocd app delete \u0026lt;appname\u0026gt; # Delete an Argo CD application. Cleanup:\nkubectl delete CustomResourceDefinition applications.argoproj.io kubectl delete CustomResourceDefinition applicationsets.argoproj.io kubectl delete CustomResourceDefinition appprojects.argoproj.io Examples:\nhelm template infra-cluster001/ | kubectl apply -f - argocd app create argo-cd --repo https://xxx/homelab.git --path environments/dev/argo-cd --dest-server https://kubernetes.default.svc --dest-namespace argocd argocd proj create infra --description \u0026#39;Infrastructure\u0026#39; --dest \u0026#39;*,*\u0026#39; --src \u0026#39;*\u0026#39; --allow-cluster-resource \u0026#39;*/*\u0026#39; argocd repocreds add https://xxx/_git/infrastructure-helm-charts --username infrastructure-helm-charts --password xxxxxxxxxxxx argocd app create root --repo https://xxx/_git/infrastructure-helm-charts --path infra-cluster001 --dest-namespace default --dest-server https://kubernetes.default.svc "},{"id":2,"href":"/docs/devops/aws/architecture/","title":"Architecture","section":"AWS","content":" Learn architecture # Exercises in simple application architecture design.\nTutorial: AWS Prescriptive Guidance\nStatic website # Web application with relational database # Use Fargate for ECS.\nData processing # Serverless data source monitoring # Problems:\nMonitoring Statistics Secrets management Configuration for Lambda functions Feature: process changes in the data source Scenario: data source has changes Given for each $currentValue in $batchOfChanges And $previousValue of $currentValue When $currenValue != $previousValue Then save $currentValue in the data index Scenario: data source has no changes Given new batch of changes When current value equals previous value Feature: notify users about interesting changes Scenario: the change is interesting Given data source has changed When the change is small Then do nothing Scenario: the change is not interesting Given data source has changed When the change is big Then send email to the user Feature: show monitoring dashboard Scenario: new event saved to data index "},{"id":3,"href":"/docs/devops/aws/aws-cli/","title":"Aws CLI","section":"AWS","content":" AWS command line interface # GitHub project "},{"id":4,"href":"/docs/devops/azure/exploring/","title":"Exploring","section":"Azure","content":" Exploring Azure resources # List subscription and tenant # az account show --query \u0026#34;{subscriptionId:id, tenantId:tenantId}\u0026#34; -o table az login az account subscription list Create service principal for RBAC # az ad sp create-for-rbac --role Contributor --name my-name-001 --scope /subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx az cloud set -n AzureCloud az login --service-principal -u *** --password=*** --tenant xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx --allow-no-subscriptions az account set --subscription xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx az aks get-credentials --resource-group $(ResourceGroup) --name $(KubernetesService) Find VM images # az vm image list-offers --publisher Canonical --location westeurope -o table az vm image list-skus --publisher Canonical --offer 0001-com-ubuntu-server-jammy --location westeurope -o table az vm image list --all --publisher Canonical --offer 0001-com-ubuntu-server-jammy --sku 22_04-lts --location westeurope -o table Refer to this image: Canonical:0001-com-ubuntu-server-jammy:22_04-lts\nList public and private IP # For current subscription:\naz vm list -d --query \u0026#34;[].{Name:name, PublicIPs:publicIps, PrivateIPs:privateIps}\u0026#34; -o table For all subscriptions:\nfor i in `az account list --query \u0026#34;[].{id:id}\u0026#34; --output tsv`; do az account set --subscription $i; az vm list -d --query \u0026#34;[].{Name:name, PublicIPs:publicIps, PrivateIPs:privateIps}\u0026#34; --output tsv; done Examples:\naz vmss show --resource-group ResGrp-xxx --name DEV-VMSS001 --query [sku] --output table az acr login --name dockercr001 Invoke-RestMethod -Headers @{\u0026#34;Metadata\u0026#34;=\u0026#34;true\u0026#34;} -Method GET -NoProxy -Uri \u0026#34;http://x.x.x.x/metadata/instance?api-version=2021-02-01\u0026#34; | ConvertTo-Json -Depth 64 $env:ARM_ACCESS_KEY = $(az storage account keys list --resource-group ResGrp-Terraform --account-name xxxx --query \u0026#39;[0].value\u0026#39; -o tsv) if ((kubectl get ns -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;).split(\u0026#39; \u0026#39;) -contains \u0026#39;keda\u0026#39;) {\u0026#34;Keda is installed\u0026#34;} $ctx = (Get-AzStorageAccount -ResourceGroupName ResGrp-xxx -Name xxx).Context $StgTable = Get-AzStorageTable -Name xxx -Context $Ctx foreach ($Row in (Get-AzTableRow -table $StgTable.CloudTable)) { $VMSize = $Row.VMSize $BuildTime = $Row.FinishTime - $Row.StartTime Write-Host (\u0026#39;{0}: {1:hh} hr {1:mm} min {1:ss} sec\u0026#39; -f $VMSize,$BuildTime) } "},{"id":5,"href":"/docs/devops/azure/snippets/","title":"Snippets","section":"Azure","content":" Snippets # az group create -g $group -l northeurope az vm create \\ -n vm1 \\ -g $group \\ -l northeurope \\ --image Win2019Datacenter \\ --admin-username $username \\ --admin-password $password \\ --nsg-rule rdp az appservice plan create \\ -n web-plan \\ -g $group \\ -l northeurope \\ --sku S1 az webapp create \\ -n $appname \\ -g $group \\ -p web-plan az webapp list -g $group --query \u0026#34;[].enabledHostNames\u0026#34; -o jsonc az group delete -g $group "},{"id":6,"href":"/docs/devops/blueprints/aws-3tier-webapp/","title":"Aws 3tier Webapp","section":"Blueprints","content":" AWS 3-Tier webapp # "},{"id":7,"href":"/docs/devops/blueprints/aws-static-webapp/","title":"Aws Static Webapp","section":"Blueprints","content":" AWS static web application # Components:\nAWS Shield Standard - free protection against common DDoS attacks, applied automatically to CloudFront and Route 53. Route 53 - Domain Name System web service, performs domain registration, DNS routing and health checking. Hosted Zone - container for records specifying how to route traffic for a specific domain. CloudFront - content delivery network for distributing static content from edge locations. S3 bucket - cloud storage for web content Argo Workflows - pipelines producing web content and publishing it to the cloud storage "},{"id":8,"href":"/docs/devops/blueprints/llm-closed-source/","title":"Llm Closed Source","section":"Blueprints","content":" Large language model (closed source) # Business logic # LangChain LlamaIndex Streamlit Vector database # ChromaDB Qdrant PostgreSQL/ PgVector Data ingestion # Spark Airflow Cache # Redis Feature store # Feast Databricks feature store Prompt management # PromptHub Observability # Arize LLM provider # OpenAI ChatGPT Anthropic Claude "},{"id":9,"href":"/docs/devops/docker/azure/","title":"Azure","section":"Docker","content":" Azure from docker # Access to the storage account # Dockerfile:\nFROM ubuntu:22.04 RUN apt-get update \u0026amp;\u0026amp; apt install -y ca-certificates pkg-config libfuse-dev cmake libcurl4-gnutls-dev libgnutls28-dev uuid-dev libgcrypt20-dev wget RUN wget https://packages.microsoft.com/config/ubuntu/20.04/packages-microsoft-prod.deb \\ \u0026amp;\u0026amp; dpkg -i packages-microsoft-prod.deb \\ \u0026amp;\u0026amp; rm packages-microsoft-prod.deb RUN apt-get update RUN apt-get install -y blobfuse fuse RUN mkdir /storage-container COPY fuse_connection.cfg /fuse_connection.cfg Contents of fuse_connection.cfg:\naccountName mystgaccnt accountKey xxxxxxxxxxxxxxxxxxxxxx containerName test-data Build and start the container:\ndocker build -t azure-data . docker run -it --rm --cap-add SYS_ADMIN --device /dev/fuse azure-data bash Create blob in test-data container:\nblobfuse /storage-container --tmp-path=/mnt/resource/blobfusetmp --config-file=/fuse_connection.cfg -o attr_timeout=240 -o entry_timeout=240 -o negative_timeout=120 mkdir /storage-container/test echo \u0026#34;hello world\u0026#34; \u0026gt; test/hello-blob.txt In another instance, read that data:\nblobfuse /storage-container --tmp-path=/mnt/resource/blobfusetmp --config-file=/fuse_connection.cfg -o attr_timeout=240 -o entry_timeout=240 -o negative_timeout=120 cat /storage-container/test/hello-blob.txt Delete blob and the directory:\nrm /storage-container/test/hello-blob.txt rmdir /storage-container/test Example:\naz login az account set --subscription \u0026#34;DEV environment\u0026#34; az acr login -n devdockercr001 docker pull devdockercr001.azurecr.io/devops-tools docker pull postgres docker run -e POSTGRES_PASSWORD=postgres -e POSTGRES_USER=postgres -d -p 5432:5432 postgres "},{"id":10,"href":"/docs/devops/docker/basics/","title":"Basics","section":"Docker","content":" Docker basics # "},{"id":11,"href":"/docs/devops/docker/compose/","title":"Compose","section":"Docker","content":" Docker compose # CI/CD test environment # version: 3.8 networks: pipeline: external: false services: postgres: image: postgres container_name: postgres restart: always environment: - POSTGRES_USER=postgres - POSTGRES_PASSWORD=postgres networks: - pipeline ports: - 5432:5432 volumes: - type: volume source: postgres target: /var/lib/postgresql/data - type: bind source: ./init.sql target: /docker-entrypoint-initdb.d/init.sql pgadmin: image: dpage/pgadmin4 container_name: pgadmin environment: - PGADMIN_DEFAULT_EMAIL=jdp@example.com - PGADMIN_DEFAULT_PASSWORD=pgadmin networks: - pipeline ports: - 15433:80 volumes: - type: volume source: pgadmin target: /var/lib/pgadmin depends_on: - postgres gitea: image: gitea/gitea container_name: gitea restart: always environment: - USER_UID=1000 - USER_GID=1000 - GITEA__database__DB_TYPE=postgres - GITEA__database__HOST=postgres:5432 - GITEA__database__NAME=gitea - GITEA__database__USER=gitea - GITEA__database__PASSWD=gitea networks: - pipeline ports: - 3000:3000 - 222:22 volumes: - type: volume source: gitea target: /data depends_on: - postgres jenkins: image: jenkins/jenkins container_name: jenkins pribileged: true user: root restart: always environment: - DOCKER_HOST=unix:///var/run/docker.sock networks: - pipeline ports: - 8080:8080 - 50000:50000 volumes: - type: bind source: /var/run/docker.sock target: /var/run/docker.sock - type: volume source: jenkins target: /var/jenkins_home volumes: postgres: pgadmin: gitea: jenkins: .env file:\nCOMPOSE_CONVERT_WINDOWS_PATHS=1 init.sql file:\nCREATE ROLE gitea WITH ENCRYPTED PASSWORD \u0026#39;gitea\u0026#39;; ALTER ROLE gitea WITH LOGIN; CREATE DATABASE gitea; GRANT ALL PRIVILEGES ON DATABASE gitea TO gitea; \\c gitea postgres GRANT ALL ON SCHEMA public TO gitea; MSSQL Server # ./db/Dockerfile:\nFROM mcr.microsoft.com/mssql/server:2019-latest WORKDIR /usr/src/app COPY ./entrypoint.sh /usr/src/app EXPOSE 1432 EXPOSE 1433 USER root CMD /bin/bash ./entrypoint.sh ./db/entrypoint.sh:\n/opt/mssql/bin/sqlservr ./docker-compose.yaml:\nversion: \u0026#34;3\u0026#34; services: db: restart: always build: context: ./db dockerfile: ./Dockerfile environment: SA_PASSWORD: p@ssw0rd ACCEPT_EULA: \u0026#34;Y\u0026#34; ports: - \u0026#34;1433:1433\u0026#34; "},{"id":12,"href":"/docs/devops/docker/installation/","title":"Installation","section":"Docker","content":" Docker installation # Windows # TODO\nLinux # TODO\nWSL # Enable integration in Docker Desktop: Settings -\u0026gt; Resources -\u0026gt; WSL Integration\n"},{"id":13,"href":"/docs/devops/docker/volumes/","title":"Volumes","section":"Docker","content":" Docker volumes # Docker Desktop for Windows stores volumes in the WSL instance docker-desktop-data. It is used by Docker Desktop backend to store images and volumes.\nTo export it:\nwsl --export docker-desktop-data docker-desktop-data.tar Creating and removing volumes # docker volume create jdp_src docker volume rm jdp_src Mounting volumes # Mounting simple volumes:\ndocker run -it --rm --volume jdp_src:/src alpine docker run -it --rm --mount source=jdp_src,target=/src alpine Mounting from WSL:\ndocker run -it --rm --volume \u0026#34;\\\\wsl$\\Ubuntu\\var\\docker\\volumes\\alpine_persistent_data:/data\u0026#34; alpine "},{"id":14,"href":"/docs/devops/gcp/cloud-shell/","title":"Cloud Shell","section":"GCP","content":" Cloud shell # Customize in ~/.customize_environment (edit .customize_environment):\n#!/bin/sh TERRAFORM_VERSION=\u0026#34;0.12.0\u0026#34; curl https://releases.hashicorp.com/terraform/${TERRAFORM_VERSION}/terraform_${TERRAFORM_VERSION}_linux_amd64.zip \u0026gt; terraform_${TERRAFORM_VERSION}_linux_amd64.zip unzip -o terraform_${TERRAFORM_VERSION}_linux_amd64.zip -d /usr/local/bin Editor is based on Eclipse Theia.\n"},{"id":15,"href":"/docs/devops/gcp/definitions/","title":"Definitions","section":"GCP","content":" Definitions # What is cloud computing? The delivery of a shared pool of on-demand computing services over the public internet, that can be rapidly provisioned and released with minimal management effort or service provider interaction. provision resources automatically without requiring human interaction available over the network pooled resources to support a multi-tenant model allowing multiple customers to share the same applications or the same physical infrastructure rapidly provision and de-provision any of the cloud computing resources resource usage can be monitored, controlled and reported using metering capabilities Zone a deployment area within a region; the smallest entity of global infrastructure Region an indepenedent geographic area holding a collection of zones Multi-Region large geographic area holding two or more regions Compute # Compute Engine (IaaS) virtual machines called instances deployed from public or private images in a specific region and zone; pre-configured images on Google Cloud Marketplace; multiple instances managed with instance groups and autoscaling; attach/detach additional disks; can use Google Cloud Storage; accessed with ssh GKE (Google Kubernetes Engine) (CaaS) container orchestrating system; built on open source Kubernetes; can integrate with on-premise Kubernetes; uses Compute Engine instances as nodes in a cluster App Engine (PaaS) managed serverless platform for hosting web applications at scale; provisions and patches servers and scales the application instances based on demand; build your app in Go, Java, .NET, NodeJS, PHP, Python, Ruby; seamlessly connects with Google services (storage, databases, \u0026hellip;); can connect to other cloud providers or external databases; integrates with Web Security Scanner to identify vulnerabilities Cloud Functions (FaaS) serverless execution environment; simple, single-purpose functions triggered when a watched event is fired, executing in a fully managed environment; written in Java Script, Python 3, Go, Java; good for ETL, webhooks, APIs, mobile backend functions Cloud Run (FaaS) serverless for containers; managed compute platform for deploying and scaling containerized applications; built upon an open standard Knative; abstracts away all infrastructure management; any language, library or binary Storage # Cloud Storage consistent, scalable, large capacity, highly durable object storage; unlimited storage with no minimum object size; eleven nines durability (99.999999999%); excellent for content delivery, data lakes, backups; comes in different storage classes (standard, nearline, coldline, archive) and availability (region, dual-region, multi-region) Filestore fully managed NFS file server compliant with NFSv3; stores data for running applications in VM instances or Kubernetes clusters Persistent Disks durable block storage for instances; comes in two options (standard, solid state); available in zonal and regional option Databases # SQL/relational # Cloud SQL managed PostgreSQL, MySQL or SQL Server; high availability across zones Cloud Spanner scalable relational database service; supports transactions, strong consistency, synchronous replication; high availability across regions and globally NoSQL # Bigtable fully managed, scalable NoSQL DB; high throughput with low latency; cluster resizing without downtime Datastore fast, fully managed, serverless, NoSQL document database; for mobile, web and IoT apps; multi-region replication; ACID transactions Firestore NoSQL realtime database optimized for offline us; cluster resizing without downtime Memorystore fully managed, highly available in-memory service for Redis and Memcached Networking # VPC (Virtual Private Cloud) core networking service, virtualized network within Google Cloud; global resource; each VPC contains a default network; additional networks can be created in your project, but networks cannot be shared between projects Firewall Rules a global distributed firewall that governs traffic comming into instances on a network; default network has a default set of firewall rules; custom rules can be created Routes advanced networking functions for instances; specifies how packets leaving an instance should be directed Load Balancing # Distribution of workloads across multiple instances.\nHTTP(S) Load Balancing distribute traffic across regions to ensure that requests are routed to the closest region or a healthy instance in the next closest region; distribute traffic based on content type Network Load Balancing distribute traffic among server instances in the same region based on incomming IP protocol data (address, port, protocol) Cloud DNS # Google Cloud DNS publish and maintain DNS records Advanced Connectivity # Cloud VPN connect your existing network to your VPC through an IPsec connection Direct Interconnect connect your existing network to your VPC using a highly available, low latency, enterprise grade connection; does not go over public internet, connects to the nearest Google backbone Direct Peering exchange internet traffic between your network and Google at Google edge network location Carrier Peering connect your infratsructure to Google network edge through highly available, lower latency connection from a service provider Resource hierarchy # Google Cloud resources are organized hierarchically using a parent/child relationship designed to map organizational structure to Google Cloud, so that it can be used to manage permissions and access control.\nPolicies are controlled by IAM. Access control policies and configuration settings on a parent resource are inherited by the child. Each child object has exactly one parent.\ncloud level Payments Profile Billing Account Domain (organizations, users, policies) account level Organization (root node, closely integrated with the domain) Folders (grouping mechanism and isolation boundary, eg. departments, teams, products) Projects (core organizational component; required to use resources; parents resources) Project Labels service level Resources (any service-level resource, eg. VM, storage bucket, databases, \u0026hellip;) Resource Labels Labels:\ncategorize resurces by using a key/value pair [https://console.cloud.google.com/freetrial]\n"},{"id":16,"href":"/docs/devops/gcp/functions/","title":"Functions","section":"GCP","content":" Cloud functions # Manual depoyment # gcloud functions deploy my-function \\ --source https://source.developers.google.com/projects/test1/repos/github_example_functions/moveable-aliases/main/paths/my-function \\ --runtime nodejs16 \\ --trigger-http \\ --entry-point helloWorld When redeploying, do not specify runtime or trigger.\nCloud Build deployment # Get project ID and project number and grant permissions:\ngcloud projects describe test1 gcloud iam service-accounts add-iam-policy-binding PROJECT_ID@appspot.gserviceaccount.com --member PROJECT_NUMBER@cloudbuild.gserviceaccount.com --role roles/iam.serviceAccountUser gcloud projects add-iam-policy-binding PROJRCT_ID --member serviceAccount:PROJECT_NUMBER@cloudbuild.gserviceaccount.com --role roles/cloudfunctions.developer Create cloudbuild.yaml:\n- steps: - name: \u0026#39;gcr.io/cloud-builders/npm\u0026#39; args: [\u0026#39;install\u0026#39;] dir: \u0026#39;my-function\u0026#39; - name: \u0026#39;gcr.io/cloud-builders/npm\u0026#39; args: [\u0026#39;test\u0026#39;] dir: \u0026#39;my-function\u0026#39; - name: \u0026#39;gcr.io/cloud-bulders/gcloud\u0026#39; args: [\u0026#39;functions\u0026#39;, \u0026#39;deploy\u0026#39;, \u0026#39;my-function\u0026#39;, \u0026#39;--trigger-http\u0026#39;, \u0026#39;--runtime\u0026#39;, \u0026#39;nodejs16\u0026#39;, \u0026#39;--entry-point\u0026#39;, \u0026#39;helloWorld\u0026#39;] dir: \u0026#39;my-function\u0026#39; Terraform deployment # locals { project_id = \u0026#34;test1\u0026#34; timestamp = formatdate(\u0026#34;YYMMDDhhmmss\u0026#34;, timestamp()) } provider \u0026#34;google\u0026#34; { project = local.project_id region = \u0026#34;us-central1\u0026#34; } data \u0026#34;archive_file\u0026#34; \u0026#34;this\u0026#34; { type = \u0026#34;zip\u0026#34; source_dir = \u0026#34;my-function\u0026#34; output_path = \u0026#34;/tmp/functions-${local.timestamp}.zip\u0026#34; } resource \u0026#34;google_storage_bucket\u0026#34; \u0026#34;this\u0026#34; { name = \u0026#34;${local.project_id}-functions\u0026#34; location = \u0026#34;US\u0026#34; } resource \u0026#34;google_storage_bucket_object\u0026#34; \u0026#34;this\u0026#34; { name = \u0026#34;functions.zip#${data.archive_file.this.output_md5}\u0026#34; bucket = google_storage_bucket.this.name source = data.archive_file.this.output_path } resource \u0026#34;google_cloudfunctions_function\u0026#34; \u0026#34;this\u0026#34; { name = \u0026#34;my-function\u0026#34; runtime = \u0026#34;nodejs16\u0026#34; available_memory_mb = 128 source_archive_bucket = google_storage_bucket.this.name source_archive_object = google_storage_bucket_object.this.name trigger_http = true entry_point = \u0026#34;helloWorld\u0026#34; } resource \u0026#34;google_cloudfunctions_function_iam_member\u0026#34; \u0026#34;invoker\u0026#34; { project = google_cloudfunctions_function.this.project region = google_cloudfunctions_function.this.region cloud_function = google_cloudfunctions_function.this.name role = \u0026#34;roles/cloudfunctions.invoker\u0026#34; member = \u0026#34;allUsers\u0026#34; } output \u0026#34;function_url\u0026#34; { value = google_cloudfunctions_function.this.https_trigger_url } NodeJS setup # npm install --save-dev @google-cloud/functions-framework Specify entry point in package.json:\n{ \u0026#34;name\u0026#34;: \u0026#34;my-function\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.0.1\u0026#34;, \u0026#34;devDependencies\u0026#34;: { \u0026#34;@google-cloud/functions-framework\u0026#34;: \u0026#34;^2.1.0\u0026#34; }, \u0026#34;script\u0026#34;: { \u0026#34;start\u0026#34;: \u0026#34;npx functions-framework --target=helloWorld --signature-type=http --port=8080\u0026#34; } } Body of the function in index.js:\nexports.helloWorld = (req, res) =\u0026gt; { res.send(\u0026#39;Hello World!\u0026#39;); } Run:\nnpm start curl localhost:8080 API Gateway # For API Gateway create service accounts.\nWrite API description in openapi2-functions.yaml to upload:\n--- swagger: \u0026#39;2.0\u0026#39; info: title: test-name-spec description: Sample API on API Gateway version: 1.0.0 schemes: - https produces: - application/json paths: /helloWorld: get: summary: Greet a user operationId: hello x-google-backend: address: https://us-central1-....cloudfunctions.net/my-function responses: \u0026#39;200\u0026#39;: description: A successful response schema: type: string "},{"id":17,"href":"/docs/devops/gcp/gcloud/","title":"Gcloud","section":"GCP","content":" GCloud # Console # Cloud console\nInstall # Google documentation\n# using snapcraft sudo systemctl status snapd snap search google-cloud-sdk sudo snap install google-cloud-sdk --classic sudo snap remove google-cloud-sdk rm -rf ~/.config/gcloud # check which gcloud which gsutil which bq gcloud version gcloud help Initialize environment # gcloud init # or: gcloud init --console-only To switch active account:\ngcloud auth list gcloud config set account dpurge@example.com Get information # gcloud info gcloud config list gcloud config configurations list Modify installation # gcloud components list gcloud components install COMPONENT gcloud components remove COMPONENT gcloud components update Beta commands interactive shell # gcloud components install beta gcloud beta interactive Switch project # gcloud projects list gcloud config get-value project gcloud config set project PROJECT_ID Switch account # gcloud config configurations list gcloud config configurations activate ACCOUNT ls ~/.config/gcloud/configurations gcloud config configurations describe ACCOUNT # you can delete non-active configuration gcloud config configurations delete ACCOUNT Connect to Cloud Shell # # create key-pair and connect gcloud alpha cloud-shell ssh Create a VPC # In Google Cloud VPC is global, it spans multiple regions. Subnets are regional and span multiple zones within a region. This is different from AWS.\nSubnets have unique names across all VPCs. You need to allow access to the subnet by creating firewall rules.\ngcloud compute networks create vpc-1 --description \u0026#34;Testing\u0026#34; --subnet-mode custom gcloud compute firewall-rules create vpc1-fw-allow-ssh --network vpc-1 --allow tcp:22 gcloud compute networks subnets create vpc1-euw2-1 --network vpc-1 --region europe-west2 --range 10.0.1.0/24 gcloud compute networks list gcloud compute networks subnets list gcloud compute networks subnets list --network vpc-1 Subnets and networks can be deleted if there are no resources running in them.\ngcloud compute networks subnets delete vpc1-euw2-1 --region europe-west2 gcloud compute firewall-rules delete vpc1-fw-allow-ssh gcloud compute networks delete vpc-1 Virtual machines # Connect to the VM:\ngcloud compute instances list gcloud compute ssh vm-1 Stop the VM before creating a disk snapshot.\n# todo Managed instance groups # create instance template create instance groups and add instances to it # todo # gcloud compute instance-templates create my-template --image-family centos-7 --machine-type f2-micro # gcloud compute target-pools create MY_POOL --target-http-proxies PROXY # gcloud compute instance-groups managed create my-ig --zone us-central1-a —target-size TARGET_SIZE Instance group autoscaling # # todo "},{"id":18,"href":"/docs/devops/gcp/gke-workload-identity/","title":"Gke Workload Identity","section":"GCP","content":" GKE Workload Identity # Source: First create the bucket using console of gcloud command. Name must be globally unique. Separate different environments in different buckets. Use multi-region for the location type, standard for the storage class, enable object versioning under protection tools.\n# 1-provider.tf # https://registry.terraform.io/providers/hashicorp/google/latest/docs provider \u0026#34;google\u0026#34; { project = \u0026#34;devops-v4\u0026#34; region = \u0026#34;us-central1\u0026#34; } # https://www.terraform.io/language/settings/backends/gcs terraform { backend \u0026#34;gcs\u0026#34; { bucket = \u0026#34;tf-state-staging\u0026#34; prefix = \u0026#34;terraform/state\u0026#34; } required_providers { google = { source = \u0026#34;hashicorp/google\u0026#34; version = \u0026#34;~\u0026gt; 4.0\u0026#34; } } } Enable API services and create VPC:\n# 2-vpc.tf # https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/google_project_service resource \u0026#34;google_project_service\u0026#34; \u0026#34;compute\u0026#34; { service = \u0026#34;compute.googleapis.com\u0026#34; } resource \u0026#34;google_project_service\u0026#34; \u0026#34;container\u0026#34; { service = \u0026#34;container.googleapis.com\u0026#34; } # https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/compute_network resource \u0026#34;google_compute_network\u0026#34; \u0026#34;main\u0026#34; { name = \u0026#34;main\u0026#34; routing_mode = \u0026#34;REGIONAL\u0026#34; auto_create_subnetworks = false mtu = 1460 delete_default_routes_on_create = false depends_on = [ google_project_service.compute, google_project_service.container ] } Create subnets:\n# 3-subnets.tf # https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/compute_subnetwork resource \u0026#34;google_compute_subnetwork\u0026#34; \u0026#34;private\u0026#34; { name = \u0026#34;private\u0026#34; ip_cidr_range = \u0026#34;10.0.0.0/18\u0026#34; region = \u0026#34;us-central1\u0026#34; network = google_compute_network.main.id private_ip_google_access = true secondary_ip_range { range_name = \u0026#34;k8s-pod-range\u0026#34; ip_cidr_range = \u0026#34;10.48.0.0/14\u0026#34; } secondary_ip_range { range_name = \u0026#34;k8s-service-range\u0026#34; ip_cidr_range = \u0026#34;10.52.0.0/20\u0026#34; } } Create cloud router:\n# 4-router.tf # https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/compute_router resource \u0026#34;google_compute_router\u0026#34; \u0026#34;router\u0026#34; { name = \u0026#34;router\u0026#34; region = \u0026#34;us-central1\u0026#34; network = google_compute_network.main.id } NAT:\n# 5-nat.tf # https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/compute_router_nat resource \u0026#34;google_compute_router_nat\u0026#34; \u0026#34;nat\u0026#34; { name = \u0026#34;nat\u0026#34; router = google_compute_router.router.name region = \u0026#34;us-central1\u0026#34; source_subnetwork_ip_ranges_to_nat = \u0026#34;LIST_OF_SUBNETWORKS\u0026#34; nat_ip_allocate_option = \u0026#34;MANUAL_ONLY\u0026#34; subnetwork { name = google_compute_subnetwork.private.id source_ip_ranges_to_nat = [\u0026#34;ALL_IP_RANGES\u0026#34;] } nat_ips = [google_compute_address.nat.self_link] } # https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/compute_address resource \u0026#34;google_compute_address\u0026#34; \u0026#34;nat\u0026#34; { name = \u0026#34;nat\u0026#34; address_type = \u0026#34;EXTERNAL\u0026#34; network_tier = \u0026#34;PREMIUM\u0026#34; depends_on = [google_project_service.compute] } Firewall:\n# 6-firewalls.tf # https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/compute_firewall resource \u0026#34;google_compute_firewall\u0026#34; \u0026#34;allow-ssh\u0026#34; { name = \u0026#34;allow-ssh\u0026#34; network = google_compute_network.main.name allow { protocol = \u0026#34;tcp\u0026#34; ports = [\u0026#34;22\u0026#34;] } source_ranges = [\u0026#34;0.0.0.0/0\u0026#34;] } Kubernetes:\nAvailability zones go down often Monitor costs; logging can cost more than infrastructure You cannot disable upgrades for the Kubernetes control plane # 7-kubernetes.tf # https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/container_cluster resource \u0026#34;google_container_cluster\u0026#34; \u0026#34;primary\u0026#34; { name = \u0026#34;primary\u0026#34; location = \u0026#34;us-central1-a\u0026#34; remove_default_node_pool = true initial_node_count = 1 network = google_compute_network.main.self_link subnetwork = google_compute_subnetwork.private.self_link logging_service = \u0026#34;logging.googleapis.com/kubernetes\u0026#34; monitoring_service = \u0026#34;monitoring.googleapis.com/kubernetes\u0026#34; networking_mode = \u0026#34;VPC_NATIVE\u0026#34; # Optional, if you want multi-zonal cluster node_locations = [ \u0026#34;us-central1-b\u0026#34; ] addons_config { http_load_balancing { disabled = true } horizontal_pod_autoscaling { disabled = false } } release_channel { channel = \u0026#34;REGULAR\u0026#34; } workload_identity_config { workload_pool = \u0026#34;PROJECT_ID.svc.id.goog\u0026#34; } ip_allocation_policy { cluster_secondary_range_name = \u0026#34;k8s-pod-range\u0026#34; services_secondary_range_name = \u0026#34;k8s-service-range\u0026#34; } private_cluster_config { enable_private_nodes = true enable_private_endpoint = false master_ipv4_cidr_block = \u0026#34;172.16.0.0/28\u0026#34; } # Jenkins use case # master_authorized_networks_config { # cidr_blocks { # cidr_block = \u0026#34;10.0.0.0/18\u0026#34; # display_name = \u0026#34;private-subnet-w-jenkins\u0026#34; # } # } } # 8-node-pools.tf # https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/google_service_account resource \u0026#34;google_service_account\u0026#34; \u0026#34;kubernetes\u0026#34; { account_id = \u0026#34;kubernetes\u0026#34; } # https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/container_node_pool resource \u0026#34;google_container_node_pool\u0026#34; \u0026#34;general\u0026#34; { name = \u0026#34;general\u0026#34; cluster = google_container_cluster.primary.id node_count = 1 management { auto_repair = true auto_upgrade = true } node_config { preemptible = false machine_type = \u0026#34;e2-small\u0026#34; labels = { role = \u0026#34;general\u0026#34; } service_account = google_service_account.kubernetes.email oauth_scopes = [ \u0026#34;https://www.googleapis.com/auth/cloud-platform\u0026#34; ] } } resource \u0026#34;google_container_node_pool\u0026#34; \u0026#34;spot\u0026#34; { name = \u0026#34;spot\u0026#34; cluster = google_container_cluster.primary.id management { auto_repair = true auto_upgrade = true } autoscaling { min_node_count = 0 max_node_count = 10 } node_config { preemptible = true machine_type = \u0026#34;e2-small\u0026#34; labels = { team = \u0026#34;devops\u0026#34; } taint { key = \u0026#34;instance_type\u0026#34; value = \u0026#34;spot\u0026#34; effect = \u0026#34;NO_SCHEDULE\u0026#34; } service_account = google_service_account.kubernetes.email oauth_scopes = [ \u0026#34;https://www.googleapis.com/auth/cloud-platform\u0026#34; ] } } gcloud auth application-default login terraform init terraform apply gcloud container clusters get-credentials primary --zone ... --project ... kubectl get svc kubectl get nodes Examples:\nAutoscaling:\n--- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment namespace: default spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 tolerations: - key: instance_type value: spot effect: NoSchedule operator: Equal affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: team operator: In values: - devops podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - nginx topologyKey: kubernetes.io/hostname kubectl apply -f example1.yaml watch kubectl get pods watch kubectl get nodes kubectl describe pod nginx-deployment-... Identity and access to list buckets:\n# 9-service-account.tf # https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/google_service_account resource \u0026#34;google_service_account\u0026#34; \u0026#34;service-a\u0026#34; { account_id = \u0026#34;service-a\u0026#34; } # https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/google_project_iam resource \u0026#34;google_project_iam_member\u0026#34; \u0026#34;service-a\u0026#34; { project = \u0026#34;devops-v4\u0026#34; role = \u0026#34;roles/storage.admin\u0026#34; member = \u0026#34;serviceAccount:${google_service_account.service-a.email}\u0026#34; } # https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/google_service_account_iam resource \u0026#34;google_service_account_iam_member\u0026#34; \u0026#34;service-a\u0026#34; { service_account_id = google_service_account.service-a.id role = \u0026#34;roles/iam.workloadIdentityUser\u0026#34; member = \u0026#34;serviceAccount:devops-v4.svc.id.goog[staging/service-a]\u0026#34; } --- apiVersion: v1 kind: Namespace metadata: name: staging --- apiVersion: v1 kind: ServiceAccount metadata: annotations: iam.gke.io/gcp-service-account: service-a@devops-v4.iam.gserviceaccount.com name: service-a namespace: staging --- apiVersion: apps/v1 kind: Deployment metadata: name: gcloud namespace: staging spec: replicas: 1 selector: matchLabels: app: gcloud template: metadata: labels: app: gcloud spec: serviceAccountName: service-a containers: - name: cloud-sdk image: google/cloud-sdk:latest command: [ \u0026#34;/bin/bash\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;--\u0026#34; ] args: [ \u0026#34;while true; do sleep 30; done;\u0026#34; ] affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: iam.gke.io/gke-metadata-server-enabled operator: In values: - \u0026#34;true\u0026#34; terraform apply kubectl apply -f example2.yaml kubectl get ns kubectl get pods -n staging kubectl get sa -n staging kubectl exec -n staging -it gloud-... -- bash gcloud alpha storage ls Nginx ingress controller with Helm:\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update helm search repo nginx # nginx-values.yaml --- controller: config: compute-full-forwarded-for: \u0026#34;true\u0026#34; use-forwarded-headers: \u0026#34;true\u0026#34; proxy-body-size: \u0026#34;0\u0026#34; ingressClassResource: name: external-nginx enabled: true default: false affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app.kubernetes.io/name operator: In values: - ingress-nginx topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; replicaCount: 1 admissionWebhooks: enabled: false service: annotations: cloud.google.com/load-balancer-type: External metrics: enabled: false helm install my-ing ingress-nginx/ingress-nginx --namespace ingress --version 4.0.17 --values nginx-values.yaml --create-namespace kubectl get pods -n ingress kubectl get svc -n ingress # 3-example.yaml --- apiVersion: v1 kind: Service metadata: name: nginx namespace: default spec: selector: app: nginx ports: - protocol: TCP port: 80 --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: nginx-ing namespace: default spec: ingressClassName: external-nginx rules: - host: example.com http: paths: - path: / pathType: Prefix backend: service: name: nginx port: number: 80 kubectl apply -f 3-example.yaml kubectl get ingressclass kubectl get ing Create DNS A-record with your DNS provider, eg. domains.google.com.\n"},{"id":19,"href":"/docs/devops/gcp/gke/","title":"Gke","section":"GCP","content":" GKE # source: Anton Putra GitHub repo\n# 0-provider.tf provider \u0026#34;google\u0026#34; { region = \u0026#34;us-west2\u0026#34; } resource \u0026#34;random_integer\u0026#34; \u0026#34;this\u0026#34; { min = 100 max = 1000000 } terraform { required_providers { google = { source = \u0026#34;hashicorp/google\u0026#34; version = \u0026#34;~\u0026gt; 3.66\u0026#34; } random = { source = \u0026#34;hashicorp/random\u0026#34; version = \u0026#34;~\u0026gt; 3.1\u0026#34; } } } # 1-locals.tf locals { region = \u0026#34;us-west2\u0026#34; org_id = \u0026#34;...\u0026#34; billing_account = \u0026#34;...\u0026#34; host_project_name = \u0026#34;host-staging\u0026#34; service_project_name = \u0026#34;k8s-staging\u0026#34; host_project_id = \u0026#34;${local.host_project_name}-${random_integer.this.result}\u0026#34; service_project_id = \u0026#34;${local.service_project_name}-${random_integer.this.result}\u0026#34; projects_api = \u0026#34;container.googleapis.com\u0026#34; secondary_ip_ranges = { \u0026#34;pod-ip-range\u0026#34; = \u0026#34;10.0.0.0/14\u0026#34;, \u0026#34;services-ip-range\u0026#34; = \u0026#34;10.4.0.0/19\u0026#34; } } # 2-projects.tf resource \u0026#34;google_project\u0026#34; \u0026#34;host-staging\u0026#34; { name = local.host_project_name project_id = local.host_project_id billing_account = local.billing_account org_id = local.org_id auto_create_network = false } resource \u0026#34;google_project\u0026#34; \u0026#34;k8s-staging\u0026#34; { name = local.service_project_name project_id = local.service_project_id billing_account = local.billing_account org_id = local.org-id auto_create_network = false } resource \u0026#34;google_project_service\u0026#34; \u0026#34;host\u0026#34; { project = google_project.host-staging.number service = local.projects_api } resource \u0026#34;google_project_service\u0026#34; \u0026#34;service\u0026#34; { project = google_project.k8s-staging.number service = local.projects_api } # 3-vpc.tf resource \u0026#34;google_compute_network\u0026#34; \u0026#34;main\u0026#34; { name = \u0026#34;main\u0026#34; project = google_compute_shared_vpc_hosts_project.host.project auto_create_subnetworks = false routing_mode = \u0026#34;REGIONAL\u0026#34; mtu = 1500 } resource \u0026#34;google_compute_subnetwork\u0026#34; \u0026#34;private\u0026#34; { name = \u0026#34;private\u0026#34; project = google_compute_shared_vpc_hosts_project.host.project ip_cidr_range = \u0026#34;10.5.0.0/20\u0026#34; region = local.region network = google_compute_network.main.self_link private_ip_google_access = true dynamic \u0026#34;secondary_ip_range\u0026#34; { for_each = local.secondary_ip_ranges content { range_name = secondary_ip_range.key ip_cidr_range = secondary_ip_range.value } } } # 4-router.tf resource \u0026#34;google_compute_router\u0026#34; \u0026#34;router\u0026#34; { name = \u0026#34;router\u0026#34; region = local.region project = local.hosts_project_id network = google_compute_network.main.self_link } # 5-nat.tf resource \u0026#34;google_compute_router_nat\u0026#34; \u0026#34;mist_nat\u0026#34; { name = \u0026#34;nat\u0026#34; project = local.hosts_project_id router = google_compute_router.router.name region = local.region nat_ip_allocate_option = \u0026#34;AUTO_ONLY\u0026#34; source_subnetwork_ip_ranges_to_nat = \u0026#34;ALL_SUBNETWORKS_ALL_IP_RANGES\u0026#34; depends_on = [google_compute_subnetwork.private] } # 6-shared-vpc.tf resource \u0026#34;google_compute_shared_vpc_host_project\u0026#34; \u0026#34;host\u0026#34; { project = google_project.host-staging.number } resource \u0026#34;google_compute_shared_vpc_service_project\u0026#34; \u0026#34;service\u0026#34; { host_project = local.host_project_id service_project = local.service_project_id depends_on = [google_compute_shared_vpc_host_project.host] } resource \u0026#34;google_compute_subnetwork_iam_binding\u0026#34; \u0026#34;binding\u0026#34; { project = google_compute_shared_vpc_host_project.host.project region = google_compute_subnetwork.private.region subnetwork = google_compute_subnetwork.private.name role = \u0026#34;roles/compute.networkUser\u0026#34; members = [ \u0026#34;serviceAccount:${google_service_account.k8s-staging.email}\u0026#34;, \u0026#34;serviceAccount:${google_project.k8s-staging.number}@cloudservices.gserviceaccount.com\u0026#34;, \u0026#34;serviceAccount:service-${google_project.k8s-staging.number}@container-engine-robot.iam.gserviceaccount.com\u0026#34; ] } resource \u0026#34;google_project_iam_binding\u0026#34; \u0026#34;container-engine\u0026#34; { project = google_compute_shared_vpc_host_project.host.project role = \u0026#34;roles/container.hostServiceAgentUser\u0026#34; members = [ \u0026#34;serviceAccount:service-${google_project.k8s-staging.number}@container-engine-robot.iam.gserviceaccount.com\u0026#34;, ] depends_on = [google_project_service.service] } # 7-kubernetes.tf resource \u0026#34;google_service_account\u0026#34; \u0026#34;k8s-staging\u0026#34; { project = local.service_project_id account_id = \u0026#34;k8s-staging\u0026#34; depends_on = [google_project.k8s-staging] } resource \u0026#34;google_container_cluster\u0026#34; \u0026#34;gke\u0026#34; { name = \u0026#34;gke\u0026#34; location = local.region project = local.service_project_id networking_mode = \u0026#34;VPC_NATIVE\u0026#34; network = google_compute_network.main.self_link subnetwork = google_compute_subnetwork.private.self_link remove_default_node_pool = true initial_node_count = 1 release_channel { channel = \u0026#34;REGULAR\u0026#34; } ip_allocation_policy { cluster_secondary_range_name = \u0026#34;pod-ip-range\u0026#34; services_secondary_range_name = \u0026#34;services-ip-range\u0026#34; } network_policy { provider = \u0026#34;PROVIDER_UNSPECIFIED\u0026#34; enabled = true } private_cluster_config { enable_private_endpoint = false enable_private_nodes = true master_ipv4_cidr_block = \u0026#34;172.16.0.0/28\u0026#34; } workload_identity_config { identity_namespace = \u0026#34;${google_project.k8s-staging.project_id}.svc.id.goog\u0026#34; } } resource \u0026#34;google_container_node_pool\u0026#34; \u0026#34;general\u0026#34; { name = \u0026#34;general\u0026#34; location = local.region cluster = google_container_cluster.gke.name project = local.service_project_id node_count = 1 management { auto_repair = true auto_upgrade = true } node_config { labels = { role = \u0026#34;general\u0026#34; } machine_type = \u0026#34;e2-medium\u0026#34; service_account = google_service_account.k8s-staging.email oauth_scopes = [ \u0026#34;https://www.googleapis.com/auth/cloud-platform\u0026#34; ] } } gcloud auth application-default login terraform fmt terraform init terraform validate terraform plan terraform apply gcloud container clusters get-credentials gke --region us-west2 --project k8s-staging-000000 kubectl get svc kubectl get nodes Example load balacer in Kubernetes:\n# 0-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment namespace: default spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 # 1-service.yaml --- apiVersion: v1 kind: Service metadata: name: nginx namespace: default spec: type: LoadBalancer ports: - protocol: TCP port: 80 selector: app: nginx kubectl apply -f . kubectl get pods kubectl get svc Public IP address of the LoadBalancer is not accessible on the public internet.\nCreate firewall rules go open access:\nkubectl describe svc nginx You can copy gcloud command to create firewall rules, or create them with terraform.\n# 8-firewall.tf resource \u0026#34;google_compute_firewall\u0026#34; \u0026#34;lb\u0026#34; { name = \u0026#34;k8s-fw-....\u0026#34; network = google_compute_network.main.name project = local.host_project_id description = \u0026#34;{\\\u0026#34;kubernetes.io/service-name\\\u0026#34;:\\\u0026#34;default/nginx\\\u0026#34;, \\\u0026#34;kubernetes.io/service-ip\\\u0026#34;:\\\u0026#34;....\\\u0026#34;}\u0026#34; allow { protocol = \u0026#34;tcp\u0026#34; ports = [\u0026#34;80\u0026#34;] } source_ranges = [\u0026#34;0.0.0.0/0\u0026#34;] target_tags = [\u0026#34;gke-gke-...-node\u0026#34;] } resource \u0026#34;google_compute_firewall\u0026#34; \u0026#34;health\u0026#34; { name = \u0026#34;k8s-...-node-http-hc\u0026#34; network = google_compute_network.main.name project = local.host_project_id description = \u0026#34;{\\\u0026#34;kubernetes.io/cluster-id\\\u0026#34;:\\\u0026#34;...\\\u0026#34;}\u0026#34; allow { protocol = \u0026#34;tcp\u0026#34; ports = [\u0026#34;10256\u0026#34;] } source_ranges = [\u0026#34;130.211.0.0/22\u0026#34;, \u0026#34;209.85.152.0/22\u0026#34;, \u0026#34;209.85.204.0/22\u0026#34;, \u0026#34;35.191.0.0/16\u0026#34;] target_tags = [\u0026#34;gke-gke-...-node\u0026#34;] } terraform apply kubectl get svc # EXTERNAL-IP should be now accessible "},{"id":20,"href":"/docs/devops/gcp/iam/","title":"Iam","section":"GCP","content":" IAM # IAM = Identity Access Management\nManages: Identity (who?) has role (what access) for which resource (or organization, project, folder).\nUse the principle of least access.\nPermissions not granted directly to the user. Permissions are grouped into roles which are granted to the authenticated members.\nIAM policy defines and enforces what roles are granted to which members. This policy is attached to a resource.\nPolicy architecture # Policy: Metadata: etags: concurrency control version: specifies schema version Audit-Config: configures audit logging Binding: Condition: constrains binding Member: [] Role: Permissions: [] Member is an identity that can access a resource. Identity is an e-mail account associated with a user, service account or a Google group, a domain name associated with a GSuite or cloud identity domains.\nGoogle Account any email address that is associated with a Google Account, either gmail.com or other domains Service Account an account for an application instead of an individual end user Google Group a named collection of Google Accounts and service accounts G Suite Domain Google Accounts created in an organization\u0026rsquo;s G Suite account Cloud Identity Domain Google Accounts in an organization that are not tied to any G Suite applications or features AllAUthenticatedUsers a special identifier that represents all service accounts and all users on the internet who have authenticated with a Google Account AllUsers a special identifier that represents anyone, including authenticated and unauthenticated users Permissions determine what operations are allowed on a resource. They correspond one-to-one with REST API methods. Permissions are not granted to users directly. You grant roles which contain one or more permissions. Permissions are represented as {service}.{resource}.{verb}, eg. compute.instances.list\nRoles are collections of permissions. You cannot grant a permission to the user directly. You grant a role to a user and all the permissions that the role contains.\nThere are three types of roles:\nPrimitive roles Owner, Editor, Viewer; historically available prior to IAM; avoid using these roles; can only be grated from a Google console Predefined roles finer-grained access control than the primitive roles; created and maintained by Google Custom roles tailor permissions to the needs of your organization; not maintained by Google; cannot be created at folder level; by default only project owner can create new roles Custom role launch stages:\nalpha = in testing beta = tested and awaiting approval ga = generally available Condition is a logic expression used to define and enforce attribute-based access control for Google Cloud resources. Conditions allow you to choose granting resource access to identities only if configured conditions are met. When a contition exists, the access request is only granted if the condition expression evaluates to true.\nMetadata etags prevent a race condition when updating the policy.\nMetadata version is used to avoid breaking existing integrations that rely on consistency in the policy structure when new policy schema versions are introduced.\nAudit config configures audit logging; determines which permission types are logged and what identities are exempted from logging.\nResource inherit the policies of all their parent resources. The policy is a union of a resource direct policy and the policies of all its parents in the resource hierarchy (organization, folders, project, resource).\nExample policy statement (can be expressed in JSON or YAML):\n{ \u0026#34;bindings\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;roles/storage.admin\u0026#34;, \u0026#34;members\u0026#34;: [ \u0026#34;user:example@gmail.com\u0026#34; ] }, { \u0026#34;role\u0026#34;: \u0026#34;roles/storage.objectViewer\u0026#34;, \u0026#34;members\u0026#34;: [ \u0026#34;user:other@gmail.com\u0026#34; ], \u0026#34;condition\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Expires_January_1_2021\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Do not grant access after Jan 2021\u0026#34;, \u0026#34;expression\u0026#34;: \u0026#34;request.time \u0026lt; timestamp(\u0026#39;2021-01-01T00:00:00.000Z\u0026#39;)\u0026#34; } } ], \u0026#34;etag\u0026#34;: \u0026#34;BeEEja0YfWJ=\u0026#34;, \u0026#34;version\u0026#34;: 3 } Querying for policies:\ngcloud projects get-iam-policy {project-id} gcloud resource-manager folders get-iam-policy {folder-id} gcloud organizations get-iam-policy {organization-id} Policy versions:\nversion 1 default; supports binding one role to one or more members; does not support conditional role bindings version 2 for Google\u0026rsquo;s internal use version 3 introduces condition statement Policy limitations # each resource can only have one policy (including organizations, projects, folders) max 1500 members or 250 Google groups per policy up to 7 minutes for a policy change to fully propagate across GCP max 100 conditional role bindings per policy Policy conditions # Condition attributes are either based on resource or based on details of the request, eg. timestamp or originating/destination IP address\nbindings: - members: - user:me@gmail.com role: roles/owner - members: - user:example@gmail.com role: roles/storage.objectViewer condition: title: Business_hours_access description: Business hours access Monday-Friday expression: request.time.getHours(\u0026#34;America/Toronto\u0026#34;) \u0026gt;= 9 \u0026amp;\u0026amp; request.time.getHours(\u0026#34;America/Toronto\u0026#34;) \u0026lt;= 17 \u0026amp;\u0026amp; // 0 = Sunday, 6 = Saturday request.time.getDayOfWeek(\u0026#34;America/Toronto\u0026#34;) \u0026gt;= 1 \u0026amp;\u0026amp; request.time.getDayOfWeek(\u0026#34;America/Toronto\u0026#34;) \u0026lt;= 5 etag: xxx version: 3 bindings: - members: - user:me@gmail.com role: roles/owner - members: - group:developer@mydomain.com role: roles/compute.instanceAdmin condition: title: Dev_only_access description: Only access to development* VMs expression: (resource.type == \u0026#39;compute.googleapis.com/Disk\u0026#39; \u0026amp;\u0026amp; resource.name.startsWith(\u0026#39;projects/project-my-example/regions/us-central1/disks/development\u0026#39;)) || (resource.type == \u0026#39;compute.googleapis.com/Instance\u0026#39; \u0026amp;\u0026amp; resource.name.startsWith(\u0026#39;projects/project-my-example/zones/us-central1-a/instances/development\u0026#39;)) || (resource.type != \u0026#39;compute.googleapis.com/Instance\u0026#39; \u0026amp;\u0026amp; resource.type != \u0026#39;compute.googleapis.com/Disk\u0026#39;) etag: xxx version: 3 Condition limitations:\nlimited to specific services primitive roles are not supported members cannot be allUsers or allAuthenticatedUsers max 100 conditional role bindings per policy max 20 role bindings for the same role and the same member Audit config logs # auditConfigs: - auditLogConfigs: - logType: DATA_READ - logType: ADMIN_READ - logType: DATA_WRITE service: allServices - auditLogConfigs: - exmptedMembers: - me@gmail.com logType: ADMIN_READ service: storage.googleapis.com Edit policy # gcloud projects get-iam-policy {project-id} \u0026gt; new-policy.yaml gcloud projects set-iam-policy {project-id} new-policy.yaml Make sure that etag is correct, or you will not be able to set the new policy.\nGranting access from the command line:\ngcloud projects add-iam-policy-binding {project-id} --member user:somebody@gmail.com --role roles/storage.admin Service accounts # A service account is both an identity and a resource.\nService account is a special kind of account used by an application or a virtual machine instance and not a person. It represents a non-human user. A service account is identified by an email address which is unique.\nThere are three service account types:\nUser-managed user created; you choose the name; default quota 100 user accounts per project Default some services automatically create user-managed service account with Editor role for the project; Google recommends revoking Editor role manually Google-managed created and managed by Google; used by Google services; some visible, some hidden; name ends with \u0026ldquo;Service Agent\u0026rdquo; or \u0026ldquo;Service Account\u0026rdquo;; do not change or revoke roles for these accounts Email format for user mananaged service accounts: {service-name}@{project-id}.iam.gserviceaccount.com\nEmail formats for the default service accounts:\n{project-id}@appspot.gserviceaccount.com {project-number}-compute@developer.gserviceaccount.com Service accounts authenticate using keys. Each service has two sets of private and public RSA keypairs.\nThere are Google managed and user managed keys.\ngcloud iam service-accounts list gcloud iam service-accounts create sa-example --display-name=\u0026#39;sa-example\u0026#39; gcloud projects add-iam-policy-binding {project-name} --member \u0026#39;serviceAccount:{account-name}@{project-name}.iam.gserviceaccount.com\u0026#39; --role \u0026#39;roles/storage.objectViewer\u0026#39; gcloud compute instances stop instance-1 --zone us-central1-a gcloud compute instances set-service-account instance-1 --zone us-central1-a --service-account sa-example@{project-name}.iam.gserviceaccount.com gcloud compute instances start instance-1 --zone us-central1 Cloud identity # Identity as a service (SSO, device management, security, reporting, directory management).\n"},{"id":21,"href":"/docs/devops/gcp/networking/","title":"Networking","section":"GCP","content":" Networking # OSI model:\nPhysical Data link Network (IPv4, IPv6, ICMP) Transport (TCP, UDP) Session Presentation Application (telnet, SSH, DNS, DHCP, HTTP, HTTPS) IP v4 # Classful addressing:\n4,294,967,296 addresses\nClass A 0.0.0.0 - 127.255.255.255; 2,147,483,648 addresses; 128 networks Class B 128.0.0.0 - 191.255.255.255; 1,073,741,824 addresses; 16,384 networks Class C 192.0.0.0 - 223.255.255.255; 536,870,912 addresses; 2,097,152 networks Private IP addresses (RFC1918):\nclass A: 10.0.0.0 - 10.255.255.255; 16,777,216 addresses class B: 172.16.0.0 - 172.31.255.255; 1,048,576 addresses class C: 192.168.0.0 - 192.168.255.255; 65,536 addresses Classless Inter-Domain Routing # CIDR = Classless Inter-Domain Routing\nAllows creating a network in any range using pattern {network-address}/{prefix}, eg: 192.168.0.0/16 which can be split in two halves with 32,768 addresses each: 192.168.0.0/17 (192.168.0.0 - 192.168.127.255) and 192.168.128.0/17 (192.168.128.0 - 192.168.255.255).\nEach of these halves can be halved again, each part with 16,384 addresses:\n192.160.0.0/18 (192.168.0.0 - 192.168.63.255) 192.160.64.0/18 (192.168.64.0 - 192.168.127.255) 192.160.128.0/18 (192.168.128.0 - 192.168.191.255) 192.160.192.0/18 (192.168.192.0 - 192.168.255.255) This subnetting process can be continued.\nCommon networks:\n192.168.0.0/8; 16+ million addresses 192.168.0.0/16; 65,536 addresses 192.168.0.0/24; 256 addresses 192.168.1.2/32; 1 address 0.0.0.0/0; all IP addresses IP v6 # Uses hexadecimal notation in which two octets are combined into hextets. Address consists of 8 hextets; it is 128 bits long. Redundand zeroes in a hextet can be removed. Double colon :: stands for a slew of zeroes.\nExample: 2001:de3::/64 (2001:de3:0000:0000:0000:0000:0000:0000 - 2001:de3:0000:0000:ffff:ffff:ffff:ffff)\nAll addresses: ::/0\nIP packet # TCP = Transmission Control Protocol\nUDP = User Datagram Protocol\nIP packet:\nSource IP address Destination IP address Protocol port number (source/destination) Data Snippets # Increase subnet range:\ngcloud compute networks subnets expand-ip-range default --region=us-west1 --prefix-length=16 gcloud compute networks subnets describe default --region=us-west1 Create regional IP address:\ngcloud compute addresses create {address-name} --region {region} Create global IP address:\ngcloud compute addresses create {address-name} --global --ip-version [IPV4|IPV6] "},{"id":22,"href":"/docs/devops/gcp/vpc/","title":"Vpc","section":"GCP","content":" VPC # VPC = Virtual Private Cloud\nVPC is a virtualized, software defined network within Google Cloud.\nVPC network, its routes and firewall rules are global resources, not associated with any particular region or zone.\nVPC is encapsulated within a project.\nVPCs do not have any IP address ranges associated with them. IP adress ranges are defined within subnetworks associated with VPC. Subnetworks are regional.\nNetwork firewall rules control traffic flowing in and out of the VPC.\nResources within a VPC can communicate with one another by using their internal (priovate) IPv4 addresses.\nVPCs only support IPv4 addresses (nodes can only send and receive IPv4 traffic). It is possible to create IPv6 address for a global load balancer.\nEach project comes with a default VPC network 10.128.0.0/9 that has a route to the default internet gateway and by default is in auto mode. There are two network types: subnet creation auto mode or custom mode.\nAuto mode networks automatically create a /20 subnet in each region. As new regions become available, new subnets are automatically added. Conversion from auto mode to custom mode is one-way. You cannot convert back; custom mode networks cannot be changed back to auto mode mentworks.\nTraffic cannot pass between separate networks unless you set up VPC peering or use a VPN connection.\nVPC network subnets # A VPC network consists of one or more subnets; each subnet is associated with a region.\nThe name or region of a subnet cannot be changed after you have created it. You have to delete a subnet (no resources may be using it) and create it again.\nPrimary and secondary ranges for subnets cannot overlap with any allocated range in other subnets or peered networks.\nSubnet address range can be increased: it must not overlap with other subnets, must stay inside the RFC 1918 adress-space, must be larger than the original. You cannot undo the expansion. /20 range can be expanded to the /16 range, but not larger. You have to convert to a custom mode to increase the range any further.\nIP addresses reserved for Google # There are 4 reserved IP addresses (first two and last two in the CIDR range) in its primary IP range:\nnetwork (first) default gateway (second) future use (second-to-last) broadcast (last) There are no reserved IP addresses in the secondary IP range.\nRouting and Private Google Access # Routes define the network traffic path from one destination to the other.\nIn a VPC route consists of a single CIDR destination and a single next hop.\nAll routes are stored in the routing table for the VPC.\nEach packet leaving a VM is delivered to the next hop of an applicable route based on a routing order.\nThere are two routing types:\nSystem generated default, subnet route Custom routes static route (created manually), dynamic route (maintained automatically by cloud routers) Default route # path to the internet path for Private Google Access can be deleted only by replacing with custom route it has the lowest priority Subnet route # define paths to each subnet in the VPC each subnet has at least one subnet route whose destination matches the primary IP range of the subnet when a subnet is created, a corresponding subnet route is created for both primary and secondary IP range you cannot delete a subnet route unless you modify or delete the subnet; when you delete a subnet, all its subnet routes are deleted automatically Static route # can use the next hop feature can be created manually static routes for the remote traffic selectors are created automatically when creating Cloud VPN tunnels Every route in the project must have a unique name. Priorities are from 0 to 1000, lower number have higher priority.\nDynamic route # managed by one or more Cloud Routers dynamically exchange routes between a VPC and on-premises networks destination IP ranges outside the VPC network used with dynamically routed VPNs and Interconnect Create VPC with gcloud # Create regional network with custom subnets.\nPublic subnet will have a default internet gateway.\nPrivate subnet will have NAT gateway as its route.\ngcloud compute networks create main --bgp-routing-mode=regional --subnet-mode=custom Add public subnet:\ngcloud compute networks subnets create public --range=10.0.0.0/24 --network=main --region=us-west2 Create private subnet:\ngcloud compute networks subnets create private --range=10.0.1.0/24 --network=main --region=us-west2 --enable-private-ip-google-access Create cloud router:\ngcloud compute routers create router --network=main --region=us-west2 Create cloud NAT gateway and select IP ranges:\ngcloud compute routers nats create nat --router=router --region=us-west2 --nat-custom-subnet-ip-ranges=private --auto-allocate-nat-external-ips Create VPC with terraform # First create a service account to be used with terraform and download credentials.\nprovider \u0026#34;google\u0026#34; { credentials = file(\u0026#34;~/google-credentials.json\u0026#34;) project = \u0026#34;my-project\u0026#34; region = \u0026#34;us-west2\u0026#34; } # main VPC resource \u0026#34;google_compute_network\u0026#34; \u0026#34;this\u0026#34; { name = \u0026#34;main\u0026#34; auto_create_subnetworks = false } # public subnet resource \u0026#34;google_compute_subnetwork\u0026#34; \u0026#34;public\u0026#34; { name = \u0026#34;public\u0026#34; ip_cidr_range = \u0026#34;10.0.0.0/24\u0026#34; region = \u0026#34;us-west2\u0026#34; network = google_compute_network.this.id } # private subnet resource \u0026#34;google_compute_subnetwork\u0026#34; \u0026#34;public\u0026#34; { name = \u0026#34;private\u0026#34; ip_cidr_range = \u0026#34;10.0.1.0/24\u0026#34; region = \u0026#34;us-west2\u0026#34; network = google_compute_network.this.id } # cloud router resource \u0026#34;google_compute_router\u0026#34; \u0026#34;this\u0026#34; { name = \u0026#34;router\u0026#34; network = google_compute_network.this.id bgp { asn = 64514 advertise_mode = \u0026#34;CUSTOM\u0026#34; } } # NAT gateway resource \u0026#34;google_compute_router_nat\u0026#34; \u0026#34;this\u0026#34; { name = \u0026#34;nat\u0026#34; router = google_compute_router.this.name region = google_compute_router.this.region nat_ip_allocate_option = \u0026#34;AUTO_ONLY\u0026#34; source_subnetwork_ip_ranges_to_nat = \u0026#34;LIST_OF_SUBNETWORKS\u0026#34; subnetwork { name = \u0026#34;private\u0026#34; source_ip_ranges_to_nat = [\u0026#34;ALL_IP_RAGES\u0026#34;] } } "},{"id":23,"href":"/docs/devops/git/branches/","title":"Branches","section":"Git","content":" Branches # git remote rm origin git remote add origin https://xxxx/xxxx git push origin --all New branch:\ngit checkout -b \u0026lt;branch\u0026gt; git push -u origin \u0026lt;branch\u0026gt; Merge:\ngit checkout master git merge hotfix List tags:\ngit fetch --tags git push --tags "},{"id":24,"href":"/docs/devops/git/configuration/","title":"Configuration","section":"Git","content":" Configuration of git # Username and email # git config --global user.name \u0026#34;D. Purge\u0026#34; git config --global user.email \u0026#34;my@email.com\u0026#34; git config --global core.autocrlf false SSH checkouts from Azure DevOps # Generate SSH keypair:\nssh-keygen -t rsa Full parameters:\nssh-keygen \\ -m PEM \\ -t rsa \\ -b 4096 \\ -C \u0026#34;user@server.example.com\u0026#34; \\ -f ~/.ssh/mykeys/privatekey \\ -N passphrase Parameters:\n-m PEM = format key as PEM -t RSA = type of the key, RSA format -b 4096 = number of bits in the key, 4096 bits -C \u0026ldquo; user@server.example.com\u0026rdquo; = a comment appended at the end of the public key, to identify it -f ~/.ssh/mykeys/privatekey = the filename of the private key filet, a corresponding public key file appended with .pub is generated in the same directory which must exist -N passphrase = a passphrase used to access the private key file Add configuration in ~/.ssh/config:\nHost ssh.dev.azure.com\rIdentityFile ~/.ssh/id_rsa\rIdentitiesOnly yes\rUser git\rPubkeyAcceptedAlgorithms +ssh-rsa\rHostkeyAlgorithms +ssh-rsa "},{"id":25,"href":"/docs/devops/helm/charts/","title":"Charts","section":"Helm","content":" Charts # Download # mkdir cluster-config cd cluster-config helm pull --help helm pull bitnami/mysql --untar=true helm install mysql ./mysql/ helm upgrade mysql --values=./mysql/custom-values.yaml ./mysql/ Convert to k8s configuration # helm template mysql ./mysql/ --values=./mysql/custom-values.yaml \u0026gt; mysql-installation.yaml Create charts # helm create example-helm-chart cd example-helm-chart/ helm template . # to test the output Reference values in the template: {{ .Values.MyKey }}\nCall function in the template with space-separated list of parameters: {{ lower .Values.MyKey }}\nPass values to a function through a pipeline: {{ .Values.MyKey | upper }}\nExample of flow control:\nimage: version: v0.1.0{{ if eq .Values.environment \u0026#34;dev\u0026#34; }}-dev{{ end }} Files containing named templates have names starting with underscore.\nExample of named template in _shared.tpl:\n{{ define \u0026#34;example\u0026#34; }} - name: xxx value: yyy {{ end }} Using named template:\nmylist: {{- include \u0026#34;example\u0026#34; . | indent 2}} Prefix k8s object names with release name:\nmetadata: name: {{ .Release.Name }}-webapp "},{"id":26,"href":"/docs/devops/helm/installation/","title":"Installation","section":"Helm","content":" Installation # Linux # TODO\nInstall Helm:\nexport HELM_VERSION=3.9.2 cd /tmp curl \\ -sSL https://get.helm.sh/helm-v${HELM_VERSION}-linux-amd64.tar.gz \\ -o helm.tar.gz tar xf helm.tar.gz linux-amd64/helm mv linux-amd64/helm /usr/local/bin/helm rmdir linux-amd64 rm helm.tar.gz Packages # Find helm charts on https://artifacthub.io/\nExample usage # helm repo list helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update helm install mysql bitnami/mysql helm list helm uninstall mysql Find chart values:\nhelm show values bitnami/mysql \u0026gt; values.yaml vim values.yaml helm upgrade mysql bitnami/mysql --set section.name=value helm upgrade mysql bitnami/mysql --values=values.yaml "},{"id":27,"href":"/docs/devops/k3s/k3d/","title":"K3d","section":"K3s","content":" K3d # K3d is a wrapper of K3s. K3d deploys docker based k3s clusters.\nInstallation # K3d CLI binaries:\nhttps://github.com/k3d-io/k3d/releases/download/v5.4.8/k3d-windows-amd64.exe https://github.com/k3d-io/k3d/releases/download/v5.4.8/k3d-linux-amd64 Test # k3d cluster create mycluster kubectl cluster-info kubectl get all k3d cluster delete mycluster "},{"id":28,"href":"/docs/devops/k3s/k3s/","title":"K3s","section":"K3s","content":" K3s # K3s deploys a virtual machine-based Kubernetes cluster.\nInstallation # curl -Lo /usr/local/bin/k3s https://github.com/k3s-io/k3s/releases/download/v1.27.5%2Bk3s1/k3s chmod a+x /usr/local/bin/k3s curl -Lo /usr/local/bin/kubectl \u0026#34;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026#34; chmod a+x /usr/local/bin/kubectl Maintenance # sudo k3s crictl images sudo k3s crictl rmi --prune Test # Start server:\nK3S_KUBECONFIG_MODE=\u0026#34;644\u0026#34; k3s server --flannel-backend none Access server:\nexport KUBECONFIG=/etc/rancher/k3s/k3s.yaml kubectl get pods --all-namespaces helm ls --all-namespaces Install in LXD container:\nsudo su - cd /tmp curl https://get.k3s.io/ -o k3s.sh chmod +x k3s.sh ./k3s.sh systemctl status k3s k3s check-config Troubleshooting:\ncat /etc/os-release ip a s tail /var/log/syslog journalctl -u k3s.service /usr/local/bin/k3s-uninstall.sh NFS # Add /var/lib/rancher/k3s/server/manifests/nfs.yaml:\n--- apiVersion: v1 kind: Namespace metadata: name: nfs --- apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: nfs namespace: nfs spec: chart: nfs-subdir-external-provisioner repo: https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner targetNamespace: nfs set: nfs.server: x.x.x.x nfs.path: /Kubernetes storageClass.name: nfs "},{"id":29,"href":"/docs/devops/kubernetes/azure-kubernetes/","title":"Azure Kubernetes","section":"Kubernetes","content":" Kubernetes on Azure # Deployment to cluster # Connect to Azure container registry:\naz account set --subscription $SubscriptionID az aks get-credentials --resource-group ResGrp-Name001 --name acrname001 Create namespace YAML:\nkubectl create namespace my-namespace-name -o yaml --dry-run=client Create deployment YAML:\nkubectl create deployment image-name --image=acrname001.azurecr.io/image_name:1.0.0 -o yaml --dry-run=client Apply namespace YAML:\nkubectl apply -f namespace.yaml kubectl get namespaces Apply deployment YAML:\nkubectl apply -f image-name-deployment.yaml kubectl logs -f deployment/image-name --namespace my-namespace-name Check deployments:\nkubectl get deployments --all-namespaces=true kubectl get deployments --namespace $CurrentNamespace kubectl describe deployment $CurrentDeployment --namespace $CurrentNamespace kubectl logs -l $LabelKey=$LabelValue List pods:\nkubectl get pods --namespace my-namespace-name Execute command on a pod:\nkubectl exec image-name-xxxxxxxxxx-xxxxx --namespace my-namespace-name -- python --version Run interactive shell on a pod:\nkubectl exec --stdin --tty image-name-xxxxxxxxxx-xxxxx --namespace my-namespace-name -- bash "},{"id":30,"href":"/docs/devops/kubernetes/deployment/","title":"Deployment","section":"Kubernetes","content":" Deployment # Deployment is a wrapper around replica set, which allows to do controlled updates to pods. When we update the pod template in the deployment, it will update the pods.\napiVersion: apps/v1 kind: Deployment metadata: name: hello labels: app.kubernetes.io/name: hello spec: replicas: 3 selector: matchLabels: app.kubernetes.io/name: hello template: metadata: labels: app.kubernetes.io/name: hello spec: containers: - name: hello-container image: busybox command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo Hello from deployment! \u0026amp;\u0026amp; sleep 3600\u0026#39;] kubectl apply -f deployment.yaml --record kubectl get deployment kubectl get rs kubectl get pod kubectl scale deployment hello --replicas=5 kubectl get po --watch kubectl scale deploy hello --replicas=3 kubectl set image deploy hello hello-container=busybox:1.31.1 --record kubectl rollout history deploy hello kubectl rollout undo deploy hello Deployment strategies:\nRecreate - deletes all pods and starts new ones RollingUpdate - you optionally define maximum unavailable and maximum surge numbers (default is 25%), new pods are created before old pods are deleted spec: strategy: type: Recreate spec: strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 40% maxSurge: 40% "},{"id":31,"href":"/docs/devops/kubernetes/design-patterns/","title":"Design Patterns","section":"Kubernetes","content":" Kubernetes design patterns # Foundational # Health probe # Every container should implement APIs allowing the platform to observe and manage the application.\nPredictable demands # Every container should declare its resource requirements and runtime dependencies and stay confined to them.\nAutomated placement # Declarative Deployment # Managed Lifecycle # Structural # Init Container # Sidecar # Adapter # Ambassador # Configuration # EnvVar Configuration # Immutable Configuration # Configuration Template # Configuration Resource # Behavioral # Batch Job # Stateful Service # Service Discovery # Periodic Job # Stateless Service # Singleton Service # Higher Level # Controller # Operator # Serverless Serving # Serverless Eventing # Elastic Scale # Image Builder # "},{"id":32,"href":"/docs/devops/kubernetes/ingress/","title":"Ingress","section":"Kubernetes","content":" Ingress # Ingress resource exposes multiple services to the outside of the cluster and manages access. It is a collection of rules and paths but needs something to apply these rules the Ingress controller. Ingress controller acts as a gateway and routes external traffic to services based on the Ingress resource and its rules.\nIngress controller is a collection of:\nkubernetes deployment, with pods running containers with a gateway or proxy server, eg. nginx, ambassador kubernetes service that exposes ingress controller pods supporting resources: configuration maps, secrets etc. apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-example spec: rules: - host: example.com http: paths: - path: /blog backend: serviceName: blogservice servicePort: 80 - path: /music backend: serviceName: musicservice servicePort: 8080 Ingress controller will have annotations that depend on the type of Ingress Controller you are using (Traeffik. Nginx, HAProxy, Ambassador, \u0026hellip;)\nExample with Ambassador gateway:\nkubectl apply -f https://www.getambassador.io/yaml/ambassador/ambassador-crds.yaml kubectl apply -f https://www.getambassador.io/yaml/ambassador/ambassador-rbac.yaml kubectl get deploy kubectl get po kubectl get svc apiVersion: v1 kind: Service metadata: name: ambassador spec: type: LoadBalancer externalTrafficPolicy: Local selector: service: ambassador ports: - port: 80 targetPort: 8080 kubectl apply -f ambassador-lb.yaml minikube service ambassador # opens browser to the service address; diagnostics: /ambassador/v0/diag/ Ingress without rules:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: ambassador name: my-ingress spec: defaultBackend: service: name: hello-world port: number: 3000 kubectl apply -f simple-ing.haml kubectl get ing minikube service ambassador Example with path-based routing:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: ambassador name: my-ingress spec: rules: - http: paths: - path: /hello pathType: Prefix backend: service: name: hello-world port: number: 3000 - path: /dog pathType: Prefix backend: service: name: dog-service port: number: 3000 kubectl apply -f path-ing.haml kubectl describe ing my-ingress minikube ip # access: # http://.../hello # http://.../dog Hostname based access:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: ambassador name: my-ingress spec: rules: - host: example.com http: paths: - path: /hello pathType: Prefix backend: service: name: hello-world port: number: 3000 - path: /dog pathType: Prefix backend: service: name: dog-service port: number: 3000 kubectl apply -f hostname-ing.yaml kubectl describe ing my-ingress # add DNS record to your domain registrar and access: http://example.com/hello Some ingress controllers will automatically set up default backend service. In Ambassador you can combine defaultBackend and rules.\nSubdomain ingress:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: ambassador name: my-ingress spec: defaultBackend: service: name: hello-world port: number: 3000 rules: - host: example.com http: paths: - path: /hello pathType: Prefix backend: service: name: hello-world port: number: 3000 - host: dog.example.com http: paths: - path: / pathType: Prefix backend: service: name: dog-service port: number: 3000 kubectl apply -f subdomain-ing.yaml kubectl describe ing my-ingress # http://example.com/hello # http://dog.example.com/ "},{"id":33,"href":"/docs/devops/kubernetes/k9s/","title":"K9s","section":"Kubernetes","content":" K9s # Home page GitHub repository Releases Installation # Windows $K9s_Version = \u0026#34;0.27.4\u0026#34; $K2s_Archive = \u0026#34;https://github.com/derailed/k9s/releases/download/v${K9s_Version}/k9s_Windows_amd64.zip\u0026#34; Invoke-WebRequest -Uri $K2s_Archive -OutFile \u0026#34;${pwd}/k9s_Windows_amd64_${K9s_Version}.zip\u0026#34; Add-Type -Assembly System.IO.Compression.FileSystem $zip = [IO.Compression.ZipFile]::OpenRead(\u0026#34;${pwd}/k9s_Windows_amd64_${K9s_Version}.zip\u0026#34;) $Executables = $zip.Entries | Where-Object {$_.Name -like \u0026#39;*.exe\u0026#39;} foreach ($Executable in $Executables) { [System.IO.Compression.ZipFileExtensions]::ExtractToFile($Executable, \u0026#34;${pwd}/$($Executable.Name)\u0026#34;, $True) } $zip.Dispose() Remove-Item -Path \u0026#34;${pwd}/k9s_Windows_amd64_${K9s_Version}.zip\u0026#34; Linux TODO "},{"id":34,"href":"/docs/devops/kubernetes/kubectl-config/","title":"Kubectl Config","section":"Kubernetes","content":" KubeCtl configuration # Configuration files # Default file:\n~/.kube/config %USERPROFILE%\\.kube\\config Multiple files defined in environment variable:\nset KUBECONFIG=%USERPROFILE%\\.kube\\config;%USERPROFILE%\\.kube\\jdpnas02.yaml KUBECONFIG=~/.kube/config:~/.kube/jdpnas02.yaml Display configured contexts:\nkubectl config get-contexts Display merged configs:\nkubectl config view Show current context:\nkubectl config current-context Set default context:\nkubectl config use-context jdpnas02 Examples:\naz account set --subscription xxxxxxxxxxx az aks get-credentials --resource-group ResGrp-xxx-K8s --name xxxx kubectl config current-context kubectl get all --namespace xxx kubectl get cronjob --namespace xxx kubectl describe cronjob --namespace xxx kubectl get pods --namespace xxx kubectl logs --namespace xxx integration-sync-28023457-cbg4j kubectl delete job --namespace xxx integration-sync-28023332 kubectl delete cronjob integration-sync --namespace xxx kubectl exec --stdin --tty xxx-867d77f465-tkcdk --namespace xxx -- /bin/bash curl -X POST 127.0.0.1/sync --data \u0026#39;{\u0026#34;time_to_sync\u0026#34;: 240}\u0026#39; --header \u0026#34;Content-Type: application/json\u0026#34; "},{"id":35,"href":"/docs/devops/kubernetes/minikube/","title":"Minikube","section":"Kubernetes","content":" Minikube # minikube delete minikube start --memory 4096 Edit service xxx:\nkubectl edit svc xxx "},{"id":36,"href":"/docs/devops/kubernetes/pod/","title":"Pod","section":"Kubernetes","content":" Pod # A pod is a collection of containers that share network and storage. Containers within a pod can talk to each other on localhost and can access the same volumes. Pods are ephemeral. Kubernetes assigns a unique IP address to a pod. Containers within a pod can listen on different ports.\nWhen the pod restarts, it gets a different IP address. A service is an abstraction that gets a stable IP address and DNS name and acts as a load balancer for pods.\nAll containers within a pod will get scaled together - a pod is the unit of scale. You cannot scale individual containers within the pod.\nCreating pods # You should not create pods directly. One of the reasons is that if the pod crashes or is deleted, it will not be restarted.\napiVersion: v1 kind: Pod metadata: name: hello-pod labels: app.kubernetes.io/name: hello spec: containers: - name: hello-container image: busybox command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo Hello World \u0026amp;\u0026amp; sleep 3600\u0026#34;] kubectl apply -f hello-pod.yaml kubectl get pods kubectl logs hello-pod lubectl describe pod hello-pod kubectl delete pod hello-pod "},{"id":37,"href":"/docs/devops/kubernetes/replica-set/","title":"Replica Set","section":"Kubernetes","content":" Replica set # Replica set maintains a stamble number of pod copies (replicas).\nReplica set controller guarantees that a specified nymber of identical pods are running at all times. It uses the selector field and pod labels to find pods that it owns.\napiVersion: apps/v1 kind: ReplicaSet metadata: name: hello labels: app.kubernetes.io/name: hello spec: replicas: 3 selector: matchLabels: app.kubernetes.io/name: hello template: metadata: labels: app.kubernetes.io/name: hello spec: containers: - name: hello-container image: busybox command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo Hello from replica set! \u0026amp;\u0026amp; sleep 3600\u0026#34;] kubectl apply -f replicaset.yaml kubectl get replicaset kubectl get pods -l app.kubernetes.io/name=hello kubectl get pods hello-xxxxx -o yaml | grep -A5 ownerReferences kubectl delete po hello-xxxxx kubectl edit rs hello kubectl get po hello-xxxxx | grep Image kubectl delete rs hello Replica set can be managed by a deployment, which can update pods managed by this replica set in a controlled, zero-downtime manner.\n"},{"id":38,"href":"/docs/devops/kubernetes/sealed-secrets/","title":"Sealed Secrets","section":"Kubernetes","content":" Sealed secrets # Install kubeseal # Command line:\ncurl -sSL https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.19.3/kubeseal-0.19.3-linux-amd64.tar.gz -o kubeseal-linux-amd64.tar.gz tar -xf kubeseal-linux-amd64.tar.gz kubeseal mv kubeseal /usr/local/bin/ kubeseal --version Controller:\nkubectl apply -f https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.19.3/controller.yaml kubectl get pods -n kube-system | grep sealed-secrets-controller kubectl logs sealed-secrets-controller-xxx-xxx -n kube-system kubectl get secret -n kube-system -l sealedsecrets.bitnami.com/sealed-secrets-key -o yaml "},{"id":39,"href":"/docs/devops/kubernetes/service/","title":"Service","section":"Kubernetes","content":" Service # Pods are ephemeral, so we cannot rely on pod IP addresses.\nExample deployment:\napiVersion: apps/v1 kind: Deployment metadata: name: web-frontend labels: app.kubernetes.io/name: web-frontend spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: web-frontend template: metadata: labels: app.kubernetes.io/name: web-frontend spec: containers: - name: web-frontend-container image: projectid/helloworld:0.1.0 ports: - containerPort: 3000 kubectl apply -f web-frontend.yaml kubectl get po -o wide kubectl run curl --image=radial/busyboxplus:curl -i --tty curl 172.17.0.6:3000 Pod\u0026rsquo;s IP is available only from within the cluster.\nKubernetes service is an abstraction which gives us access to pod\u0026rsquo;s IP in a reliable way. Service controller maintains a list of pod endpoints/IP addresses. It uses selectors and labels to find its pods.\napiVersion: v1 kind: Service metadata: name: web-frontend labels: app.kubernetes.io/name: web-frontend spec: selector: app.kubernetes.io/name: web-frontend ports: - port: 80 name: http targetPort: 3000 kubectl apply -f web-frontend-service.yaml kubectl get service kubectl attach curl -c curl -i -t curl 10.109.203.211 # Cluster IP curl web-frontend.default.svc.cluster.local # DNS name with namespace, kind and cluster name specified curl web-frontend # same namespace curl web-frontend.default # namespace specified You can use proxy to access the API server from local computer:\nkubectl proxy --port=8080 curl localhost:8080/api/v1/pods curl localhost:8080/api/v1/namespaces curl -v -L localhost:8080/api/v1/namespaces/default/services/web-frontend:80/proxy Getting information about service:\nkubectl describe svc web-frontend kubectl get endpoints Service types kubectl describe svc web-frontend:\nClusterIP - cluster internal IP address; default; used for applications running inside the cluster NodePort - a specific port opened on every node in the cluster, forwards traffic to the service and pods LoadBalancer - expose kubernetes services to external traffic ExternalName - Node port should be between 30000 and 32767, and can optionally be set as nodePort property under ports. However, the best practice is to leave it out and let Kubernetes choose the port number. You can access them on the node IP: kubectl describe node | grep InternalIP\nkind: Service spec: type: NodePort ports: - port: 80 name: http targetPort: 3000 nodePort: 32767 # optional; best practice is to leave it out In the cloud, load balancer service will be exposed by the cloud implementation of the load balancer. Locally, even if external IP is pending, load balancer service will be available on localhost address. On Minikube, minikube tunnel will assingn a public IP to any service with type LoadBalancer.\nkind: Service spec: type: LoadBalancer ports: - port: 80 name: http targetPort: 3000 ExternalName service type does not use selectors; instead it uses DNS names to route traffic to the application that lives outside of the cluster. You can use it to map a Kubernetes service to the external DNS name; eg. when you are migrating an application to Kubernetes.\napiVersion: v1 kind: Service metadata: name: my-database spec: type: ExternalName externalName: db.example.com "},{"id":40,"href":"/docs/devops/kubernetes/short-names/","title":"Short Names","section":"Kubernetes","content":" Resource short names # Short names can be displayed with a command: kubectl api-resources\nName Short name Kind API version Namespaced pods po Pod v1 true replicasets rs ReplicaSet apps/v1 true deployments deploy Deployment apps/v1 true services svc Service v1 true statefulsets sts StatefulSet apps/v1 true daemonsets ds DaemonSet apps/v1 true persistentvolumes pv PersistentVolume v1 false persistentvolumeclaims pvc PersistentVolumeClaim v1 true configmaps cm ConfigMap v1 true namespaces ns Namespace v1 false nodes no Node v1 false secrets Secret v1 true jobs Job batch/v1 true cronjobs cj CronJob batch/v1 true endpoints ep Endpoints v1 true ingresses ing Ingress networking.k8s.io/v1 true "},{"id":41,"href":"/docs/devops/linux/blocky/","title":"Blocky","section":"Linux","content":" Blocky # Blocky is a DNS proxy and ad-blocker.\nwget -q \u0026#34;https://github.com/0xERR0R/blocky/releases/download/v0.21/blocky_v0.21_Linux_x86_64.tar.gz\u0026#34; tar xf blocky_v0.21_Linux_x86_64.tar.gz sudo mv blocky /usr/local/bin/ sudo setcap cap_net_bind_service=ep /usr/local/bin/blocky Create user:\nsudo groupadd --system blocky sudo useradd --system --gid blocky --create-home --home-dir /var/lib/blocky --shell /usr/sbin/nologin --comment \u0026#34;Blocky DNS proxy\u0026#34; blocky Configuration file /etc/blocky/config.yml:\nupstream: default: - 8.8.8.8 - 1.1.1.1 blocking: blackLists: ads: - https://raw.githubusercontent.com/dpurge/dns-hole/main/blacklist/dpurge.txt - https://raw.githubusercontent.com/StevenBlack/hosts/master/hosts whiteLists: ads: - https://raw.githubusercontent.com/dpurge/dns-hole/main/whitelist/dpurge.txt clientGroupsBlock: default: - ads downloadTimeout: 4m downloadAttempts: 5 downloadCooldown: 10s customDNS: mapping: example.home.arpa: 127.0.0.1 ports: dns: 53 bootstrapDns: - 8.8.8.8 - 1.1.1.1 log: level: warn Service file /etc/systemd/system/blocky.service:\n[Unit] Description=Blocky DNS resolver ConditionPathExists=/usr/local/bin/blocky After=local-fs.target [Service] User=blocky Group=blocky Type=simple WorkingDirectory=/var/lib/blocky ExecStart=/usr/local/bin/blocky --config /etc/blocky/config.yml Restart=on-failure RestartSec=10 SyslogIdentifier=blocky [Install] WantedBy=multi-user.target Install:\nsystemctl is-active systemd-resolved sudo systemctl disable --now systemd-resolved.service sudo systemctl daemon-reload sudo systemctl enable blocky sudo systemctl start blocky systemctl is-active blocky sudo less /var/log/syslog Manage:\nsudo systemctl status blocky sudo systemctl stop blocky sudo systemctl start blocky sudo systemctl restart blocky Configure:\nsudo vim /etc/blocky/config.yml sudo vim /etc/systemd/system/blocky.service Uninstall:\nsystemctl stop blocky systemctl disable --now blocky.service systemctl enable --now systemd-resolved.service rm -rf /etc/systemd/system/blocky.service rm -rf /usr/local/bin/blocky userdel -r -f blocky "},{"id":42,"href":"/docs/devops/linux/caddy/","title":"Caddy","section":"Linux","content":" Caddy # Install xcaddy # wget -q https://github.com/caddyserver/xcaddy/releases/download/v0.3.4/xcaddy_0.3.4_linux_amd64.tar.gz tar xf xcaddy_0.3.4_linux_amd64.tar.gz sudo mv xcaddy /usr/local/bin/ Build caddy # xcaddy build \\ --with github.com/wxh06/caddy-uwsgi-transport sudo mv caddy /usr/local/bin/ sudo setcap cap_net_bind_service+eip /usr/local/bin/caddy Install caddy # sudo groupadd --system caddy sudo useradd --system --gid caddy --create-home --home-dir /var/lib/caddy --shell /usr/sbin/nologin --comment \u0026#34;Caddy web server\u0026#34; caddy Create files and directories:\nsudo mkdir /etc/caddy sudo touch /etc/caddy/Caddyfile sudo chown -R root:caddy /etc/caddy sudo mkdir /etc/ssl/caddy sudo chown -R root:caddy /etc/ssl/caddy sudo chmod 0770 /etc/ssl/caddy sudo mkdir -p /var/www/public_html sudo chown caddy:caddy /var/www/public_html sudo touch /etc/systemd/system/caddy.service sudo chmod 644 /etc/systemd/system/caddy.service sudo touch /var/www/public_html/index.html Service file /etc/systemd/system/caddy.service:\n[Unit] Description=Caddy web server Documentation=https://caddyserver.com/docs/ After=network.target network-online.target Requires=network-online.target [Service] Type=notify User=caddy Group=caddy ExecStart=/usr/local/bin/caddy run --environ --config /etc/caddy/Caddyfile ExecReload=/usr/local/bin/caddy reload --config /etc/caddy/Caddyfile --force TimeoutStopSec=5s LimitNOFILE=1048576 LimitNPROC=512 PrivateTmp=true ProtectSystem=full AmbientCapabilities=CAP_NET_ADMIN CAP_NET_BIND_SERVICE [Install] WantedBy=multi-user.target Caddy configuration file /etc/caddy/Caddyfile:\n{ auto_https off } http://hide.example.home.arpa { root * /var/www/public_html encode gzip file_server header / { Content-Security-Policy = \u0026#34;upgrade-insecure-requests; default-src \u0026#39;self\u0026#39;; style-src \u0026#39;self\u0026#39;; script-src \u0026#39;self\u0026#39;; img-src \u0026#39;self\u0026#39;; object-src \u0026#39;self\u0026#39;; worker-src \u0026#39;self\u0026#39;; manifest-src \u0026#39;self\u0026#39;;\u0026#34; Strict-Transport-Security = \u0026#34;max-age=63072000; includeSubDomains; preload\u0026#34; X-Xss-Protection = \u0026#34;1; mode=block\u0026#34; X-Frame-Options = \u0026#34;DENY\u0026#34; X-Content-Type-Options = \u0026#34;nosniff\u0026#34; Referrer-Policy = \u0026#34;strict-origin-when-cross-origin\u0026#34; Permissions-Policy = \u0026#34;fullscreen=(self)\u0026#34; Cache-Control = \u0026#34;max-age=0,no-cache,no-store,must-revalidate\u0026#34; } } Index file /var/www/public_html/index.html:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta http-equiv=\u0026#34;content-type\u0026#34; content=\u0026#34;text/html; charset=utf-8\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34;content=\u0026#34;width=device-width,initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;link href=\u0026#34;https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; integrity=\u0026#34;sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js\u0026#34; integrity=\u0026#34;sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;title\u0026gt;Static page example\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body class=\u0026#34;text-center\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;jumbotron text-center\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;Welcome to Caddy\u0026lt;/h1\u0026gt; \u0026lt;h2 class=\u0026#34;h2 card-title\u0026#34;\u0026gt;Welcome to Caddy\u0026lt;/h2\u0026gt; \u0026lt;h6 class=\u0026#34;h6 card-text\u0026#34;\u0026gt;Your installation of Caddy is working.\u0026lt;/h6\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Run service:\nsudo systemctl daemon-reload sudo systemctl enable caddy sudo systemctl start caddy sudo systemctl stop caddy systemctl status caddy.service sudo journalctl -xeu caddy.service sudo journalctl -u caddy --no-pager | less http://example.home.arpa { @api { path /config path /healthz path /stats/errors path /stats/checker } @static { path /static/* } @notstatic { not path /static/* } @imageproxy { path /image_proxy } @notimageproxy { not path /image_proxy } header { X-XSS-Protection \u0026#34;1; mode=block\u0026#34; X-Content-Type-Options \u0026#34;nosniff\u0026#34; X-Robots-Tag \u0026#34;noindex, noarchive, nofollow\u0026#34; -Server } header @api { Access-Control-Allow-Methods \u0026#34;GET, OPTIONS\u0026#34; Access-Control-Allow-Origin \u0026#34;*\u0026#34; } handle { encode zstd gzip reverse_proxy localhost:8001 { header_up X-Forwarded-Port {http.request.port} header_up X-Forwarded-Proto {http.request.scheme} } } } "},{"id":43,"href":"/docs/devops/linux/podman/","title":"Podman","section":"Linux","content":" Podman # container management tool does not require a daemon each container is run in its own process supports rootless containers created by RedHat is open source Podman vs Docker # Differences:\nPodman has deamonless architecture and runs containers as processes. Docker utilizes a client-server architecture, docker daemon runs as a background process. Podman supports rootless containers. Docker generally requires administrative privileges to run containers. Podman is considered more secure because it does not require a deamon and can run containers as non-root. Docker raises security concerns because of a daemon running with root privileges. Podman supports multiple image formats. Docker uses docker image format. Podman leverages the host system networking directly. Docker manages networking through its own implementation. Similarities:\nsimilar command line interface container formats are compatible Installation # MacOS:\nexport PATH=\u0026#34;/opt/homebrew/bin:$PATH\u0026#34; brew install podman podman --version Podman machine # Manages lightweight, virtualized environments for running containers.\npodman machine init podman1 podman machine start podman1 podman machine list podman machine info podman machine inspect podman1 podman machine stop podman1 podman machine rm podman1 Images # blueprints for containers contain necessary configurations and components podman pull alpine:latest podman info # check registries.search podman images Containers # Container lifecycle:\ncreated: defined and configured, but not yet started up: running the application or service paused: halted temporarily, its processes frozen; can be resumed maintaining its state stopped: its processes halted; does not consume resources; can be restarted exited: completed its execution podman run hello-world podman ps -a podman run -d -it --name alp1 docker.io/library/alpine:latest /bin/sh podman ps podman exec -it alp1 /bin/sh podman stop alp1 podman start alp1 podman rm alp1 Custom container # from flask import Flask app = Flask(__name__) @app.route(\u0026#39;/\u0026#39;) def hello_world(): return \u0026#39;Hello, World!\u0026#39; if __name__ == \u0026#34;main\u0026#34;: app.run(host =\u0026#39;0.0.0.0\u0026#39;, port = 80) FROM alpine:latest LABEL maintainer=\u0026#34;me@example.com\u0026#34; WORKDIR /app COPY . /app RUN apk add --no-cache python3 EXPOSE 80 CMD [\u0026#34;python3\u0026#34;, \u0026#34;./app.py\u0026#34;] podman build -t my-flask:latest . podman images Webserver # \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Hello World from Flask\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Hello, world!!!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Welcome to the web server...\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; FROM nginx:latest COPY index.html /usr/share/nginx/html podman build -t webserver . podman images podman run -d --name webserver -p 9090:80 webserver podman ps "},{"id":44,"href":"/docs/devops/linux/screen/","title":"Screen","section":"Linux","content":" Screen # Basics # Start screen session and give it a name: screen -S ScreenTest Detach from current session: Ctrl-A d List sessions: screen -ls Re-attach to an existing session: screen -r \u0026lt;PID\u0026gt; Re-attach session, if necessary detach or create: screen -dRR ScreenTest Force-exit screen: Ctrl-A Ctrl-\\ Kill session: screen -S ScreenTest -X quit Clean all dead screen sessions: screen -wipe "},{"id":45,"href":"/docs/devops/linux/searxng/","title":"Searxng","section":"Linux","content":" SearXNG # sudo apt update -y sudo apt upgrade -y sudo apt-get install -y python3-dev python3-babel python3-venv uwsgi uwsgi-plugin-python3 git build-essential libxslt-dev zlib1g-dev libffi-dev libssl-dev Install # Create user:\nsudo groupadd --system searxng sudo useradd --system --gid searxng --create-home --home-dir /var/lib/searxng --shell /bin/bash --comment \u0026#34;Privacy-respecting metasearch engine\u0026#34; searxng sudo -u searxng -i (searxng)$ git clone \u0026#34;https://github.com/searxng/searxng\u0026#34; \u0026#34;/var/lib/searxng/searxng-src\u0026#34; (searxng)$ python3 -m venv \u0026#34;/var/lib/searxng/searxng-pyenv\u0026#34; Autoload virtual enviroment in /var/lib/searxng/.profile:\n# Automatically load virtual environment if [ -f \u0026#34;$HOME/searxng-pyenv/bin/activate\u0026#34; ] ; then . \u0026#34;$HOME/searxng-pyenv/bin/activate\u0026#34; fi Exit and start in a new session. Make sure that pyenv was activated automatically:\n(searxng-pyenv)$ command -v python (searxng-pyenv)$ python --version Install dependencies in pyenv:\ncd $HOME/searxng-src pip install -U pip pip install -U setuptools pip install -U wheel pip install -U pyyaml pip install -e . Configure # sudo mkdir -p \u0026#34;/etc/searxng\u0026#34; sudo cp \u0026#34;/var/lib/searxng/searxng-src/utils/templates/etc/searxng/settings.yml\u0026#34; \u0026#34;/etc/searxng/settings.yml\u0026#34; Generate secret key:\nopenssl rand -hex 16 Update secret_key in the settings. Remove connection to Redis. Set port to 8001. Set bind_address to 127.0.0.1.\nuse_default_settings: true server: base_url: http://jdplxd01.home.arpa # change this! port: 8001 bind_address: \u0026#34;127.0.0.1\u0026#34; secret_key: \u0026#34;ultrasecretkey\u0026#34; # change this! limiter: false # can be disabled for a private instance image_proxy: true ui: static_use_hash: true redis: #url: redis://redis:6379/0 url: false Test interactively:\nsudo -u searxng -i cd /var/lib/searxng/searxng-src export SEARXNG_SETTINGS_PATH=\u0026#34;/etc/searxng/settings.yml\u0026#34; python searx/webapp.py curl --location --verbose --head --insecure 127.0.0.1:8001 uWSGI # sudo touch /etc/uwsgi/apps-available/searxng.ini sudo ln -s /etc/uwsgi/apps-available/searxng.ini /etc/uwsgi/apps-enabled/ Set configuration in /etc/uwsgi/apps-available/searxng.ini:\n[uwsgi] uid = searxng gid = searxng env = LANG=C.UTF-8 env = LANGUAGE=C.UTF-8 env = LC_ALL=C.UTF-8 chdir = /var/lib/searxng/searxng-src/searx env = SEARXNG_SETTINGS_PATH=/etc/searxng/settings.yml disable-logging = true chmod-socket = 666 single-interpreter = true master = true lazy-apps = true plugin = python3,http enable-threads = true module = searx.webapp virtualenv = /var/lib/searxng/searxng-pyenv pythonpath = /var/lib/searxng/searxng-src socket = /var/lib/searxng/run/socket buffer-size = 8192 static-map = /static=/usr/local/searxng/searxng-src/searx/static static-expires = /* 31557600 static-gzip-all = True offload-threads = %k cache2 = name=searxngcache,items=2000,blocks=2000,blocksize=4096,bitmap=1 Management of the service:\ncreate /etc/uwsgi/apps-available/searxng.ini enable: sudo -H ln -s /etc/uwsgi/apps-available/searxng.ini /etc/uwsgi/apps-enabled/ start: sudo -H service uwsgi start searxng restart: sudo -H service uwsgi restart searxng stop: sudo -H service uwsgi stop searxng disable: sudo -H rm /etc/uwsgi/apps-enabled/searxng.ini uwsgi \u0026ndash;socket 0.0.0.0:8001 \u0026ndash;protocol=http -w wsgi:searx.webapp sudo usermod -a -G www-data caddy\n[Unit] Description=SearXNG After=network.target [Service] User=searxng Group=www-data WorkingDirectory=/var/lib/searxng/searxng-src Environment=\u0026#34;SEARXNG_SETTINGS_PATH=/etc/searxng/settings.yml\u0026#34; ExecStart=python searx/webapp.py [Install] WantedBy=multi-user.target "},{"id":46,"href":"/docs/devops/linux/setup/","title":"Setup","section":"Linux","content":" Setup # Boot in text mode # Change the grub configuration file (/etc/default/grub):\n# GRUB_CMDLINE_LINUX_DEFAULT=\u0026#34;quiet splash\u0026#34; GRUB_CMDLINE_LINUX=\u0026#34;text\u0026#34; GRUB_TERMINAL=console sudo update-grub sudo systemctl set-default multi-user.target shutdown -r now SSH # sudo apt update sudo apt install openssh-server sudo systemctl status ssh sudo ufw allow ssh Check network configuration:\nsudo apt install net-tools ifconfig Docker # sudo apt install docker.io -y sudo apt install docker-compose -y Git # "},{"id":47,"href":"/docs/devops/linux/ubuntu/","title":"Ubuntu","section":"Linux","content":" Ubuntu # Get network configuration:\nip a s ifconfig -a Static IP in /etc/netplan/10-hostname.yaml:\nnetwork: version: 2 ethernets: eth0: dhcp4: false dhcp6: false dhcp-identifier: mac addresses: - 192.168.0.11/24 routes: - to: default via: 192.168.0.1 nameservers: addresses: - 192.168.0.1 Check disk size:\nlsblk --scsi sudo fdisk -l /dev/sda sudo parted -l df -h Check memory:\nfree Resize logical volume to all disk:\nvgdisplay lvdisplay lvextend -l +100%FREE /dev/ubuntu-vg/ubuntu-lv resize2fs /dev/mapper/ubuntu--vg-ubuntu--lv Generalize cloud image # sudo cloud-init clean sudo rm -rf /var/lib/cloud/instances sudo truncate -s 0 /etc/machine-id sudo rm /var/lib/dbus/machine-id sudo ln -s /etc/machine-id /var/lib/dbus/machine-id sudo poweroff LXC server setup # Add user and install basic software:\nadduser david usermod -aG sudo david sudo apt update sudo apt dist-upgrade sudo apt install curl wget git neovim mc tmux python3 jq golang sqlite3 autofs ca-certificates net-tools sudo wget https://github.com/mikefarah/yq/releases/download/v4.44.1/yq_linux_amd64 -O /usr/local/bin/yq sudo chmod +x /usr/local/bin/yq wget https://github.com/google/go-jsonnet/releases/download/v0.20.0/jsonnet-go_0.20.0_linux_amd64.deb -O /tmp/jsonnet.deb sudo apt install /tmp/jsonnet.deb rm /tmp/jsonnet.deb curl --fail --location --progress-bar --output /tmp/deno.zip https://github.com/denoland/deno/releases/download/v1.44.1/deno-x86_64-unknown-linux-gnu.zip sudo unzip -d /usr/local/bin -o /tmp/deno.zip rm /tmp/deno.zip wget https://github.com/go-task/task/releases/download/v3.37.2/task_linux_amd64.deb -O /tmp/task.deb sudo apt install /tmp/task.deb rm /tmp/task.deb Generalize image:\nsudo apt clean sudo apt autoremove sudo rm /etc/ssh/ssh_host_* sudo truncate -s 0 /etc/machine-id sudo poweroff Prepare container after creation from image:\nsudo dpkg-reconfigure openssh-server Install K3s in Proxmox LXC container:\nIn Proxmox host, change /etc/sysctl.conf:\nnet.ipv4.ip_forward=1 vm.swapiness=0 Disable swap in /etc/fstab and on command line: swapoff -a\nEdit container confirguration in /etc/pve/lxc/$ContainerID.conf:\nfeatures: nesting=1 swap: 0 lxc.apparmor.profile: unconfined lxc.cgroup2.devices.allow: a lxc.cap.drop: lxc.mount.auto: \u0026#34;proc:rw sys:rw\u0026#34; Create /etc/rc.local:\n#!/bin/sh -e if [ ! -e /dev/kmsg ]; then ln -s /dev/console /dev/kmsg fi mount --make-rshared / Make it executable nad run it:\nchmod +x /etc/rc.local /etc/rc.local Install K3s:\ncurl -sfL https://get.k3s.io | sh - systemctl status k3s sudo cat /etc/rancher/k3s/k3s.yaml Setup NFS storage for persistent volumes:\nCheck that storage is exported: showmount -e xxx\nCreate /var/lib/rancher/k3s/server/manifests/nfs.yaml:\n--- apiVersion: v1 kind: Namespace metadata: name: nfs --- apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: nfs namespace: nfs spec: chart: nfs-subdir-external-provisioner repo: https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner targetNamespace: nfs set: nfs.server: x.x.x.x nfs.path: /Kubernetes storageClass.name: nfs Check kubectl get storageclasses.\n"},{"id":48,"href":"/docs/devops/macos/homebrew/","title":"Homebrew","section":"MacOS","content":" Homebrew # Install # Download package from https://github.com/Homebrew/brew/releases/.\nIt installs in /opt/homebrew.\nConfiguration # Add line to ~/.zprofile:\neval \u0026#34;$(/opt/homebrew/bin/brew shellenv)\u0026#34; Usage # brew help brew analytics off brew doctor brew update brew shellenv brew cleanup brew search xxx brew info xxx brew install xxx brew reinstall xxx brew uninstall xxx brew bundle dump brew bundle install --file ./Brewfile Hashicorp tools # Install:\nbrew tap hashicorp/tap brew install hashicorp/tap/terraform Upgrade:\nbrew update brew upgrade hashicorp/tap/terraform "},{"id":49,"href":"/docs/devops/macos/java/","title":"Java","section":"MacOS","content":" Java on MacOS # Install with homebrew:\nbrew search java brew info java brew install java sudo ln -sfn /opt/homebrew/opt/openjdk/libexec/openjdk.jdk /Library/Java/JavaVirtualMachines/openjdk.jdk Check:\nls -lsah /Library/Java/JavaVirtualMachines/ java -version /usr/libexec/java_home -V Setup JAVA_HOME in ~/.zshenv:\nexport JAVA_HOME=$(/usr/libexec/java_home -v\u0026#34;21.0.1\u0026#34;) Refresh shell: source ~/.zshenv\nInstall Maven:\nbrew install maven mvn -version brew list maven Settings are in /opt/homebrew/opt/maven/libexec/conf/settings.xml.\nTo upgrade or uninstall:\nbrew upgrade maven brew uninstall maven Install Java 8:\nbrew search openjdk brew install openjdk@8 # Cannot install - it requires x86_64 architecture "},{"id":50,"href":"/docs/devops/macos/multipass/","title":"Multipass","section":"MacOS","content":" Multipass # Install with package from https://multipass.run/install.\nThe command is installed in /usr/local/bin/multipass.\nLogs are in /Library/Logs/Multipass/multipassd.log.\nOr use brew to install:\nbrew install --cask multipass Usage:\nmultipass networks multipass set local.bridged-network=en0 multipass launch docker --name jdp --cpus 2 --memory 6G --disk 50G --bridged multipass info jdp multipass shell jdp multipass start jdp multipass restart jdp multipass stop jdp multipass delete jdp multipass launch docker --cpus 2 --memory 6G --disk 50G multipass list multipass info --all multipass purge Config # Add to ~/.zprofile:\nPATH=\u0026#34;${PATH}:/Users/david/Library/Application Support/multipass/bin\u0026#34; Applications to install:\nKubeCtl K3D Inside Ubuntu, install additional tools:\n# kubectl curl -LO https://dl.k8s.io/release/v1.28.3/bin/linux/arm64/kubectl sudo chmod +x kubectl sudo mv kubectl /usr/local/bin # K3D sudo curl -sfL https://github.com/k3d-io/k3d/releases/download/v5.6.0/k3d-linux-arm64 --output k3d sudo chmod +x k3d sudo mv k3d /usr/local/bin In MacOS terminal:\nmultipass mount $HOME jdp multipass mount /opt/jdp/src jdp multipass alias docker:kubectl kubectl multipass alias docker:k3d k3d MacOS firewall has problems.\nIf you see errors related to /var/db/dhcpd_leases your firewall is likely blocking the bootpd process.\nOn unmanaged Mac, you can try to unblock bootpd from the MacOS builtin firewall:\n/usr/libexec/ApplicationFirewall/socketfilterfw --add /usr/libexec/bootpd /usr/libexec/ApplicationFirewall/socketfilterfw --unblock /usr/libexec/bootpd "},{"id":51,"href":"/docs/devops/macos/python/","title":"Python","section":"MacOS","content":" Python on MacOS # brew update brew install pyenv brew install pyenv-virtualenv Add to ~/.zshenv:\neval \u0026#34;$(pyenv init -)\u0026#34; eval \u0026#34;$(pyenv virtualenv-init -)\u0026#34; Check:\npyenv root Install python:\npyenv install -l pyenv install 3.12.0 pyenv global 3.12.0 pyenv version Prepare virtual environment for LLM:\npyenv virtualenv 3.12.0 llm pyenv global llm pip install langchain Install packages:\npip install --upgrade pip pip install awscli "},{"id":52,"href":"/docs/devops/packer/installation/","title":"Installation","section":"Packer","content":" Installation # Windows $pkr_version = \u0026ldquo;1.9.4\u0026rdquo; $pkr_archive = \u0026ldquo; https://releases.hashicorp.com/packer/${pkr_version}/packer_${pkr_version}_windows_amd64.zip\" Invoke-WebRequest -Uri $pkr_archive -OutFile \u0026ldquo;${pwd}/packer_${pkr_version}_windows_amd64.zip\u0026rdquo;\nAdd-Type -Assembly System.IO.Compression.FileSystem $zip = [IO.Compression.ZipFile]::OpenRead(\u0026quot;${pwd}/packer_${pkr_version}windows_amd64.zip\u0026rdquo;) $Executables = $zip.Entries | Where-Object {$.Name -like \u0026lsquo;*.exe\u0026rsquo;} foreach ($Executable in $Executables) { [System.IO.Compression.ZipFileExtensions]::ExtractToFile($Executable, \u0026ldquo;${pwd}/$($Executable.Name)\u0026rdquo;, $True) } $zip.Dispose() Remove-Item -Path \u0026ldquo;${pwd}/packer_${pkr_version}_windows_amd64.zip\u0026rdquo;\nLinux export PACKER_VERSION=1.7.10 curl \\ -sSL https://releases.hashicorp.com/packer/${PACKER_VERSION}/packer_${PACKER_VERSION}_linux_amd64.zip \\ -o /tmp/packer_linux_amd64.zip cd /usr/local/bin unzip /tmp/packer_linux_amd64.zip rm /tmp/packer_linux_amd64.zip Example:\npacker validate src/build-agent-ubuntu.pkr.hcl packer build src/build-agent-ubuntu.pkr.hcl "},{"id":53,"href":"/docs/devops/terraform/aws/","title":"Aws","section":"Terraform","content":" AWS samples # Web server with load balancing and auto-scaling # provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-east-2\u0026#34; } variable \u0026#34;server_port\u0026#34; { description = \u0026#34;The port on which the HTTP server will run\u0026#34; default = 8080 type = number validation { condition = var.server_port \u0026gt; 0 \u0026amp;\u0026amp; var.server_port \u0026lt; 65536 error_message = \u0026#34;Port must be between 1 and 65535\u0026#34; } sensitive = true } data \u0026#34;aws_vpc\u0026#34; \u0026#34;this\u0026#34; { default = true } data \u0026#34;aws_subnets\u0026#34; \u0026#34;this\u0026#34; { filter { name = \u0026#34;vpc-id\u0026#34; values = [data.aws_vpc.this.id] } } resource \u0026#34;aws_security_group\u0026#34; \u0026#34;this\u0026#34; { name = \u0026#34;allow-web\u0026#34; ingress { from_port = var.server_port to_port = var.server_port protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } } resource \u0026#34;aws_security_group\u0026#34; \u0026#34;alb\u0026#34; { name = \u0026#34;web-alb\u0026#34; ingress { from_port = 80 to_port = 80 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } } resource \u0026#34;aws_launch_configuration\u0026#34; \u0026#34;this\u0026#34; { image_id = \u0026#34;ami-0c94855ba95c574c8\u0026#34; instance_type = \u0026#34;t3.micro\u0026#34; security_groups = [aws_security_group.this.id] user_data = \u0026lt;\u0026lt;-EOF #!/bin/bash echo \u0026#34;Hello, World!\u0026#34; \u0026gt; index.html nohup busybox httpd -f -p ${var.server_port} \u0026amp; EOF lifecycle { create_before_destroy = true } } resource \u0026#34;aws_autoscaling_group\u0026#34; \u0026#34;this\u0026#34; { launch_configuration = aws_launch_configuration.this.name vpc_zone_identifier = data.aws_subnets.this.ids target_group_arns = [aws_lb_target_group.this.arn] health_check_type = \u0026#34;ELB\u0026#34; min_size = 2 max_size = 4 tag { key = \u0026#34;Name\u0026#34; value = \u0026#34;web\u0026#34; propagate_at_launch = true } } resource \u0026#34;aws_lb\u0026#34; \u0026#34;this\u0026#34; { name = \u0026#34;web\u0026#34; load_balancer_type = \u0026#34;application\u0026#34; subnets = data.aws_subnets.this.ids security_groups = [aws_security_group.alb.id] } resource \u0026#34;aws_lb_listener\u0026#34; \u0026#34;http\u0026#34; { load_balancer_arn = aws_lb.this.arn port = 80 protocol = \u0026#34;HTTP\u0026#34; default_action { type = \u0026#34;fixed-response\u0026#34; fixed_response { content_type = \u0026#34;text/plain\u0026#34; message_body = \u0026#34;404: page not found\u0026#34; status_code = 404 } } } resource \u0026#34;aws_lb_target_group\u0026#34; \u0026#34;this\u0026#34; { name = \u0026#34;web-example\u0026#34; port = var.server_port protocol = \u0026#34;HTTP\u0026#34; vpc_id = data.aws_vpc.this.id health_check { path = \u0026#34;/\u0026#34; protocol = \u0026#34;HTTP\u0026#34; matcher = \u0026#34;200\u0026#34; interval = 15 timeout = 3 healthy_threshold = 2 unhealthy_threshold = 2 } } resource \u0026#34;aws_lb_listener_rule\u0026#34; \u0026#34;this\u0026#34; { listener_arn = aws_lb_listener.http.arn priority = 100 condition { path_pattern { values = [\u0026#34;*\u0026#34;] } } action { type = \u0026#34;forward\u0026#34; target_group_arn = aws_lb_target_group.this.arn } } output \u0026#34;alb_dns_name\u0026#34; { description = \u0026#34;DNS name of the application load balancer\u0026#34; value = aws_lb.this.dns_name sensitive = false } #!/bin/bash set -o xtrace terraform apply --auto-approve ALB=$(terraform output alb_dns_name | tr -d \u0026#39;\u0026#34;\u0026#39;) curl http://${ALB}/ "},{"id":54,"href":"/docs/devops/terraform/basics/","title":"Basics","section":"Terraform","content":" Basic commands # Resource management # terraform init terraform plan terraform apply terraform destroy terraform graph terraform output Work with code # terraform validate terraform fmt Work with state # terraform state list terraform state mv terraform state show terraform state rm \\\u0026lt;type\\\u0026gt;.\\\u0026lt;resource\\\u0026gt; Examples:\n[System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String((kubectl get secret api-secret -o jsonpath=\u0026#39;{.data.db-connectionstring}\u0026#39;))) terragrunt plan -var-file secrets.tfvars --terragrunt-log-level debug az ad group show --group xxx-Contributors --query id --output tsv "},{"id":55,"href":"/docs/devops/terraform/installation/","title":"Installation","section":"Terraform","content":" Installation # Linux Install tfswitch:\nexport TFSWITCH_VERSION=0.13.1300 curl \\ -sSL https://github.com/warrensbox/terraform-switcher/releases/download/${TFSWITCH_VERSION}/terraform-switcher_${TFSWITCH_VERSION}_linux_amd64.tar.gz \\ -o /tmp/tfswitch_linux_amd64.tar.gz tar --directory=/usr/local/bin -xf /tmp/tfswitch_linux_amd64.tar.gz tfswitch rm /tmp/tfswitch_linux_amd64.tar.gz Install terraform:\ntfswitch 1.3.7 Install tflint:\nexport TFLINT_VERSION=0.38.1 curl \\ -sSL https://github.com/terraform-linters/tflint/releases/download/v${TFLINT_VERSION}/tflint_linux_amd64.zip \\ -o /tmp/tflint_linux_amd64.zip cd /usr/local/bin unzip /tmp/tflint_linux_amd64.zip rm /tmp/tflint_linux_amd64.zip MacOS brew tap hashicorp/tap brew install hashicorp/tap/terraform Windows $tf_version = \u0026#34;1.6.4\u0026#34; $tf_archive = \u0026#34;https://releases.hashicorp.com/terraform/${tf_version}/terraform_${tf_version}_windows_amd64.zip\u0026#34; Invoke-WebRequest -Uri $tf_archive -OutFile \u0026#34;${pwd}/terraform_${tf_version}_windows_amd64.zip\u0026#34; Add-Type -Assembly System.IO.Compression.FileSystem $zip = [IO.Compression.ZipFile]::OpenRead(\u0026#34;${pwd}/terraform_${tf_version}_windows_amd64.zip\u0026#34;) $Executables = $zip.Entries | Where-Object {$_.Name -like \u0026#39;*.exe\u0026#39;} foreach ($Executable in $Executables) { [System.IO.Compression.ZipFileExtensions]::ExtractToFile($Executable, \u0026#34;${pwd}/$($Executable.Name)\u0026#34;, $True) } $zip.Dispose() Remove-Item -Path \u0026#34;${pwd}/terraform_${tf_version}_windows_amd64.zip\u0026#34; "},{"id":56,"href":"/docs/devops/terragrunt/basics/","title":"Basics","section":"Terragrunt","content":" Basic commands # Work with state # terragrunt state pull \u0026gt; backup.tfstate terragrunt state push backup.tfstate "},{"id":57,"href":"/docs/devops/vagrant/installation/","title":"Installation","section":"Vagrant","content":" Installation # Windows # Install Hyper-V Check Samba configuration Configure Hyper-V as default Vagrant provider Install Vagrant\u0026rsquo;s reload plugin Create NAT switch for Hyper-V Get-SmbServerConfiguration [Environment]::SetEnvironmentVariable(\u0026#34;VAGRANT_DEFAULT_PROVIDER\u0026#34;, \u0026#34;hyperv\u0026#34;, \u0026#34;User\u0026#34;) vagrant plugin install vagrant-reload # Get-NetAdapter | Format-Table -AutoSize Get-VMSwitch | Select-Object -ExpandProperty Name # if NatSwitch not listed New-VMSwitch -SwitchName \u0026#34;NATSwitch\u0026#34; -SwitchType Internal Get-NetIPAddress | Select-Object -ExpandProperty IPAddress # if 192.168.200.1 not listed New-NetIPAddress -IPAddress 192.168.200.1 -PrefixLength 24 -InterfaceAlias \u0026#34;vEthernet (NATSwitch)\u0026#34; Get-NetNAT | Select-Object -ExpandProperty InternalIPInterfaceAddressPrefix # if 192.168.200.0/24 not listed New-NetNAT -Name \u0026#34;NATNetwork\u0026#34; -InternalIPInterfaceAddressPrefix 192.168.200.0/24 Vagrantfile # This template is ready to create several VMs on Hyper-V and set static address to all of them.\n# Groovy Gorilla Vagrant.configure(\u0026#34;2\u0026#34;) do |config| servers=[ { :hostname =\u0026gt; \u0026#34;srv01\u0026#34;, :box =\u0026gt; \u0026#34;generic/ubuntu2010\u0026#34;, :ip =\u0026gt; \u0026#34;192.168.200.101\u0026#34;, :ssh_port =\u0026gt; \u0026#39;2201\u0026#39;, :memory =\u0026gt; 512, :cpus =\u0026gt; 1 } ] gateway=\u0026#34;192.168.200.1\u0026#34; servers.each do |srv| config.vm.define srv[:hostname] do |node| node.vm.box = srv[:box] node.vm.hostname = srv[:hostname] node.vm.provider :hyperv node.vm.network :public_network, auto_config: false node.vm.network \u0026#34;forwarded_port\u0026#34;, guest: 22, host: srv[:ssh_port], id: \u0026#34;ssh\u0026#34; node.vm.synced_folder \u0026#34;.\u0026#34;, \u0026#34;/vagrant\u0026#34;, disabled: true node.vm.provision \u0026#34;shell\u0026#34;, path: \u0026#34;./scripts/configure-static-ip.sh\u0026#34;, :args =\u0026gt; \u0026#34;#{srv[:ip]} #{gateway}\u0026#34; servers.each do |host| node.vm.provision \u0026#34;shell\u0026#34;, path: \u0026#34;./scripts/add-hosts-entry.sh\u0026#34;, :args =\u0026gt; \u0026#34;#{host[:ip]} #{host[:hostname]}\u0026#34; end node.vm.provision :reload node.vm.provider :hyperv do |h| h.vmname = srv[:hostname] h.enable_virtualization_extensions = true h.linked_clone = true h.maxmemory = 2048 h.memory = srv[:memory] h.cpus = srv[:cpus] end end end end Script to update /etc/hosts file in scripts/add-hosts-entry.sh:\n#!/bin/sh echo -n \u0026#39;Adding IP to hosts file: \u0026#39; echo $1 echo -n \u0026#39;Host name is: \u0026#39; echo $2 echo \u0026#34;$1 $2\u0026#34; \u0026gt;\u0026gt; /etc/hosts Ubuntu scripts/configure-static-ip.sh # #!/bin/sh echo -n \u0026#39;Setting static IP address: \u0026#39; echo $1 echo -n \u0026#39;Setting gateway address: \u0026#39; echo $2 cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/netplan/01-netcfg.yaml network: version: 2 ethernets: eth0: dhcp4: no addresses: [$1/24] gateway4: $2 nameservers: addresses: [8.8.8.8,8.8.4.4] EOF CentOS scripts/configure-static-ip.sh # #!/bin/sh echo -n \u0026#39;Setting static IP address: \u0026#39; echo $1 echo -n \u0026#39;Setting gateway address: \u0026#39; echo $2 cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=eth0 BOOTPROTO=none ONBOOT=yes PREFIX=24 IPADDR=$1 GATEWAY=$2 DNS1=8.8.8.8 EOF Commands # The Hyper-V provider requires that Vagrant be run with administrative privileges. This is a limitation of Hyper-V itself.\nvagrant up vagrant ssh srv01 vagrant halt vagrant destroy Username is vagrant, password is vagrant.\n"},{"id":58,"href":"/docs/devops/windows/ime/","title":"Ime","section":"Windows","content":" Input methods # Xiaohe shuangpin 小鹤双拼 # Windows Registry Editor Version 5.00\r[HKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\InputMethod\\Settings\\CHS]\r\u0026#34;Enable Double Pinyin\u0026#34;=dword:00000001\r\u0026#34;DoublePinyinScheme\u0026#34;=dword:0000000a\r\u0026#34;UserDefinedDoublePinyinScheme0\u0026#34;=\u0026#34;小鹤双拼*2*^*iuvdjhcwfg^xmlnpbksqszxkrltvyovt\u0026#34; "},{"id":59,"href":"/docs/devops/windows/winget/","title":"Winget","section":"Windows","content":" Winget # winget.exe install --id Microsoft.Powershell --source winget Git # winget install --id Git.Git -e --source winget After installation, copy SSH keys to ~\\.ssh.\n"},{"id":60,"href":"/docs/devops/wsl/installation/","title":"Installation","section":"WSL","content":" Installation of WSL # Basic commands:\nwsl --list --online wsl --list wsl --install -d \u0026lt;distro\u0026gt; wsl -d \u0026lt;distro\u0026gt; wsl --terminate \u0026lt;distro\u0026gt; WSL2 # Enable Windows subsystem for Linux:\nEnable-WindowsOptionalFeature -Online -NoRestart -FeatureName Microsoft-Windows-Subsystem-Linux Enable-WindowsOptionalFeature -Online -NoRestart -FeatureName VirtualMachinePlatform Enable version 2 of WSL:\nInvoke-WebRequest https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi -OutFile c:\\wsl_update_x64.msi -UseBasicParsing Invoke-WebRequest https://... -OutFile D:\\dat\\WSL\\kernel\\vmlinux -UseBasicParsing start D:\\dat\\WSL\\wsl_update_x64.msi wsl --set-default-version 2 Configure non-default kernel in %USERPROFILE%\\.wslconfig:\n[wsl2] kernel=D:\\\\dat\\\\WSL\\\\kernel\\\\vmlinux Import Ubuntu image from docker # Download image: Invoke-WebRequest https://raw.githubusercontent.com/tianon/docker-brew-ubuntu-core/fbca80af7960ffcca085d509c20f53ced1697ade/kinetic/ubuntu-kinetic-oci-amd64-root.tar.gz -OutFile C:\\ubuntu-kinetic-oci-amd64-root.tar.gz -UseBasicParsing Import image: wsl --import jdp-ubuntu C:\\dat\\WSL\\jdp-ubuntu C:\\ubuntu-kinetic-oci-amd64-root.tar.gz Update system: apt-get update Import Arch image from docker # Download image: Invoke-WebRequest https://gitlab.archlinux.org/archlinux/archlinux-docker/-/package_files/2816/download -OutFile C:\\arch-base-20220704.0.66039.tar.zst -UseBasicParsing Import image: wsl --import jdp-arch C:\\dat\\WSL\\jdp-arch C:\\arch-base-20220704.0.66039.tar Uncomment locale file: /etc/locale.gen Generate locale: locale-gen Initialize key: pacman-key --init Update system: pacman -Syu Mount host directory # Create directory in WSL: mkdir /src\nEdit /etc/fstab and add line: C:/src /src drvfs defaults 0 0\nReload the fstab file: sudo mount -a\nEnable SystemD # Install store version of WSL: https://aka.ms/wslstorepage\nEnable systemd inside distribution by editting /etc/wsl.conf:\n[boot] systemd=true Reboot WSL and test:\nsystemctl list-units --type=service systemctl list-unit-files --type=service "},{"id":61,"href":"/docs/devops/wsl/ubuntu2004/","title":"Ubuntu2004","section":"WSL","content":" Ubuntu 20.04 # Invoke-WebRequest -Uri https://aka.ms/wslubuntu2004 -OutFile c:\\ubuntu2004.appx -UseBasicParsing Add-AppxPackage c:\\ubuntu2004.appx # DISM.EXE /Online /Add-ProvisionedAppxPackage /PackagePath:c:\\\\ubuntu2004.appx /SkipLicense --AllUsers Start Ubuntu 20.04 console to install the base system, and then run:\nsudo apt update sudo apt upgrade Docker in Ubuntu 20.04 # sudo apt install --no-install-recommends apt-transport-https ca-certificates curl gnupg2 net-tools source /etc/os-release curl -fsSL https://download.docker.com/linux/${ID}/gpg | sudo apt-key add - echo \u0026#34;deb [arch=amd64] https://download.docker.com/linux/${ID} ${VERSION_CODENAME} stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list sudo apt update sudo apt install docker-ce docker-ce-cli containerd.io docker-compose "},{"id":62,"href":"/docs/devops/wsl/ubuntu2204/","title":"Ubuntu2204","section":"WSL","content":" Ubuntu 22.04 # Update the system:\nsudo apt update sudo apt -y upgrade RDP connections # Install xrdp and xwindows:\nsudo apt install -y xrdp sudo apt install -y xfce4 xfce4-goodies sudo sed -i \u0026#39;s/3389/3390/g\u0026#39; /etc/xrdp/xrdp.ini sudo sed -i \u0026#39;s/max_bpp=32/max_bpp=128/g\u0026#39; /etc/xrdp/xrdp.ini sudo sed -i \u0026#39;s/xserverbpp=24/xserverbpp=128/g\u0026#39; /etc/xrdp/xrdp.ini echo xfce4-session \u0026gt; ~/.xsession Comment out and add line in /etc/xrdp/startwm.sh:\n# test -x /etc/X11/Xsession \u0026amp;\u0026amp; exec /etc/X11/Xsession # exec /bin/sh /etc/X11/Xsession startxfce4 Start XRDP:\nsudo /etc/init.d/xrdp start Connect in RDP to localhost:3390.\nInstall SystemD # curl -L -O \u0026#34;https://raw.githubusercontent.com/nullpo-head/wsl-distrod/main/install.sh\u0026#34; chmod +x install.sh sudo ./install.sh install /opt/distrod/bin/distrod enable Restart distribution:\nwsl --terminate Ubuntu After restart, check the init system:\nps -o comm 1 Enable LXD # Install LXD:\nsnap info lxd sudo snap install lxd sudo lxd init Check available containers:\nsudo lxc ls List available images:\nsudo lxc image list ubuntu: 22.04 architecture=x86_64 Create container:\nsudo lxc launch images:ubuntu/22.04 sudo lxc exec closing-gazelle -- su --login ubuntu List local images:\nsudo lxc image list Delete container:\nsudo lxc stop closing-gazelle sudo lxc delete closing-gazelle Delete image:\nsudo lxc image list sudo lxc image delete a3e17f0f0cbc Create containers with names:\nlxc launch images:centos/7 haproxy lxc launch ubuntu:18.04 controller-0 --profile k8s lxc launch ubuntu:18.04 controller-1 --profile k8s lxc launch ubuntu:18.04 controller-2 --profile k8s lxc launch ubuntu:18.04 worker-0 --profile k8s lxc launch ubuntu:18.04 worker-1 --profile k8s lxc launch ubuntu:18.04 worker-2 --profile k8s lxc list "},{"id":63,"href":"/docs/other/books/snippets/","title":"Snippets","section":"Books","content":" Snippets # Export PDF to PNG # Import-Module -Name D:\\src\\github.com\\dpurge\\jdp-psmodule\\src\\JdpBookbind Invoke-Book2Image -OutputDirectory img -InputFile book.pdf Merge two directories with images # $a = Get-ChildItem -Filter *.png ./img1 $b = Get-ChildItem -Filter *.png ./img2 $imgs = $a + $b $i = 0 foreach ($img in $imgs) { $f = \u0026#34;./img/page-{0:d3}.png\u0026#34; -f $i Write-Host \u0026#34;$img -\u0026gt; $f\u0026#34; Move-Item -Path $img -Destination $f $i++ } Render PDF pages # 0..350 | %{D:\\pgm\\ImageMagick\\convert.exe -density 300 book.pdf[$_] -quality 100 (\u0026#34;./img/page-{0:d3}.tif\u0026#34; -f $_)} 0..350 | %{D:\\pgm\\ImageMagick\\convert.exe -density 300 book.pdf[$_] -quality 100 -type Grayscale -filter Lanczos (\u0026#34;./img/page-{0:d3}.png\u0026#34; -f $_)} Convert to grayscale # $files = Get-ChildItem -Filter *.png foreach($file in $files) { $outfile = \u0026#34;${pwd}\\out\\$($file.Name)\u0026#34; Write-Host \u0026#34;$file -\u0026gt; $outfile\u0026#34; # D:\\pgm\\ImageMagick\\convert.exe -grayscale Rec709Luminance $file $outfile D:\\pgm\\ImageMagick\\convert.exe -negate $file $outfile # D:\\pgm\\ImageMagick\\convert.exe $file -depth 4 -colorspace gray -define png:color-type=0 -define png:bit-depth=4 $outfile } Convert FLAC to MP3 # $files = Get-ChildItem -Filter *.flac foreach($file in $files) { $outfile = ($file.Fullname -replace \u0026#39;.flac$\u0026#39;,\u0026#39;.mp3\u0026#39;) Write-Host \u0026#34;$file -\u0026gt; $outfile\u0026#34; D:\\pgm\\ffmpeg\\bin\\ffmpeg.exe -i $file -ab 320k -map_metadata 0 -id3v2_version 3 $outfile } Convert WMA to MP3 # $files = Get-ChildItem -Filter *.wma foreach($file in $files) { $outfile = ($file.Fullname -replace \u0026#39;.wma$\u0026#39;,\u0026#39;.mp3\u0026#39;) Write-Host \u0026#34;$file -\u0026gt; $outfile\u0026#34; D:\\pgm\\ffmpeg\\bin\\ffmpeg.exe -i $file -ab 192k $outfile } Convert WEBM to MP3 # D:\\pgm\\ffmpeg\\bin\\ffmpeg.exe -i nagranie.webm -vn -ab 128k -ar 44100 -y nagranie.mp3 Download from YouTube # youtube-dl -x --audio-format mp3 \u0026lt;Video-URL\u0026gt; youtube-dl -f bestaudio[ext=m4a] --embed-thumbnail --add-metadata \u0026lt;Video-URL\u0026gt; Convert PNG to SVG # $files = Get-ChildItem -Filter *.png foreach($pngfile in $files) { $bmpfile = ($pngfile.Fullname -replace \u0026#39;([^.]).png\u0026#39;,\u0026#39;.bmp\u0026#39;) $svgfile = ($pngfile.Fullname -replace \u0026#39;([^.]).png\u0026#39;,\u0026#39;.svg\u0026#39;) Write-Host \u0026#34;$pngfile -\u0026gt; $bmpfile -\u0026gt; $svgfile\u0026#34; D:\\pgm\\ImageMagick\\convert.exe $pngfile $bmpfile potrace.exe -s -a0 $bmpfile } Print scanned book for binding # $sheets = 8 $pages = 4 * $sheets $files = Get-ChildItem -Filter *.png $blank = $files[0] $batches=for($i=0; $i -lt $files.length; $i+=$pages){ ,($files[$i..($i + $pages - 1)])} foreach ($i in 1..($pages - $batches[-1].length)) { $batches[-1] += $blank } $batchnr = 0 foreach($batch in $batches) { $batchnr += 1 $outfile = \u0026#34;../batch-{0:d2}.pdf\u0026#34; -f $batchnr $printBatch = @() foreach ($i in 0..($batch.length / 4 - 1)) { $printBatch += $batch[($pages - 1 - 2 * $i)] # last page $printBatch += $batch[(2 * $i)] # first page $printBatch += $batch[(2 * $i + 1)] # second page $printBatch += $batch[($pages - 1 - 2 * $i - 1)] # last-but-one page } Write-Host $outfile Write-Host $printBatch.basename D:\\pgm\\ImageMagick\\convert.exe $printBatch $outfile } "},{"id":64,"href":"/docs/programming/awk/basics/","title":"Basics","section":"Awk","content":" Awk basics # "},{"id":65,"href":"/docs/programming/bash/basics/","title":"Basics","section":"Bash","content":" Bash basics # Get unix timestamp # date +%s "},{"id":66,"href":"/docs/programming/batch/basics/","title":"Basics","section":"Batch","content":" Batch basics # Map drive to directory # Create mapping:\nsubst G: C:\\jdp\\src\\github.com\\dpurge Remove mapping:\nsubst G: /D "},{"id":67,"href":"/docs/programming/csharp/basics/","title":"Basics","section":"C#","content":" C# Basics # "},{"id":68,"href":"/docs/programming/go/basics/","title":"Basics","section":"Go","content":" Go Basics # Project template # Set up:\nmkdir example-project cd ./example-project go mod init ./.gitignore:\n./Makefile:\n.PHONY: clean build test clean: echo \u0026#34;Not implemented\u0026#34; build: echo \u0026#34;Not implemented\u0026#34; test: echo \u0026#34;Not implemented\u0026#34; install: echo \u0026#34;Not implemented\u0026#34; ./main.go:\npackage main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;Hello, World!\u0026#34;) } Run project:\ngo run . Taskfile # Task file home Task file repository "},{"id":69,"href":"/docs/programming/go/cgi/","title":"Cgi","section":"Go","content":" CGI # Simple CGI app # package main import ( \u0026#34;bytes\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; ) func main() { var buf bytes.Buffer fmt.Fprintf(\u0026amp;buf, \u0026#34;Time at %s: %s\\n\u0026#34;, os.Getenv(\u0026#34;SERVER_NAME\u0026#34;), time.Now().Format(time.RFC1123)) fmt.Println(\u0026#34;Content-type: text/plain\u0026#34;) fmt.Printf(\u0026#34;Content-Length: %d\\n\\n\u0026#34;, buf.Len()) buf.WriteTo(os.Stdout) } "},{"id":70,"href":"/docs/programming/go/installation/","title":"Installation","section":"Go","content":" Installation # Linux ```sh\rsudo apt update\rsudo apt upgrade -y\rsudo apt install golang -y\r```\rWindows $go_version = \u0026quot;1.21.4\u0026quot;\r$go_archive = \u0026quot;https://dl.google.com/go/go${go_version}.windows-amd64.zip\u0026quot;\rInvoke-WebRequest -Uri $go_archive -OutFile \u0026quot;${pwd}/go${go_version}.windows-amd64.zip\u0026quot;\rAdd-Type -Assembly System.IO.Compression.FileSystem\r[System.IO.Compression.ZipFile]::ExtractToDirectory(\u0026quot;${pwd}/go${go_version}.windows-amd64.zip\u0026quot;, \u0026quot;${pwd}\u0026quot;)\rRemove-Item -Path \u0026quot;${pwd}/go${go_version}.windows-amd64.zip\u0026quot;\r$path_list = [System.Environment]::GetEnvironmentVariable('PATH', 'Machine') -split ';'\rif ($path_list -notcontains \u0026quot;${pwd}\\go\\bin\u0026quot;) {\r$path_string = ($path_list + \u0026quot;${pwd}\\go\\bin\u0026quot;) -join ';'\r[System.Environment]::SetEnvironmentVariable('PATH', $path_string, 'Machine')\r}\rCheck version: go version\n"},{"id":71,"href":"/docs/programming/powershell/azdo/","title":"Azdo","section":"Powershell","content":" Azure DevOps # URIs and authentication header # $AccessToken = $env:AZDO_ADMIN_TOKEN $Organization = \u0026#34;MyOrganization\u0026#34; $AuthenicationHeader = @{ Authorization = \u0026#39;Basic \u0026#39; + [Convert]::ToBase64String([Text.Encoding]::ASCII.GetBytes(\u0026#34;:${AccessToken}\u0026#34;)) } $OrganizationUri = \u0026#34;https://dev.azure.com/${Organization}/\u0026#34; $ProjectsUri = \u0026#34;${OrganizationUri}_apis/projects?api-version=5.1\u0026#34; $RepositoriesUri = \u0026#34;${OrganizationUri}_apis/git/repositories?api-version=7.0\u0026#34; AzDO projects # $Projects = Invoke-RestMethod -Uri $ProjectsUri -Method get -Headers $AuthenicationHeader foreach ($Project in $Projects.Value) { $ProjectName = $Project.name $ProjectUri = $Project.url Write-Host \u0026#34;${ProjectName}: ${ProjectUri}\u0026#34; } AzDO repositories # $Repositories = Invoke-RestMethod -Uri $RepositoriesUri -Method get -Headers $AuthenicationHeader foreach ($Repository in $Repositories.value) { $ProjectName = $Repository.Project.name $RepositoryName = $Repository.name $RepositoryWebUri = $Repository.webUrl $RepositoryIsActive = -not $Repository.IsDisabled if ($RepositoryIsActive) { $Folder = Join-Path -Path $PWD -ChildPath $ProjectName if (-not (Test-Path $Folder)) { New-Item $Folder -ItemType Directory | Out-Null } Push-Location -Path $Folder Write-Host \u0026#34;${ProjectName}/${RepositoryName} ...\u0026#34; git clone --depth 1 $RepositoryWebUri Pop-Location } } "},{"id":72,"href":"/docs/programming/powershell/basics/","title":"Basics","section":"Powershell","content":" Powershell basics # "},{"id":73,"href":"/docs/programming/powershell/snippets/","title":"Snippets","section":"Powershell","content":" Powershell snippets # Get unix timestamp # [int](Get-Date -UFormat %s -Millisecond 0) Match glob pattern # function Test-GlobMatch { param ( [string] $value, [string] $pattern ) $position = 0 foreach ($char in $pattern.toCharArray()) { Switch ($char) { \u0026#39;?\u0026#39; { continue } \u0026#39;*\u0026#39; { foreach ($i in $value.Length .. $position) { if (Test-GlobMatch $value.Substring($i) $pattern.Substring($position + 1)) { return $True } } return $False } default { if ($value.Length -eq $position -or $pattern[$position] -ne $value[$position]) { return $False } } } $position++ } return $value.Length -eq $position } Remove git directories # Get-ChildItem -Recurse -Directory -Hidden -Include \u0026#39;.git\u0026#39; | Remove-Item -Force -Recurse Compress as standard zip # foreach ($item in (Get-ChildItem -Directory)) { 7z a -mm=Deflate -mfb=258 -mpass=15 -r \u0026#34;${item}.zip\u0026#34; \u0026#34;${item}/*\u0026#34; } Compress maximally # foreach ($item in (Get-ChildItem -Directory)) { 7z a -t7z -mx=9 -mfb=273 -ms -md=31 -myx=9 -mtm=- -mmt -mmtf -md=1536m -mmf=bt3 -mmc=10000 -mpb=0 -mlc=0 \u0026#34;${item}.7z\u0026#34; \u0026#34;${item}\u0026#34; } "},{"id":74,"href":"/docs/programming/python/basics/","title":"Basics","section":"Python","content":" Python basics # Application structure # TODO\nClass structure # TODO\nExecutable module # TODO\n"},{"id":75,"href":"/docs/programming/python/environment/","title":"Environment","section":"Python","content":" Virtual environment # TODO\nVenv # Create and activate virtual environment:\nBash echo Hello Powershell python -m venv .venv .venv\\Scripts\\activate.ps1 Cmd echo Hello Upgrade pip and install requirements.txt:\npython -m pip install --upgrade pip pip install -r requirements.txt Deactivate virtual environment:\ndeactivate Pipenv # TODO\n"},{"id":76,"href":"/docs/programming/python/installation/","title":"Installation","section":"Python","content":" Installation # PyEnv on Linux # Install build dependencies:\nsudo apt update sudo apt install \\ build-essential \\ libssl-dev \\ zlib1g-dev \\ libbz2-dev \\ libreadline-dev \\ libsqlite3-dev \\ curl \\ llvm \\ libncursesw5-dev \\ xz-utils \\ tk-dev \\ libxml2-dev \\ libxmlsec1-dev \\ libffi-dev \\ liblzma-dev Install PyEnv scripts:\ncurl https://pyenv.run | bash Add to ~/.profile:\nexport PATH=\u0026#34;$HOME/.pyenv/bin:$PATH\u0026#34; if command -v pyenv 1\u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then eval \u0026#34;$(pyenv init -)\u0026#34; fi PyEnv on Windows # Copy pyenv-win from https://github.com/pyenv-win/pyenv-win/archive/master.zip to E:\\pgm\\pyenv-win.\nEnvironment variables:\nName Value PYENV_HOME E:\\pgm\\pyenv-win PYENV_ROOT E:\\pgm\\pyenv-win PYENV E:\\pgm\\pyenv-win Add to PATH:\nE:\\pgm\\pyenv-win\\bin E:\\pgm\\pyenv-win\\shims List available versions: pyenv install -l\nInstall chosen version: pyenv install 3.11.4\nSet global Python version: pyenv global 3.11.4\nSet local Python version: pyenv local 3.11.4\nInstall basic packages # pip install invoke pip install pipenv Create environment variable PIPENV_VENV_IN_PROJECT=1.\nInstall additional packages # pip install jupyterlab pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117 "},{"id":77,"href":"/docs/programming/sqlite/data/","title":"Data","section":"SQLite","content":" Data manipulations # CVS data # Example data in data.csv:\nindex, code, name\r1, EX1, Example 1\r2, EX2, Example 2\r3, EX3, Example 3 Start new database:\nsqlite3 data.db Load data into table and delete headers in sqlite shell:\ncreate table data (index, code, name); .mode csv .headers ON .separator , .import data.csv data delete from data where index = \u0026#39;index\u0026#39;; select * from data limit 60; "},{"id":78,"href":"/docs/programming/typescript/nodejs/","title":"Nodejs","section":"Typescript","content":" NodeJS # Installation on Linux # git clone https://github.com/nvm-sh/nvm.git /opt/nvm cd /opt/nvm git checkout v0.39.3 Add to ~/.profile:\nexport NVM_DIR=\u0026#34;$HOME/.nvm\u0026#34; if [ -s \u0026#34;$NVM_DIR/nvm.sh\u0026#34; ]; then . \u0026#34;$NVM_DIR/nvm.sh\u0026#34; fi if [ -s \u0026#34;$NVM_DIR/bash_completion\u0026#34; ]; then . \u0026#34;$NVM_DIR/bash_completion\u0026#34; fi Install NodeJS:\nnvm install node nvm install-latest-npm npm install -g yarn Installation on Windows # Install NVM for Windows\nSettings:\nroot: D:\\pgm\\nvm path: D:\\pgm\\nodejs Install NodeJS:\nnvm list available nvm install latest nvm use newest Install yarn:\nnpm install -g yarn "},{"id":79,"href":"/docs/devops/aws/","title":"AWS","section":"Devops","content":" Amazon Web Services # Common # Regions, Availability Zones, Local Zones IAM (Identity and Access Management) Cloudwatch Logs CDK (Cloud Development Kit) and CloudFormation VPC (Virtual Private Cloud) S3 (Simple Storage Service) Backend # EC2 (Elastic Cloud Compute) Lambda functions ECS (Elastic Container Service) EKS (Elastic Kubernetes Service) Load balancers Certificate managers SNS (Simple Notification Service) SQS (Simple Queue Service) Step functions DynamoDB RDS (Relational Database Service) ElastiCache Frontend # Amplify CloudFront API Gateway Cognito DevOps # CodeCommit CodeBuild CodePipeline Cloudwatch Alarms Cloudwatch Monitoring Cloudwatch Dashboards Data Engineering # Glue Batch Redshift Athena Lake Formation "},{"id":80,"href":"/docs/devops/kubernetes/","title":"Kubernetes","section":"Devops","content":" Kubernetes # Container orchestration:\nHigh availability Scalability Disaster recovery Architecture # A node is a virtual or physical machine.\nA Kubernetes cluster is made from at least one master node and connected to it worker nodes. All nodes are connected by cluster\u0026rsquo;s Virtual Network. All of these components create one unified machine.\nKubernetes does not manage data persistence. It is easier to host databases outside of Kubernetes cluster.\nEach worker node has a kubelet process running. Kubelet is a node agent that allows cluster to communicate with nodes and execute tasks on them. Worker nodes have application containers deployed on them. Applications run on worker nodes.\nMaster node runs Control Plane processes necessary for managing the cluster:\nAPI Server (entry point to the cluster for kubernetes clients) Controller Manager (keeps track of what is happening in the cluster) Scheduler (decides on which worker node to schedule new Pod) etcd key-value store (keeps current configuration and state of the cluster) Components # Pod Abstraction of container runtime for one or more containers. Smallest unit of computing that is assigned an individual IP address and can be deployed and managed. Service A permanent IP address and load balancer for Pods. Publishes its own virtual address either as an environment variable in every Pod or, if cluster is using CoreDNS, as a DNS entry. Services can abstract access not only to Pods, but also to databases, external host or other services. Ingress (forwards requests using domain name to a specific Service) ConfigMap (external configuration for application) Secret (stores secret configuration data encoded in base64) Deployment An abstraction allowing to manage the state of a set of Pods or Replica Sets. It allows to run a group of identical Pods with a common configuration. Volume (external disk storage - local or remote - connected to the cluster) Job (???) CronJob (???) StatefulSet Provides unique network identifiers, persistent storage, ordered deployment and scaling. DaemonSet Ensures that all matching Nodes run a copy of a Pod. ReplicaSet A group of duplicated identical pods. ReplicationController (???) "},{"id":81,"href":"/docs/devops/macos/","title":"MacOS","section":"Devops","content":"Quick start:\nXCode # xcode-select --install ssh-keygen -t ed25519 -C \u0026#34;username@example.com\u0026#34; git config --global user.email \u0026#34;username@example.com\u0026#34; git config --global user.name \u0026#34;username\u0026#34; Run software updates.\nSetup profile for Terminal in Settings/Profiles. Copy from Pro.\nAdd to ~/.zshenv:\nplugins=(git ssh-agent) Configure GitHub:\nssh-keygen -t rsa -C \u0026#34;dpurge@example.com\u0026#34; ssh-add ~/.ssh/id_rsa_dpurge Add to ~/.ssh/config:\n# username account Host github.com-username HostName github.com User git IdentityFile ~/.ssh/id_ed25519 # dpurge account Host github.com-dpurge HostName github.com User git IdentityFile ~/.ssh/id_rsa_dpurge Create workspace:\nmkdir -p ~/src/github.com/dpurge cd ~/src/github.com/dpurge git clone git@github.com:dpurge/dpurge.github.io.git cd dpurge.github.io git config user.email \u0026#34;dpurge@example.com\u0026#34; git config user.name \u0026#34;D. Purge\u0026#34; VPN certificate request # openssl ecparam -out ~/Documents/${USER}.key -name secp384r1 -genkey openssl req -new -key ~/Documents/${USER}.key -out ~/Documents/${USER}.csr -subj \u0026#34;/CN=${USER}/SN=$(system_profiler SPHardwareDataType | awk \u0026#39;/Serial Number \\(system\\)/{print $NF}\u0026#39;)\u0026#34; Homebrew # Use installer from GitHub releases.\nbrew update brew install openssl readline sqlite3 xz zlib brew instll mc brew install pyenv brew install pyenv-virtualenv pyenv install 3.12.7 pyenv global 3.12.7 brew install deno brew install warrensbox/tap/tgswitch brew install warrensbox/tap/tfswitch tfswitch -u tgswitch pip install jupyterlab deno jupyter --install pip install ansible brew install go-task/tap/go-task brew install k3d brew install podman-compose Add to ~/.zshenv:\nPATH=\u0026#34;${HOME}/bin:/usr/local/bin:${PATH}\u0026#34; eval \u0026#34;$(pyenv init -)\u0026#34; eval \u0026#34;$(pyenv virtualenv-init -)\u0026#34; export PATH VS Code # Use installer from download page.\nPodman # Use installer from GitHub releases.\npodman machine init --cpus 4 --memory 2048 --disk-size 100 podman machine set --rootful Go # Use installer from install page.\nJupyter # "},{"id":82,"href":"/docs/devops/ollama/","title":"Ollama","section":"Devops","content":" VS Code setup # ollama pull llama3.1 ollama pull deepseek-coder ollama pull nomic-embed-text { \u0026#34;models\u0026#34;: [ { \u0026#34;title\u0026#34;: \u0026#34;Llama 3.1\u0026#34;, \u0026#34;provider\u0026#34;: \u0026#34;ollama\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;llama3.1:latest\u0026#34; } ], \u0026#34;tabAutocompleteModel\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;DeepSeek Coder\u0026#34;, \u0026#34;provider\u0026#34;: \u0026#34;ollama\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;deepseek-coder:latest\u0026#34; }, \u0026#34;embeddingsProvider\u0026#34;: { \u0026#34;provider\u0026#34;: \u0026#34;ollama\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;nomic-embed-text\u0026#34; } } "},{"id":83,"href":"/docs/devops/qemu/","title":"QEmu","section":"Devops","content":"Manual:\nqemu-img create -f qcow2 ubuntu.img 50G qemu-system-x86_64 -boot d -cdrom ubuntu.iso -m 4G -smp 2 -hda ubuntu.img -nic user,hostfwd=tcp::8888-:22 # reboot without cdrom qemu-system-x86_64 -m 4G -smp 2 -hda ubuntu.img -nic user,hostfwd=tcp::8888-:22 -virtfs local,path=$PWD,mount_tag=host0,security_model=passthrough,id=host0 -nographic "},{"id":84,"href":"/docs/devops/terraform/","title":"Terraform","section":"Devops","content":"Terraform can provision infratsructure on public and private clouds.\nGeneric formats:\nprovider \u0026#34;{name}\u0026#34; { ...} variable \u0026#34;{name}\u0026#34; { ... } data \u0026#34;{provider}_{type}\u0026#34; \u0026#34;{name}\u0026#34; { ... } resource \u0026#34;{provider}_{type}\u0026#34; \u0026#34;{name}\u0026#34; { ... } output \u0026#34;{name}\u0026#34; { ... } Reference formats:\nvar.{name} data.{provider}_{type}.{name}.{attribute} "}]